{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"datapizza-ai","text":"<p>Build reliable Gen AI solutions without overhead </p> <p><code>datapizza-ai</code> provides clear interfaces and predictable behavior for agents and RAG. End-to-end visibility and reliable orchestration keep engineers in control from PoC to scale</p>"},{"location":"#installation","title":"Installation","text":"<p>Install the library using pip:</p> <pre><code>pip install datapizza-ai\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Integration with AI Providers: Seamlessly connect with AI services like OpenAI and Google VertexAI.</li> <li>Complex workflows, minimal code.: Design, automate, and scale powerful agent workflows without the overhead of boilerplate.</li> <li>Retrieval-Augmented Generation (RAG): Enhance AI responses with document retrieval.</li> <li>Faster delivery, easier onboarding for new engineers: Rebuild a RAG + tools agent without multi-class plumbing; parity with simpler, typed interfaces.</li> <li>Up to 40% less debugging time: Trace and log every LLM/tool call with inputs/outputs</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>To get started with <code>datapizza-ai</code>, ensure you have Python <code>&gt;=3.10.0,&lt;3.13.0</code> installed.</p> <p>Here's a basic example demonstrating how to use agents in <code>datapizza-ai</code>:</p> <pre><code>from datapizza.agents import Agent\nfrom datapizza.clients.openai import OpenAIClient\nfrom datapizza.tools import tool\n\n@tool\ndef get_weather(city: str) -&gt; str:\n    return f\"The weather in {city} is sunny\"\n\nclient = OpenAIClient(api_key=\"YOUR_API_KEY\")\nagent = Agent(name=\"assistant\", client=client, tools = [get_weather])\n\nresponse = agent.run(\"What is the weather in Rome?\")\n# output: The weather in Rome is sunny\n</code></pre>"},{"location":"API%20Reference/","title":"API Reference","text":"<p>Here you can find the API reference for <code>datapizza-ai</code>. Most of what you find here is work in progress! \u26a0\ufe0f</p> <p>Use the navigation on the left or search to find the classes you are interested in.</p>"},{"location":"API%20Reference/memory/","title":"Memory","text":""},{"location":"API%20Reference/memory/#datapizza.memory.memory.Memory","title":"datapizza.memory.memory.Memory","text":"<p>A class for storing the memory of a chat, organized by conversation turns. Each turn can contain multiple blocks (text, function calls, or structured data).</p>"},{"location":"API%20Reference/memory/#datapizza.memory.memory.Memory.__bool__","title":"__bool__","text":"<pre><code>__bool__()\n</code></pre> <p>Return True if memory contains any turns, False otherwise.</p>"},{"location":"API%20Reference/memory/#datapizza.memory.memory.Memory.__delitem__","title":"__delitem__","text":"<pre><code>__delitem__(index)\n</code></pre> <p>Delete a specific turn.</p>"},{"location":"API%20Reference/memory/#datapizza.memory.memory.Memory.__eq__","title":"__eq__","text":"<pre><code>__eq__(other)\n</code></pre> <p>Compare two Memory objects based on their content hash. This is more efficient than comparing the full content structure.</p>"},{"location":"API%20Reference/memory/#datapizza.memory.memory.Memory.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(index)\n</code></pre> <p>Get all blocks from a specific turn.</p>"},{"location":"API%20Reference/memory/#datapizza.memory.memory.Memory.__hash__","title":"__hash__","text":"<pre><code>__hash__()\n</code></pre> <p>Creates a deterministic hash based on the content of memory turns.</p>"},{"location":"API%20Reference/memory/#datapizza.memory.memory.Memory.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Iterate through all blocks in all turns.</p>"},{"location":"API%20Reference/memory/#datapizza.memory.memory.Memory.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the total number of turns.</p>"},{"location":"API%20Reference/memory/#datapizza.memory.memory.Memory.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> <p>Return a detailed string representation of the memory.</p>"},{"location":"API%20Reference/memory/#datapizza.memory.memory.Memory.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(index, value)\n</code></pre> <p>Set blocks for a specific turn.</p>"},{"location":"API%20Reference/memory/#datapizza.memory.memory.Memory.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>Return a string representation of the memory.</p>"},{"location":"API%20Reference/memory/#datapizza.memory.memory.Memory.add_to_last_turn","title":"add_to_last_turn","text":"<pre><code>add_to_last_turn(block)\n</code></pre> <p>Add a block to the most recent turn. Creates a new turn if memory is empty. Args:     block (Block): The block to add to the most recent turn.</p>"},{"location":"API%20Reference/memory/#datapizza.memory.memory.Memory.add_turn","title":"add_turn","text":"<pre><code>add_turn(blocks, role)\n</code></pre> <p>Add a new conversation turn containing one or more blocks.</p> <p>Parameters:</p> Name Type Description Default <code>blocks</code> <code>list[Block] | Block</code> <p>The blocks to add to the new turn.</p> required <code>role</code> <code>ROLE</code> <p>The role of the new turn.</p> required"},{"location":"API%20Reference/memory/#datapizza.memory.memory.Memory.clear","title":"clear","text":"<pre><code>clear()\n</code></pre> <p>Clear all memory.</p>"},{"location":"API%20Reference/memory/#datapizza.memory.memory.Memory.copy","title":"copy","text":"<pre><code>copy()\n</code></pre> <p>Deep copy the memory.</p>"},{"location":"API%20Reference/memory/#datapizza.memory.memory.Memory.iter_blocks","title":"iter_blocks","text":"<pre><code>iter_blocks()\n</code></pre> <p>Iterate through blocks.</p>"},{"location":"API%20Reference/memory/#datapizza.memory.memory.Memory.json_dumps","title":"json_dumps","text":"<pre><code>json_dumps()\n</code></pre> <p>Serialize the memory to JSON.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The JSON representation of the memory.</p>"},{"location":"API%20Reference/memory/#datapizza.memory.memory.Memory.json_loads","title":"json_loads","text":"<pre><code>json_loads(json_str)\n</code></pre> <p>Deserialize JSON to the memory.</p> <p>Parameters:</p> Name Type Description Default <code>json_str</code> <code>str</code> <p>The JSON string to deserialize.</p> required"},{"location":"API%20Reference/memory/#datapizza.memory.memory.Memory.new_turn","title":"new_turn","text":"<pre><code>new_turn(role=ROLE.ASSISTANT)\n</code></pre> <p>Add a new conversation turn.</p> <p>Parameters:</p> Name Type Description Default <code>role</code> <code>ROLE</code> <p>The role of the new turn. Defaults to ROLE.ASSISTANT.</p> <code>ASSISTANT</code>"},{"location":"API%20Reference/memory/#datapizza.memory.memory.Memory.to_dict","title":"to_dict","text":"<pre><code>to_dict()\n</code></pre> <p>Convert memory to a dictionary.</p> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: The dictionary representation of the memory.</p>"},{"location":"API%20Reference/Agents/agent/","title":"Agent","text":""},{"location":"API%20Reference/Agents/agent/#datapizza.agents.agent.Agent","title":"datapizza.agents.agent.Agent","text":""},{"location":"API%20Reference/Agents/agent/#datapizza.agents.agent.Agent.__init__","title":"__init__","text":"<pre><code>__init__(\n    name=None,\n    client=None,\n    *,\n    system_prompt=None,\n    tools=None,\n    max_steps=None,\n    terminate_on_text=True,\n    stateless=True,\n    gen_args=None,\n    memory=None,\n    stream=None,\n    can_call=None,\n    logger=None,\n    planning_interval=0,\n    planning_prompt=PLANNING_PROMT,\n)\n</code></pre> <p>Initialize the agent.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the agent. Defaults to None.</p> <code>None</code> <code>client</code> <code>Client</code> <p>The client to use for the agent. Defaults to None.</p> <code>None</code> <code>system_prompt</code> <code>str</code> <p>The system prompt to use for the agent. Defaults to None.</p> <code>None</code> <code>tools</code> <code>list[Tool]</code> <p>A list of tools to use with the agent. Defaults to None.</p> <code>None</code> <code>max_steps</code> <code>int</code> <p>The maximum number of steps to execute. Defaults to None.</p> <code>None</code> <code>terminate_on_text</code> <code>bool</code> <p>Whether to terminate the agent on text. Defaults to True.</p> <code>True</code> <code>stateless</code> <code>bool</code> <p>Whether to use stateless execution. Defaults to True.</p> <code>True</code> <code>gen_args</code> <code>dict[str, Any]</code> <p>Additional arguments to pass to the agent's execution. Defaults to None.</p> <code>None</code> <code>memory</code> <code>Memory</code> <p>The memory to use for the agent. Defaults to None.</p> <code>None</code> <code>stream</code> <code>bool</code> <p>Whether to stream the agent's execution. Defaults to None.</p> <code>None</code> <code>can_call</code> <code>list[Agent]</code> <p>A list of agents that can call the agent. Defaults to None.</p> <code>None</code> <code>logger</code> <code>AgentLogger</code> <p>The logger to use for the agent. Defaults to None.</p> <code>None</code> <code>planning_interval</code> <code>int</code> <p>The planning interval to use for the agent. Defaults to 0.</p> <code>0</code> <code>planning_prompt</code> <code>str</code> <p>The planning prompt to use for the agent planning steps. Defaults to PLANNING_PROMT.</p> <code>PLANNING_PROMT</code>"},{"location":"API%20Reference/Agents/agent/#datapizza.agents.agent.Agent.a_run","title":"a_run  <code>async</code>","text":"<pre><code>a_run(task_input, tool_choice='auto', **gen_kwargs)\n</code></pre> <p>Run the agent on a task input asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>task_input</code> <code>str</code> <p>The input text/prompt to send to the model</p> required <code>tool_choice</code> <code>Literal['auto', 'required', 'none', 'required_first'] | list[str]</code> <p>Controls which tool to use (\"auto\" by default)</p> <code>'auto'</code> <code>**gen_kwargs</code> <p>Additional keyword arguments to pass to the agent's execution</p> <code>{}</code> <p>Returns:</p> Type Description <code>StepResult | None</code> <p>The final result of the agent's execution</p>"},{"location":"API%20Reference/Agents/agent/#datapizza.agents.agent.Agent.a_stream_invoke","title":"a_stream_invoke  <code>async</code>","text":"<pre><code>a_stream_invoke(\n    task_input, tool_choice=\"auto\", **gen_kwargs\n)\n</code></pre> <p>Stream the agent's execution asynchronously, yielding intermediate steps and final result.</p> <p>Parameters:</p> Name Type Description Default <code>task_input</code> <code>str</code> <p>The input text/prompt to send to the model</p> required <code>tool_choice</code> <code>Literal['auto', 'required', 'none', 'required_first'] | list[str]</code> <p>Controls which tool to use (\"auto\" by default)</p> <code>'auto'</code> <code>**gen_kwargs</code> <p>Additional keyword arguments to pass to the agent's execution</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncGenerator[ClientResponse | StepResult | Plan | None]</code> <p>The intermediate steps and final result of the agent's execution</p>"},{"location":"API%20Reference/Agents/agent/#datapizza.agents.agent.Agent.run","title":"run","text":"<pre><code>run(task_input, tool_choice='auto', **gen_kwargs)\n</code></pre> <p>Run the agent on a task input.</p> <p>Parameters:</p> Name Type Description Default <code>task_input</code> <code>str</code> <p>The input text/prompt to send to the model</p> required <code>tool_choice</code> <code>Literal['auto', 'required', 'none', 'required_first'] | list[str]</code> <p>Controls which tool to use (\"auto\" by default)</p> <code>'auto'</code> <code>**gen_kwargs</code> <p>Additional keyword arguments to pass to the agent's execution</p> <code>{}</code> <p>Returns:</p> Type Description <code>StepResult | None</code> <p>The final result of the agent's execution</p>"},{"location":"API%20Reference/Agents/agent/#datapizza.agents.agent.Agent.stream_invoke","title":"stream_invoke","text":"<pre><code>stream_invoke(task_input, tool_choice='auto', **gen_kwargs)\n</code></pre> <p>Stream the agent's execution, yielding intermediate steps and final result.</p> <p>Parameters:</p> Name Type Description Default <code>task_input</code> <code>str</code> <p>The input text/prompt to send to the model</p> required <code>tool_choice</code> <code>Literal['auto', 'required', 'none', 'required_first'] | list[str]</code> <p>Controls which tool to use (\"auto\" by default)</p> <code>'auto'</code> <code>**gen_kwargs</code> <p>Additional keyword arguments to pass to the agent's execution</p> <code>{}</code> <p>Yields:</p> Type Description <code>ClientResponse | StepResult | Plan | None</code> <p>The intermediate steps and final result of the agent's execution</p>"},{"location":"API%20Reference/Clients/cache/","title":"Cache","text":""},{"location":"API%20Reference/Clients/cache/#datapizza.core.cache.cache.Cache","title":"datapizza.core.cache.cache.Cache","text":"<p>               Bases: <code>ABC</code></p> <p>This is the abstract base class for all cache implementations. Concrete subclasses must provide implementations for the abstract methods that define how caching is handled.</p> <p>When a cache instance is attached to a client, it will automatically store the results of the client`s method calls. If the same method is invoked multiple times with identical arguments, the cache returns the stored result instead of re-executing the method.</p>"},{"location":"API%20Reference/Clients/cache/#datapizza.core.cache.cache.Cache.get","title":"get  <code>abstractmethod</code>","text":"<pre><code>get(key)\n</code></pre> <p>Retrieve an object from the cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to retrieve the object for.</p> required <p>Returns:</p> Type Description <code>object</code> <p>The object stored in the cache.</p>"},{"location":"API%20Reference/Clients/cache/#datapizza.core.cache.cache.Cache.set","title":"set  <code>abstractmethod</code>","text":"<pre><code>set(key, value)\n</code></pre> <p>Store an object in the cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to store the object for.</p> required <code>value</code> <code>str</code> <p>The object to store in the cache.</p> required"},{"location":"API%20Reference/Clients/client_factory/","title":"Client Factory","text":"<p>The ClientFactory provides a convenient way to create LLM clients for different providers without having to import and instantiate each client type individually.</p>"},{"location":"API%20Reference/Clients/client_factory/#datapizza.clients.factory.ClientFactory","title":"datapizza.clients.factory.ClientFactory","text":"<p>Factory for creating LLM clients</p>"},{"location":"API%20Reference/Clients/client_factory/#datapizza.clients.factory.ClientFactory.create","title":"create  <code>staticmethod</code>","text":"<pre><code>create(\n    provider,\n    api_key,\n    model,\n    system_prompt=\"\",\n    temperature=0.7,\n    **kwargs,\n)\n</code></pre> <p>Create a client instance based on the specified provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str | Provider</code> <p>The LLM provider to use (openai, google, or anthropic)</p> required <code>api_key</code> <code>str</code> <p>API key for the provider</p> required <code>model</code> <code>str</code> <p>Model name to use (provider-specific)</p> required <code>system_prompt</code> <code>str</code> <p>System prompt to use</p> <code>''</code> <code>temperature</code> <code>float</code> <p>Temperature for generation (0-2)</p> <code>0.7</code> <code>**kwargs</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Client</code> <p>An instance of the appropriate client</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provider is not supported</p>"},{"location":"API%20Reference/Clients/client_factory/#example-usage","title":"Example Usage","text":"<pre><code>from datapizza.clients.factory import ClientFactory, Provider\n\n# Create an OpenAI client\nopenai_client = ClientFactory.create(\n    provider=Provider.OPENAI,\n    api_key=\"OPENAI_API_KEY\",\n    model=\"gpt-4\",\n    system_prompt=\"You are a helpful assistant.\",\n    temperature=0.7\n)\n\n# Create a Google client using string provider\ngoogle_client = ClientFactory.create(\n    provider=\"google\",\n    api_key=\"GOOGLE_API_KEY\",\n    model=\"gemini-pro\",\n    system_prompt=\"You are a helpful assistant.\",\n    temperature=0.5\n)\n\n# Create an Anthropic client with custom parameters\nanthropic_client = ClientFactory.create(\n    provider=Provider.ANTHROPIC,\n    api_key=\"ANTHROPIC_API_KEY\",\n    model=\"claude-3-sonnet-20240229\",\n    system_prompt=\"You are a helpful assistant.\",\n    temperature=0.3,\n)\n\n# Use the client\nresponse = openai_client.invoke(\"What is the capital of France?\")\nprint(response.content)\n</code></pre>"},{"location":"API%20Reference/Clients/client_factory/#supported-providers","title":"Supported Providers","text":"<ul> <li><code>openai</code> - OpenAI GPT models</li> <li><code>google</code> - Google Gemini models  </li> <li><code>anthropic</code> - Anthropic Claude models</li> <li><code>mistral</code> - Mistral AI models</li> </ul>"},{"location":"API%20Reference/Clients/clients/","title":"Clients","text":""},{"location":"API%20Reference/Clients/clients/#datapizza.core.clients.client.Client","title":"datapizza.core.clients.client.Client","text":"<p>               Bases: <code>ChainableProducer</code></p> <p>Represents the base class for all clients. Concrete implementations must implement the abstract methods to handle the actual inference.</p>"},{"location":"API%20Reference/Clients/clients/#datapizza.core.clients.client.Client.a_embed","title":"a_embed  <code>async</code>","text":"<pre><code>a_embed(text, model_name=None, **kwargs)\n</code></pre> <p>Embed a text using the model</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | list[str]</code> <p>The text to embed</p> required <code>model_name</code> <code>str</code> <p>The name of the model to use. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the model's embedding method</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[float] | list[list[float]]</code> <p>list[float]: The embedding vector for the text</p>"},{"location":"API%20Reference/Clients/clients/#datapizza.core.clients.client.Client.a_invoke","title":"a_invoke  <code>async</code>","text":"<pre><code>a_invoke(\n    input,\n    tools=None,\n    memory=None,\n    tool_choice=\"auto\",\n    temperature=None,\n    max_tokens=None,\n    system_prompt=None,\n    **kwargs,\n)\n</code></pre> <p>Performs a single inference request to the model.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>The input text/prompt to send to the model</p> required <code>tools</code> <code>List[Tool]</code> <p>List of tools available for the model to use. Defaults to [].</p> <code>None</code> <code>memory</code> <code>Memory</code> <p>Memory object containing conversation history. Defaults to None.</p> <code>None</code> <code>tool_choice</code> <code>str</code> <p>Controls which tool to use. Defaults to \"auto\".</p> <code>'auto'</code> <code>temperature</code> <code>float</code> <p>Controls randomness in responses. Defaults to None.</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens in the response. Defaults to None.</p> <code>None</code> <code>system_prompt</code> <code>str</code> <p>System-level instructions for the model. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the model's inference method</p> <code>{}</code> <p>Returns:</p> Type Description <code>ClientResponse</code> <p>A ClientResponse object containing the model's response</p>"},{"location":"API%20Reference/Clients/clients/#datapizza.core.clients.client.Client.a_stream_invoke","title":"a_stream_invoke  <code>async</code>","text":"<pre><code>a_stream_invoke(\n    input,\n    tools=None,\n    memory=None,\n    tool_choice=\"auto\",\n    temperature=None,\n    max_tokens=None,\n    system_prompt=None,\n    **kwargs,\n)\n</code></pre> <p>Streams the model's response token by token asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>The input text/prompt to send to the model</p> required <code>tools</code> <code>List[Tool]</code> <p>List of tools available for the model to use. Defaults to [].</p> <code>None</code> <code>memory</code> <code>Memory</code> <p>Memory object containing conversation history. Defaults to None.</p> <code>None</code> <code>tool_choice</code> <code>str</code> <p>Controls which tool to use. Defaults to \"auto\".</p> <code>'auto'</code> <code>temperature</code> <code>float</code> <p>Controls randomness in responses. Defaults to None.</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens in the response. Defaults to None.</p> <code>None</code> <code>system_prompt</code> <code>str</code> <p>System-level instructions for the model. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the model's inference method</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncIterator[ClientResponse]</code> <p>An async iterator yielding ClientResponse objects containing the model's response</p>"},{"location":"API%20Reference/Clients/clients/#datapizza.core.clients.client.Client.a_structured_response","title":"a_structured_response  <code>async</code>","text":"<pre><code>a_structured_response(\n    *,\n    input,\n    output_cls,\n    memory=None,\n    temperature=None,\n    max_tokens=None,\n    system_prompt=None,\n    tools=None,\n    tool_choice=\"auto\",\n    **kwargs,\n)\n</code></pre> <p>Structures the model's response according to a specified output class.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>The input text/prompt to send to the model</p> required <code>output_cls</code> <code>Type[Model]</code> <p>The class type to structure the response into</p> required <code>memory</code> <code>Memory</code> <p>Memory object containing conversation history. Defaults to None.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Controls randomness in responses. Defaults to None.</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens in the response. Defaults to None.</p> <code>None</code> <code>system_prompt</code> <code>str</code> <p>System-level instructions for the model. Defaults to None.</p> <code>None</code> <code>tools</code> <code>List[Tool]</code> <p>List of tools available for the model to use. Defaults to [].</p> <code>None</code> <code>tool_choice</code> <code>Literal['auto', 'required', 'none'] | list[str]</code> <p>Controls which tool to use (\"auto\" by default). Defaults to \"auto\".</p> <code>'auto'</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the model's inference method</p> <code>{}</code> <p>Returns:</p> Type Description <code>ClientResponse</code> <p>A ClientResponse object containing the structured response</p>"},{"location":"API%20Reference/Clients/clients/#datapizza.core.clients.client.Client.embed","title":"embed","text":"<pre><code>embed(text, model_name=None, **kwargs)\n</code></pre> <p>Embed a text using the model</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | list[str]</code> <p>The text to embed</p> required <code>model_name</code> <code>str</code> <p>The name of the model to use. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the model's embedding method</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[float]</code> <p>list[float]: The embedding vector for the text</p>"},{"location":"API%20Reference/Clients/clients/#datapizza.core.clients.client.Client.invoke","title":"invoke","text":"<pre><code>invoke(\n    input,\n    tools=None,\n    memory=None,\n    tool_choice=\"auto\",\n    temperature=None,\n    max_tokens=None,\n    system_prompt=None,\n    **kwargs,\n)\n</code></pre> <p>Performs a single inference request to the model.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>The input text/prompt to send to the model</p> required <code>tools</code> <code>List[Tool]</code> <p>List of tools available for the model to use. Defaults to [].</p> <code>None</code> <code>memory</code> <code>Memory</code> <p>Memory object containing conversation history. Defaults to None.</p> <code>None</code> <code>tool_choice</code> <code>str</code> <p>Controls which tool to use. Defaults to \"auto\".</p> <code>'auto'</code> <code>temperature</code> <code>float</code> <p>Controls randomness in responses. Defaults to None.</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens in the response. Defaults to None.</p> <code>None</code> <code>system_prompt</code> <code>str</code> <p>System-level instructions for the model. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the model's inference method</p> <code>{}</code> <p>Returns:</p> Type Description <code>ClientResponse</code> <p>A ClientResponse object containing the model's response</p>"},{"location":"API%20Reference/Clients/clients/#datapizza.core.clients.client.Client.stream_invoke","title":"stream_invoke","text":"<pre><code>stream_invoke(\n    input,\n    tools=None,\n    memory=None,\n    tool_choice=\"auto\",\n    temperature=None,\n    max_tokens=None,\n    system_prompt=None,\n    **kwargs,\n)\n</code></pre> <p>Streams the model's response token by token.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>The input text/prompt to send to the model</p> required <code>tools</code> <code>List[Tool]</code> <p>List of tools available for the model to use. Defaults to [].</p> <code>None</code> <code>memory</code> <code>Memory</code> <p>Memory object containing conversation history. Defaults to None.</p> <code>None</code> <code>tool_choice</code> <code>str</code> <p>Controls which tool to use. Defaults to \"auto\".</p> <code>'auto'</code> <code>temperature</code> <code>float</code> <p>Controls randomness in responses. Defaults to None.</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens in the response. Defaults to None.</p> <code>None</code> <code>system_prompt</code> <code>str</code> <p>System-level instructions for the model. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the model's inference method</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[ClientResponse]</code> <p>An iterator yielding ClientResponse objects containing the model's response</p>"},{"location":"API%20Reference/Clients/clients/#datapizza.core.clients.client.Client.structured_response","title":"structured_response","text":"<pre><code>structured_response(\n    *,\n    input,\n    output_cls,\n    memory=None,\n    temperature=None,\n    max_tokens=None,\n    system_prompt=None,\n    tools=None,\n    tool_choice=\"auto\",\n    **kwargs,\n)\n</code></pre> <p>Structures the model's response according to a specified output class.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>The input text/prompt to send to the model</p> required <code>output_cls</code> <code>Type[Model]</code> <p>The class type to structure the response into</p> required <code>memory</code> <code>Memory</code> <p>Memory object containing conversation history. Defaults to None.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Controls randomness in responses. Defaults to None.</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens in the response. Defaults to None.</p> <code>None</code> <code>system_prompt</code> <code>str</code> <p>System-level instructions for the model. Defaults to None.</p> <code>None</code> <code>tools</code> <code>List[Tool]</code> <p>List of tools available for the model to use. Defaults to [].</p> <code>None</code> <code>tool_choice</code> <code>Literal['auto', 'required', 'none'] | list[str]</code> <p>Controls which tool to use (\"auto\" by default). Defaults to \"auto\".</p> <code>'auto'</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the model's inference method</p> <code>{}</code> <p>Returns:</p> Type Description <code>ClientResponse</code> <p>A ClientResponse object containing the structured response</p>"},{"location":"API%20Reference/Clients/models/","title":"Response","text":""},{"location":"API%20Reference/Clients/models/#datapizza.core.clients.ClientResponse","title":"datapizza.core.clients.ClientResponse","text":"<p>A class for storing the response from a client. Contains a list of blocks that can be text, function calls, or structured data, maintaining the order in which they were generated.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>List[Block]</code> <p>A list of blocks.</p> required <code>delta</code> <code>str</code> <p>The delta of the response. Used for streaming responses.</p> <code>None</code>"},{"location":"API%20Reference/Clients/models/#datapizza.core.clients.ClientResponse.first_text","title":"first_text  <code>property</code>","text":"<pre><code>first_text\n</code></pre> <p>Returns the content of the first TextBlock or None</p>"},{"location":"API%20Reference/Clients/models/#datapizza.core.clients.ClientResponse.function_calls","title":"function_calls  <code>property</code>","text":"<pre><code>function_calls\n</code></pre> <p>Returns all function calls in order</p>"},{"location":"API%20Reference/Clients/models/#datapizza.core.clients.ClientResponse.structured_data","title":"structured_data  <code>property</code>","text":"<pre><code>structured_data\n</code></pre> <p>Returns all structured data in order</p>"},{"location":"API%20Reference/Clients/models/#datapizza.core.clients.ClientResponse.text","title":"text  <code>property</code>","text":"<pre><code>text\n</code></pre> <p>Returns concatenated text from all TextBlocks in order</p>"},{"location":"API%20Reference/Clients/models/#datapizza.core.clients.ClientResponse.thoughts","title":"thoughts  <code>property</code>","text":"<pre><code>thoughts\n</code></pre> <p>Returns all thoughts in order</p>"},{"location":"API%20Reference/Clients/models/#datapizza.core.clients.ClientResponse.is_pure_function_call","title":"is_pure_function_call","text":"<pre><code>is_pure_function_call()\n</code></pre> <p>Returns True if response contains only FunctionCallBlocks</p>"},{"location":"API%20Reference/Clients/models/#datapizza.core.clients.ClientResponse.is_pure_text","title":"is_pure_text","text":"<pre><code>is_pure_text()\n</code></pre> <p>Returns True if response contains only TextBlocks</p>"},{"location":"API%20Reference/Clients/Avaiable_Clients/anthropic/","title":"Anthropic","text":"<pre><code>pip install datapizza-ai-clients-anthropic\n</code></pre>"},{"location":"API%20Reference/Clients/Avaiable_Clients/anthropic/#datapizza.clients.anthropic.AnthropicClient","title":"datapizza.clients.anthropic.AnthropicClient","text":"<p>               Bases: <code>Client</code></p> <p>A client for interacting with the Anthropic API (Claude).</p> <p>This class provides methods for invoking the Anthropic API to generate responses based on given input data. It extends the Client class.</p>"},{"location":"API%20Reference/Clients/Avaiable_Clients/anthropic/#datapizza.clients.anthropic.AnthropicClient.__init__","title":"__init__","text":"<pre><code>__init__(\n    api_key,\n    model=\"claude-3-5-sonnet-latest\",\n    system_prompt=\"\",\n    temperature=None,\n    cache=None,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key for the Anthropic API.</p> required <code>model</code> <code>str</code> <p>The model to use for the Anthropic API.</p> <code>'claude-3-5-sonnet-latest'</code> <code>system_prompt</code> <code>str</code> <p>The system prompt to use for the Anthropic API.</p> <code>''</code> <code>temperature</code> <code>float | None</code> <p>The temperature to use for the Anthropic API.</p> <code>None</code> <code>cache</code> <code>Cache | None</code> <p>The cache to use for the Anthropic API.</p> <code>None</code>"},{"location":"API%20Reference/Clients/Avaiable_Clients/anthropic/#usage-example","title":"Usage example","text":"<pre><code>from datapizza.clients.anthropic import AnthropicClient\n\nclient = AnthropicClient(\n    api_key=\"YOUR_API_KEY\"\n    model=\"claude-3-5-sonnet-20240620\",\n)\nresposne = client.invoke(\"hi\")\nprint(response.text)\n</code></pre>"},{"location":"API%20Reference/Clients/Avaiable_Clients/anthropic/#show-thinking","title":"Show thinking","text":"<pre><code>import os\n\nfrom datapizza.clients.anthropic import AnthropicClient\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclient = AnthropicClient(\n    api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n    model=\"claude-sonnet-4-0\",\n)\n\nresponse = client.invoke(\"Hi\", thinking =  {\"type\": \"enabled\", \"budget_tokens\": 1024})\nprint(response)\n</code></pre>"},{"location":"API%20Reference/Clients/Avaiable_Clients/google/","title":"Google","text":"<pre><code>pip install datapizza-ai-clients-google\n</code></pre>"},{"location":"API%20Reference/Clients/Avaiable_Clients/google/#datapizza.clients.google.GoogleClient","title":"datapizza.clients.google.GoogleClient","text":"<p>               Bases: <code>Client</code></p> <p>A client for interacting with Google's Generative AI APIs.</p> <p>This class provides methods for invoking the Google GenAI API to generate responses based on given input data. It extends the Client class.</p>"},{"location":"API%20Reference/Clients/Avaiable_Clients/google/#datapizza.clients.google.GoogleClient.__init__","title":"__init__","text":"<pre><code>__init__(\n    api_key=None,\n    model=\"gemini-2.0-flash\",\n    system_prompt=\"\",\n    temperature=None,\n    cache=None,\n    project_id=None,\n    location=None,\n    credentials_path=None,\n    use_vertexai=False,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str | None</code> <p>The API key for the Google API.</p> <code>None</code> <code>model</code> <code>str</code> <p>The model to use for the Google API.</p> <code>'gemini-2.0-flash'</code> <code>system_prompt</code> <code>str</code> <p>The system prompt to use for the Google API.</p> <code>''</code> <code>temperature</code> <code>float | None</code> <p>The temperature to use for the Google API.</p> <code>None</code> <code>cache</code> <code>Cache | None</code> <p>The cache to use for the Google API.</p> <code>None</code> <code>project_id</code> <code>str | None</code> <p>The project ID for the Google API.</p> <code>None</code> <code>location</code> <code>str | None</code> <p>The location for the Google API.</p> <code>None</code> <code>credentials_path</code> <code>str | None</code> <p>The path to the credentials for the Google API.</p> <code>None</code> <code>use_vertexai</code> <code>bool</code> <p>Whether to use Vertex AI for the Google API.</p> <code>False</code>"},{"location":"API%20Reference/Clients/Avaiable_Clients/google/#usage-example","title":"Usage example","text":"<pre><code>import os\n\nfrom datapizza.clients.google import GoogleClient\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclient = GoogleClient(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n\nresponse = client.invoke(\"Hello!\")\nprint(response.text)\n</code></pre>"},{"location":"API%20Reference/Clients/Avaiable_Clients/mistral/","title":"Mistral","text":"<pre><code>pip install datapizza-ai-clients-mistral\n</code></pre>"},{"location":"API%20Reference/Clients/Avaiable_Clients/mistral/#datapizza.clients.mistral.MistralClient","title":"datapizza.clients.mistral.MistralClient","text":"<p>               Bases: <code>Client</code></p> <p>A client for interacting with the Mistral API.</p> <p>This class provides methods for invoking the Mistral API to generate responses based on given input data. It extends the Client class.</p>"},{"location":"API%20Reference/Clients/Avaiable_Clients/mistral/#datapizza.clients.mistral.MistralClient.__init__","title":"__init__","text":"<pre><code>__init__(\n    api_key,\n    model=\"mistral-large-latest\",\n    system_prompt=\"\",\n    temperature=None,\n    cache=None,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key for the Mistral API.</p> required <code>model</code> <code>str</code> <p>The model to use for the Mistral API.</p> <code>'mistral-large-latest'</code> <code>system_prompt</code> <code>str</code> <p>The system prompt to use for the Mistral API.</p> <code>''</code> <code>temperature</code> <code>float | None</code> <p>The temperature to use for the Mistral API.</p> <code>None</code> <code>cache</code> <code>Cache | None</code> <p>The cache to use for the Mistral API.</p> <code>None</code>"},{"location":"API%20Reference/Clients/Avaiable_Clients/mistral/#usage-example","title":"Usage Example","text":"<pre><code>from datapizza.clients.mistral import MistralClient\n\nclient = MistralClient(\n    api_key=os.getenv(\"MISTRAL_API_KEY\"),\n    model=\"mistral-small-latest\",\n    system_prompt=\"You are a helpful assistant that responds short and concise.\",\n)\nresponse = client.invoke(\"hi\")\nprint(response.text)\n</code></pre>"},{"location":"API%20Reference/Clients/Avaiable_Clients/openai-like/","title":"Openai like","text":"<pre><code>pip install datapizza-ai-clients-openai-like\n</code></pre>"},{"location":"API%20Reference/Clients/Avaiable_Clients/openai-like/#datapizza.clients.openai_like.OpenAILikeClient","title":"datapizza.clients.openai_like.OpenAILikeClient","text":"<p>               Bases: <code>Client</code></p> <p>A client for interacting with the OpenAI API.</p> <p>This class provides methods for invoking the OpenAI API to generate responses based on given input data. It extends the Client class.</p>"},{"location":"API%20Reference/Clients/Avaiable_Clients/openai-like/#key-differences-from-openaiclient","title":"Key Differences from OpenAIClient","text":"<p>The main difference between OpenAILikeClient and OpenAIClient is the API endpoint they use:</p> <ul> <li>OpenAILikeClient uses the chat completions API</li> <li>OpenAIClient uses the responses API</li> </ul> <p>This makes OpenAILikeClient compatible with services that implement the OpenAI-compatible completions API, such as local models served through Ollama or other providers that follow the OpenAI API specification but only support the completions endpoint.</p>"},{"location":"API%20Reference/Clients/Avaiable_Clients/openai-like/#usage-example","title":"Usage example","text":"<pre><code>from datapizza.clients.openai_like import OpenAILikeClient\n\nclient = OpenAILikeClient(\n    api_key=\"OPENAI_API_KEY\",\n    system_prompt=\"You are a helpful assistant.\",\n)\n\nresponse = client.invoke(\"What is the capital of France?\")\nprint(response.content)\n</code></pre>"},{"location":"API%20Reference/Clients/Avaiable_Clients/openai/","title":"Openai","text":"<pre><code>pip install datapizza-ai-clients-openai\n</code></pre>"},{"location":"API%20Reference/Clients/Avaiable_Clients/openai/#datapizza.clients.openai.OpenAIClient","title":"datapizza.clients.openai.OpenAIClient","text":"<p>               Bases: <code>Client</code></p> <p>A client for interacting with the OpenAI API.</p> <p>This class provides methods for invoking the OpenAI API to generate responses based on given input data. It extends the Client class.</p>"},{"location":"API%20Reference/Clients/Avaiable_Clients/openai/#datapizza.clients.openai.OpenAIClient.__init__","title":"__init__","text":"<pre><code>__init__(\n    api_key,\n    model=\"gpt-4o-mini\",\n    system_prompt=\"\",\n    temperature=None,\n    cache=None,\n    base_url=None,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key for the OpenAI API.</p> required <code>model</code> <code>str</code> <p>The model to use for the OpenAI API.</p> <code>'gpt-4o-mini'</code> <code>system_prompt</code> <code>str</code> <p>The system prompt to use for the OpenAI API.</p> <code>''</code> <code>temperature</code> <code>float | None</code> <p>The temperature to use for the OpenAI API.</p> <code>None</code> <code>cache</code> <code>Cache | None</code> <p>The cache to use for the OpenAI API.</p> <code>None</code> <code>base_url</code> <code>str | URL | None</code> <p>The base URL for the OpenAI API.</p> <code>None</code>"},{"location":"API%20Reference/Clients/Avaiable_Clients/openai/#usage-example","title":"Usage example","text":"<pre><code>from datapizza.clients.openai import OpenAIClient\n\nclient = OpenAIClient(\n    api_key=\"YOUR_API_KEY\"\n    model=\"gpt-4o-mini\",\n)\nresponse = client.invoke(\"Hello!\")\nprint(response.text)\n</code></pre>"},{"location":"API%20Reference/Clients/Avaiable_Clients/openai/#include-thinking","title":"Include thinking","text":"<pre><code>from datapizza.clients.openai import OpenAIClient\n\nclient = OpenAIClient(\n    model= \"gpt-5\",\n    api_key=\"YOUR_API_KEY\",\n)\n\nresponse = client.invoke(\"Hi\",reasoning={\n        \"effort\": \"low\",\n        \"summary\": \"auto\"\n    }\n)\nprint(response)\n</code></pre>"},{"location":"API%20Reference/Embedders/chunk_embedder/","title":"ChunkEmbedder","text":""},{"location":"API%20Reference/Embedders/chunk_embedder/#datapizza.embedders.ChunkEmbedder","title":"datapizza.embedders.ChunkEmbedder","text":"<p>               Bases: <code>PipelineComponent</code></p> <p>ChunkEmbedder is a module that given a list of chunks, it put a list of embeddings in each chunk.</p>"},{"location":"API%20Reference/Embedders/chunk_embedder/#datapizza.embedders.ChunkEmbedder.__init__","title":"__init__","text":"<pre><code>__init__(\n    client,\n    model_name=None,\n    embedding_name=None,\n    batch_size=2047,\n)\n</code></pre> <p>Initialize the ChunkEmbedder.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>BaseEmbedder</code> <p>The client to use for embedding.</p> required <code>model_name</code> <code>str</code> <p>The model name to use for embedding. Defaults to None.</p> <code>None</code> <code>embedding_name</code> <code>str</code> <p>The name of the embedding to use. Defaults to None.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>The batch size to use for embedding. Defaults to 2047.</p> <code>2047</code>"},{"location":"API%20Reference/Embedders/chunk_embedder/#datapizza.embedders.ChunkEmbedder.a_embed","title":"a_embed  <code>async</code>","text":"<pre><code>a_embed(nodes)\n</code></pre> <p>Asynchronously embeds the given list of chunks.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>list[Chunk]</code> <p>The list of chunks to embed.</p> required <p>Returns:</p> Type Description <code>list[Chunk]</code> <p>list[Chunk]: The list of chunks with embeddings.</p>"},{"location":"API%20Reference/Embedders/chunk_embedder/#datapizza.embedders.ChunkEmbedder.embed","title":"embed","text":"<pre><code>embed(nodes)\n</code></pre> <p>Embeds the given list of chunks.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>list[Chunk]</code> <p>The list of chunks to embed.</p> required <p>Returns:</p> Type Description <code>list[Chunk]</code> <p>list[Chunk]: The list of chunks with embeddings.</p>"},{"location":"API%20Reference/Embedders/chunk_embedder/#usage","title":"Usage","text":"<pre><code>from datapizza.embedders import ChunkEmbedder\nfrom datapizza.core.clients import Client\n\n# Initialize with any compatible client\nclient = Client(...)  # Your client instance\nembedder = ChunkEmbedder(\n    client=client,\n    model_name=\"text-embedding-ada-002\",  # Optional model override\n    embedding_name=\"my_embeddings\",       # Optional custom embedding name\n    batch_size=100                        # Optional batch size for processing\n)\n\n# Embed chunks - adds embeddings to chunk objects\nembedded_chunks = embedder.embed(chunks)\n</code></pre>"},{"location":"API%20Reference/Embedders/chunk_embedder/#features","title":"Features","text":"<ul> <li>Specialized for embedding lists of Chunk objects</li> <li>Batch processing with configurable batch size</li> <li>Adds embeddings directly to Chunk objects</li> <li>Preserves original chunk structure and metadata</li> <li>Async embedding support with <code>a_embed()</code></li> <li>Memory efficient batch processing</li> <li>Works with any compatible LLM client</li> </ul>"},{"location":"API%20Reference/Embedders/chunk_embedder/#examples","title":"Examples","text":""},{"location":"API%20Reference/Embedders/chunk_embedder/#basic-chunk-embedding","title":"Basic Chunk Embedding","text":"<pre><code>import os\n\nfrom datapizza.embedders import ChunkEmbedder\nfrom datapizza.embedders.openai import OpenAIEmbedder\nfrom datapizza.type import Chunk\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Create client and embedder\nclient = OpenAIEmbedder(api_key=os.getenv(\"OPENAI_API_KEY\"))\nembedder = ChunkEmbedder(\n    client=client,\n    model_name=\"text-embedding-ada-002\",\n    batch_size=50\n)\n\n# Create sample chunks\nchunks = [\n    Chunk(id=\"1\", text=\"First chunk of text\", metadata={\"source\": \"doc1\"}),\n    Chunk(id=\"2\", text=\"Second chunk of text\", metadata={\"source\": \"doc2\"}),\n    Chunk(id=\"3\", text=\"Third chunk of text\", metadata={\"source\": \"doc3\"})\n]\n\n# Embed chunks (modifies chunks in-place)\nembedded_chunks = embedder.embed(chunks)\n\n# Check embeddings were added\nfor i, chunk in enumerate(embedded_chunks):\n    print(f\"Chunk {i+1}:\")\n    print(f\"  Text: {chunk.text[:50]}...\")\n    print(f\"  Embeddings: {len(chunk.embeddings)}\")\n    if chunk.embeddings:\n        print(f\"  Embedding name: {chunk.embeddings[0].name}\")\n        print(f\"  Vector size: {len(chunk.embeddings[0].vector)}\")\n</code></pre>"},{"location":"API%20Reference/Embedders/cohere_embedder/","title":"CohereEmbedder","text":"<pre><code>pip install datapizza-ai-embedders-cohere\n</code></pre>"},{"location":"API%20Reference/Embedders/cohere_embedder/#datapizza.embedders.cohere.CohereEmbedder","title":"datapizza.embedders.cohere.CohereEmbedder","text":"<p>               Bases: <code>BaseEmbedder</code></p>"},{"location":"API%20Reference/Embedders/cohere_embedder/#usage","title":"Usage","text":"<pre><code>from datapizza.embedders.cohere import CohereEmbedder\n\nembedder = CohereEmbedder(\n    api_key=\"your-cohere-api-key\",\n    base_url=\"https://api.cohere.ai/v1\",\n    input_type=\"search_document\"  # or \"search_query\"\n)\n\n# Embed a single text\nembedding = embedder.embed(\"Hello world\", model_name=\"embed-english-v3.0\")\n\n# Embed multiple texts\nembeddings = embedder.embed(\n    [\"Hello world\", \"Another text\"],\n    model_name=\"embed-english-v3.0\"\n)\n</code></pre>"},{"location":"API%20Reference/Embedders/cohere_embedder/#features","title":"Features","text":"<ul> <li>Supports Cohere's embedding models</li> <li>Configurable input type for search documents or queries</li> <li>Handles both single text and batch text embedding</li> <li>Async embedding support with <code>a_embed()</code></li> <li>Custom endpoint support for compatible APIs</li> <li>Uses Cohere's ClientV2 for optimal performance</li> </ul>"},{"location":"API%20Reference/Embedders/cohere_embedder/#examples","title":"Examples","text":""},{"location":"API%20Reference/Embedders/cohere_embedder/#basic-text-embedding","title":"Basic Text Embedding","text":"<pre><code>from datapizza.embedders.cohere import CohereEmbedder\n\nembedder = CohereEmbedder(\n    api_key=\"your-cohere-api-key\",\n    base_url=\"https://api.cohere.ai/v1\",\n    input_type=\"search_document\"\n)\n\n# Single text embedding\ntext = \"This is a sample document for embedding.\"\nembedding = embedder.embed(text, model_name=\"embed-english-v3.0\")\n\nprint(f\"Embedding dimensions: {len(embedding)}\")\nprint(f\"First 5 values: {embedding[:5]}\")\n</code></pre>"},{"location":"API%20Reference/Embedders/cohere_embedder/#search-query-embedding","title":"Search Query Embedding","text":"<pre><code>from datapizza.embedders.cohere import CohereEmbedder\n\n# Configure for search queries\nembedder = CohereEmbedder(\n    api_key=\"your-cohere-api-key\",\n    base_url=\"https://api.cohere.ai/v1\",\n    input_type=\"search_query\"\n)\n\nquery = \"What is machine learning?\"\nembedding = embedder.embed(query, model_name=\"embed-english-v3.0\")\n\nprint(f\"Query embedding size: {len(embedding)}\")\n</code></pre>"},{"location":"API%20Reference/Embedders/cohere_embedder/#batch-text-embedding","title":"Batch Text Embedding","text":"<pre><code>from datapizza.embedders.cohere import CohereEmbedder\n\nembedder = CohereEmbedder(\n    api_key=\"your-cohere-api-key\",\n    base_url=\"https://api.cohere.ai/v1\"\n)\n\ntexts = [\n    \"First document to embed\",\n    \"Second document to embed\",\n    \"Third document to embed\"\n]\n\nembeddings = embedder.embed(texts, model_name=\"embed-english-v3.0\")\n\nfor i, emb in enumerate(embeddings):\n    print(f\"Document {i+1} embedding size: {len(emb)}\")\n</code></pre>"},{"location":"API%20Reference/Embedders/cohere_embedder/#async-embedding","title":"Async Embedding","text":"<pre><code>import asyncio\nfrom datapizza.embedders.cohere import CohereEmbedder\n\nasync def embed_async():\n    embedder = CohereEmbedder(\n        api_key=\"your-cohere-api-key\",\n        base_url=\"https://api.cohere.ai/v1\"\n    )\n\n    text = \"Async embedding example\"\n    embedding = await embedder.a_embed(text, model_name=\"embed-english-v3.0\")\n\n    return embedding\n\n# Run async function\nembedding = asyncio.run(embed_async())\n</code></pre>"},{"location":"API%20Reference/Embedders/fast_embedder/","title":"FastEmbedder","text":"<pre><code>pip install datapizza-ai-embedders-fastembedder\n</code></pre>"},{"location":"API%20Reference/Embedders/fast_embedder/#datapizza.embedders.fastembedder.FastEmbedder","title":"datapizza.embedders.fastembedder.FastEmbedder","text":"<p>               Bases: <code>BaseEmbedder</code></p>"},{"location":"API%20Reference/Embedders/fast_embedder/#usage","title":"Usage","text":"<pre><code>from datapizza.embedders.fastembedder import FastEmbedder\n\nembedder = FastEmbedder(\n    model_name=\"Qdrant/bm25\",\n    embedding_name=\"bm25_embeddings\",\n)\n\n# Embed text (returns sparse embeddings)\nembeddings = embedder.embed([\"Hello world\", \"Another text\"])\nprint(embeddings)\n</code></pre>"},{"location":"API%20Reference/Embedders/fast_embedder/#features","title":"Features","text":"<ul> <li>Uses FastEmbed for efficient sparse text embeddings</li> <li>Local model execution (no API calls required)</li> <li>Configurable model caching directory</li> <li>Custom embedding naming</li> <li>Sparse embedding format for memory efficiency</li> <li>Both sync and async embedding support</li> </ul>"},{"location":"API%20Reference/Embedders/google_embedder/","title":"GoogleEmbedder","text":"<pre><code>pip install datapizza-ai-embedders-google\n</code></pre>"},{"location":"API%20Reference/Embedders/google_embedder/#datapizza.embedders.google.GoogleEmbedder","title":"datapizza.embedders.google.GoogleEmbedder","text":"<p>               Bases: <code>BaseEmbedder</code></p>"},{"location":"API%20Reference/Embedders/google_embedder/#usage","title":"Usage","text":"<pre><code>from datapizza.embedders.google import GoogleEmbedder\n\nembedder = GoogleEmbedder(\n    api_key=\"your-google-api-key\"\n)\n\n# Embed a single text\nembedding = embedder.embed(\"Hello world\", model_name=\"models/embedding-001\")\n\n# Embed multiple texts\nembeddings = embedder.embed(\n    [\"Hello world\", \"Another text\"],\n    model_name=\"models/embedding-001\"\n)\n</code></pre>"},{"location":"API%20Reference/Embedders/google_embedder/#features","title":"Features","text":"<ul> <li>Supports Google's Gemini embedding models</li> <li>Handles both single text and batch text embedding</li> <li>Async embedding support with <code>a_embed()</code></li> <li>Automatic client initialization and management</li> <li>Uses Google's Generative AI SDK</li> </ul>"},{"location":"API%20Reference/Embedders/google_embedder/#examples","title":"Examples","text":""},{"location":"API%20Reference/Embedders/google_embedder/#basic-text-embedding","title":"Basic Text Embedding","text":"<pre><code>from datapizza.embedders.google import GoogleEmbedder\n\nembedder = GoogleEmbedder(api_key=\"your-google-api-key\")\n\n# Single text embedding\ntext = \"This is a sample document for embedding.\"\nembedding = embedder.embed(text, model_name=\"models/embedding-001\")\n\nprint(f\"Embedding dimensions: {len(embedding)}\")\nprint(f\"First 5 values: {embedding[:5]}\")\n</code></pre>"},{"location":"API%20Reference/Embedders/google_embedder/#async-embedding","title":"Async Embedding","text":"<pre><code>import asyncio\nfrom datapizza.embedders.google import GoogleEmbedder\n\nasync def embed_async():\n    embedder = GoogleEmbedder(api_key=\"your-google-api-key\")\n\n    text = \"Async embedding example\"\n    embedding = await embedder.a_embed(text, model_name=\"models/embedding-001\")\n\n    return embedding\n\n# Run async function\nembedding = asyncio.run(embed_async())\n</code></pre>"},{"location":"API%20Reference/Embedders/openai_embedder/","title":"OpenAIEmbedder","text":""},{"location":"API%20Reference/Embedders/openai_embedder/#datapizza.embedders.openai.OpenAIEmbedder","title":"datapizza.embedders.openai.OpenAIEmbedder","text":"<p>               Bases: <code>BaseEmbedder</code></p>"},{"location":"API%20Reference/Embedders/openai_embedder/#usage","title":"Usage","text":"<pre><code>from datapizza.embedders.openai import OpenAIEmbedder\n\nembedder = OpenAIEmbedder(\n    api_key=\"your-openai-api-key\",\n    base_url=\"https://api.openai.com/v1\"  # Optional custom base URL\n)\n\n# Embed a single text\nembedding = embedder.embed(\"Hello world\", model_name=\"text-embedding-ada-002\")\n\n# Embed multiple texts\nembeddings = embedder.embed(\n    [\"Hello world\", \"Another text\"],\n    model_name=\"text-embedding-ada-002\"\n)\n</code></pre>"},{"location":"API%20Reference/Embedders/openai_embedder/#features","title":"Features","text":"<ul> <li>Supports OpenAI's embedding models</li> <li>Handles both single text and batch text embedding</li> <li>Async embedding support with <code>a_embed()</code></li> <li>Custom base URL support for compatible APIs</li> <li>Automatic client initialization and management</li> </ul>"},{"location":"API%20Reference/Embedders/openai_embedder/#examples","title":"Examples","text":""},{"location":"API%20Reference/Embedders/openai_embedder/#basic-text-embedding","title":"Basic Text Embedding","text":"<pre><code>from datapizza.embedders.openai import OpenAIEmbedder\n\nembedder = OpenAIEmbedder(api_key=\"your-api-key\")\n\n# Single text embedding\ntext = \"This is a sample document for embedding.\"\nembedding = embedder.embed(text, model_name=\"text-embedding-ada-002\")\n\nprint(f\"Embedding dimensions: {len(embedding)}\")\nprint(f\"First 5 values: {embedding[:5]}\")\n</code></pre>"},{"location":"API%20Reference/Embedders/openai_embedder/#async-embedding","title":"Async Embedding","text":"<pre><code>import asyncio\nfrom datapizza.embedders.openai import OpenAIEmbedder\n\nasync def embed_async():\n    embedder = OpenAIEmbedder(api_key=\"your-api-key\")\n\n    text = \"Async embedding example\"\n    embedding = await embedder.a_embed(text, model_name=\"text-embedding-ada-002\")\n\n    return embedding\n\n# Run async function\nembedding = asyncio.run(embed_async())\n</code></pre>"},{"location":"API%20Reference/Modules/","title":"Modules","text":"<p>This section contains API reference documentation for all datapizza-ai modules. Modules are organized by functionality and include both core modules (included by default) and optional modules that require separate installation.</p>"},{"location":"API%20Reference/Modules/#core-modules-included-by-default","title":"Core Modules (Included by Default)","text":"<p>These modules are included with <code>datapizza-ai-core</code> and are available without additional installation:</p> <ul> <li>Parsers - Convert documents into structured Node representations</li> <li>Captioners - Generate captions and descriptions for content</li> <li>Metatagger - Add metadata tags to content</li> <li>Prompt - Manage prompts and prompt templates</li> <li>Rewriters - Transform and rewrite content</li> <li>Splitters - Split content into smaller chunks</li> <li>Treebuilder - Build hierarchical tree structures from content</li> </ul>"},{"location":"API%20Reference/Modules/#optional-modules-separate-installation-required","title":"Optional Modules (Separate Installation Required)","text":"<p>These modules require separate installation via pip:</p> <ul> <li>Rerankers - Rerank and score content relevance</li> </ul> <p>Each module page includes installation instructions and usage examples.</p>"},{"location":"API%20Reference/Modules/captioners/","title":"Captioners","text":"<p>Captioners are pipeline components that generate captions and descriptions for media content such as images, figures, and tables. They use LLM clients to analyze visual content and produce descriptive text.</p>"},{"location":"API%20Reference/Modules/captioners/#llmcaptioner","title":"LLMCaptioner","text":"<p>A captioner that uses language models to generate captions for media nodes (figures and tables) within document hierarchies.</p> <pre><code>from datapizza.clients.openai import OpenAIClient\nfrom datapizza.modules.captioners import LLMCaptioner\nfrom datapizza.type import ROLE, Media, MediaNode, NodeType\n\nclient = OpenAIClient(api_key=\"OPENAI_API_KEY\", model=\"gpt-4o\")\ncaptioner = LLMCaptioner(\n    client=client,\n    max_workers=3,\n    system_prompt_table=\"Describe this table in detail.\",\n    system_prompt_figure=\"Describe this figure/image in detail.\"\n)\n\ndocument_node = MediaNode( node_type=NodeType.FIGURE, children=[], metadata={}, media=Media(source_type=\"path\", source=\"gogole.png\", extension=\"png\", media_type=\"image\"))\ncaptioned_document = captioner(document_node)\nprint(captioned_document)\n</code></pre> <p>Parameters:</p> <ul> <li><code>client</code> (Client): The LLM client to use for caption generation</li> <li><code>max_workers</code> (int): Maximum number of concurrent workers for parallel processing (default: 3)</li> <li><code>system_prompt_table</code> (str, optional): System prompt for table captioning</li> <li><code>system_prompt_figure</code> (str, optional): System prompt for figure captioning</li> </ul> <p>Features:</p> <ul> <li>Automatically finds all media nodes (figures and tables) in a document hierarchy</li> <li>Generates captions using configurable system prompts</li> <li>Supports concurrent processing for better performance</li> <li>Creates new paragraph nodes containing the original content plus generated captions</li> <li>Preserves original node metadata and structure</li> <li>Supports both sync and async processing</li> </ul> <p>Supported Node Types:</p> <ul> <li><code>FIGURE</code>: Images and visual figures</li> <li><code>TABLE</code>: Tables and tabular data</li> </ul> <p>Output Format:</p> <p>The captioner creates new paragraph nodes with content in the format: <pre><code>{original_content} &lt;{node_type}&gt; [{generated_caption}]\n</code></pre></p>"},{"location":"API%20Reference/Modules/captioners/#datapizza.modules.captioners.LLMCaptioner","title":"datapizza.modules.captioners.LLMCaptioner","text":"<p>               Bases: <code>NodeCaptioner</code></p> <p>Captioner that uses an LLM client to caption a node.</p>"},{"location":"API%20Reference/Modules/captioners/#datapizza.modules.captioners.LLMCaptioner.__init__","title":"__init__","text":"<pre><code>__init__(\n    client,\n    max_workers=3,\n    system_prompt_table=\"Generate concise captions for tables.\",\n    system_prompt_figure=\"Generate descriptive captions for figures.\",\n)\n</code></pre> <p>Captioner that uses an LLM client to caption a node. Args:     client: The LLM client to use.     max_workers: The maximum number of workers to use. in sync mode is the number of threads spawned, in async mode is the number of batches.     system_prompt_table: The system prompt to use for table captioning.     system_prompt_figure: The system prompt to use for figure captioning.</p>"},{"location":"API%20Reference/Modules/captioners/#datapizza.modules.captioners.LLMCaptioner.a_caption","title":"a_caption  <code>async</code>","text":"<pre><code>a_caption(node)\n</code></pre> <p>async Caption a node. Args:     node: The node to caption.</p> <p>Returns:</p> Type Description <code>Node</code> <p>The same node with the caption.</p>"},{"location":"API%20Reference/Modules/captioners/#datapizza.modules.captioners.LLMCaptioner.a_caption_media","title":"a_caption_media  <code>async</code>","text":"<pre><code>a_caption_media(media, system_prompt=None)\n</code></pre> <p>async Caption image. Args:     media: The media to caption.     system_prompt: Optional system prompt to guide the captioning.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string caption.</p>"},{"location":"API%20Reference/Modules/captioners/#datapizza.modules.captioners.LLMCaptioner.caption","title":"caption","text":"<pre><code>caption(node)\n</code></pre> <p>Caption a node. Args:     node: The node to caption.</p> <p>Returns:</p> Type Description <code>Node</code> <p>The same node with the caption.</p>"},{"location":"API%20Reference/Modules/captioners/#datapizza.modules.captioners.LLMCaptioner.caption_media","title":"caption_media","text":"<pre><code>caption_media(media, system_prompt=None)\n</code></pre> <p>Caption an image. Args:     media: The media to caption.     system_prompt: Optional system prompt to guide the captioning.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string caption.</p>"},{"location":"API%20Reference/Modules/metatagger/","title":"Metatagger","text":"<p>Metataggers are pipeline components that add metadata tags to content chunks using language models. They analyze text content and generate relevant keywords, tags, or other metadata to enhance content discoverability and organization.</p> <p>A metatagger that uses language models to generate keywords and metadata for text chunks.</p> <pre><code>from datapizza.modules.metatagger import KeywordMetatagger\nfrom datapizza.clients.openai import OpenAIClient\n\nclient = OpenAIClient(api_key=\"your-api-key\")\nmetatagger = KeywordMetatagger(\n    client=client,\n    max_workers=3,\n    system_prompt=\"Generate relevant keywords for the given text.\",\n    user_prompt=\"Extract 5-10 keywords from this text:\",\n    keyword_name=\"keywords\"\n)\n\n# Process chunks\ntagged_chunks = metatagger.tag(chunks)\n</code></pre> <p>Features:</p> <ul> <li>Processes chunks in parallel for better performance</li> <li>Configurable prompts for different keyword extraction strategies</li> <li>Adds generated keywords to chunk metadata</li> <li>Supports custom metadata field naming</li> <li>Handles both individual chunks and lists of chunks</li> <li>Uses memory-based conversation for consistent prompting</li> </ul> <p>Input/Output:</p> <ul> <li>Input: <code>Chunk</code> objects or lists of <code>Chunk</code> objects</li> <li>Output: Same <code>Chunk</code> objects with additional metadata containing generated keywords</li> </ul>"},{"location":"API%20Reference/Modules/metatagger/#datapizza.modules.metatagger.KeywordMetatagger","title":"datapizza.modules.metatagger.KeywordMetatagger","text":"<p>               Bases: <code>Metatagger</code></p> <p>Keyword metatagger that uses an LLM client to add metadata to a chunk.</p>"},{"location":"API%20Reference/Modules/metatagger/#datapizza.modules.metatagger.KeywordMetatagger.__init__","title":"__init__","text":"<pre><code>__init__(\n    client,\n    max_workers=3,\n    system_prompt=None,\n    user_prompt=None,\n    keyword_name=\"keywords\",\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>The LLM client to use.</p> required <code>max_workers</code> <code>int</code> <p>The maximum number of workers to use.</p> <code>3</code> <code>system_prompt</code> <code>str | None</code> <p>The system prompt to use.</p> <code>None</code> <code>user_prompt</code> <code>str | None</code> <p>The user prompt to use.</p> <code>None</code> <code>keyword_name</code> <code>str</code> <p>The name of the keyword field.</p> <code>'keywords'</code>"},{"location":"API%20Reference/Modules/metatagger/#datapizza.modules.metatagger.KeywordMetatagger.a_tag","title":"a_tag  <code>async</code>","text":"<pre><code>a_tag(chunks)\n</code></pre> <p>async Add metadata to a chunk.</p>"},{"location":"API%20Reference/Modules/metatagger/#datapizza.modules.metatagger.KeywordMetatagger.tag","title":"tag","text":"<pre><code>tag(chunks)\n</code></pre> <p>Add metadata to a chunk.</p>"},{"location":"API%20Reference/Modules/metatagger/#usage-examples","title":"Usage Examples","text":""},{"location":"API%20Reference/Modules/metatagger/#basic-keyword-extraction","title":"Basic Keyword Extraction","text":"<pre><code>import uuid\n\nfrom datapizza.clients.openai import OpenAIClient\nfrom datapizza.modules.metatagger import KeywordMetatagger\nfrom datapizza.type import Chunk\n\n# Initialize client and metatagger\nclient = OpenAIClient(api_key=\"OPENAI_API_KEY\", model=\"gpt-4o\")\nmetatagger = KeywordMetatagger(\n    client=client,\n    system_prompt=\"You are a keyword extraction expert. Generate relevant, concise keywords.\",\n    user_prompt=\"Extract 5-8 important keywords from this text:\",\n    keyword_name=\"keywords\"\n)\n\n# Process chunks\nchunks = [\n    Chunk(id=str(uuid.uuid4()), text=\"Machine learning algorithms are transforming healthcare diagnostics.\"),\n    Chunk(id=str(uuid.uuid4()), text=\"Climate change impacts ocean temperatures and marine ecosystems.\")\n]\n\ntagged_chunks = metatagger.tag(chunks)\n\n# Access generated keywords\nfor chunk in tagged_chunks:\n    print(f\"Content: {chunk.text}\")\n    print(f\"Keywords: {chunk.metadata.get('keywords', [])}\")\n</code></pre>"},{"location":"API%20Reference/Modules/rewriters/","title":"Rewriters","text":"<p>Rewriters are pipeline components that transform and rewrite text content using language models. They can modify content style, format, tone, or structure while preserving meaning and important information.</p> <p>A rewriter that uses language models to transform text content with specific instructions and tools.</p> <pre><code>from datapizza.clients.openai import OpenAIClient\nfrom datapizza.modules.rewriters import ToolRewriter\n\nclient = OpenAIClient(api_key=\"OPENAI_API_KEY\", model=\"gpt-4o\")\n\n# Create a simplification rewriter\nsimplifier = ToolRewriter(\n    client=client,\n    system_prompt=\"You are an expert at simplifying complex text for general audiences.\",\n)\n\n# Simplify technical content\ntechnical_text = \"\"\"\nThe algorithmic implementation utilizes a recursive binary search methodology\nto optimize computational complexity in logarithmic time scenarios.\n\"\"\"\n\nsimplified_text = simplifier(technical_text)\nprint(simplified_text)\n# Output: recursive binary search\n</code></pre> <p>Features:</p> <ul> <li>Flexible content transformation with custom instructions</li> <li>Support for various rewriting tasks (summarization, style changes, format conversion)</li> <li>Integration with tool calling for enhanced capabilities</li> <li>Preserves important information while transforming presentation</li> <li>Supports both sync and async processing</li> <li>Configurable prompting for different rewriting strategies</li> </ul>"},{"location":"API%20Reference/Modules/rewriters/#datapizza.modules.rewriters.ToolRewriter","title":"datapizza.modules.rewriters.ToolRewriter","text":"<p>               Bases: <code>Rewriter</code></p> <p>A tool-based query rewriter that uses LLMs to transform user queries through structured tool interactions.</p>"},{"location":"API%20Reference/Modules/rewriters/#datapizza.modules.rewriters.ToolRewriter.a_rewrite","title":"a_rewrite  <code>async</code>","text":"<pre><code>a_rewrite(user_prompt, memory=None)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>user_prompt</code> <code>str</code> <p>The user query to rewrite.</p> required <code>memory</code> <code>Memory | None</code> <p>The memory to use for the rewrite.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The rewritten query.</p>"},{"location":"API%20Reference/Modules/rewriters/#datapizza.modules.rewriters.ToolRewriter.rewrite","title":"rewrite","text":"<pre><code>rewrite(user_prompt, memory=None)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>user_prompt</code> <code>str</code> <p>The user query to rewrite.</p> required <code>memory</code> <code>Memory | None</code> <p>The memory to use for the rewrite.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The rewritten query.</p>"},{"location":"API%20Reference/Modules/treebuilder/","title":"Treebuilder","text":"<p>Treebuilders are pipeline components that construct hierarchical tree structures (Node objects) from various types of content. They convert flat or unstructured content into organized, nested representations that facilitate better processing and understanding.</p> <p>A treebuilder that uses language models to analyze content and create hierarchical structures based on semantic understanding.</p> <pre><code>from datapizza.clients.openai import OpenAIClient\nfrom datapizza.modules.treebuilder import LLMTreeBuilder\n\nclient = OpenAIClient(api_key=\"OPENAI_API_KEY\", model=\"gpt-4o\")\ntreebuilder = LLMTreeBuilder(client=client)\n\nflat_content = \"This is a flat piece of content. It should be converted into a hierarchical structure.\"\n\nstructured_document = treebuilder.parse(flat_content)\n\nprint(structured_document)\n</code></pre> <p>Features:</p> <ul> <li>Semantic understanding of content organization</li> <li>Configurable tree depth and structure rules</li> <li>Support for various content types (articles, reports, manuals, etc.)</li> <li>Preserves original content while adding hierarchical organization</li> <li>Metadata extraction and tagging during structure creation</li> <li>Supports both sync and async processing</li> </ul>"},{"location":"API%20Reference/Modules/treebuilder/#datapizza.modules.treebuilder.LLMTreeBuilder","title":"datapizza.modules.treebuilder.LLMTreeBuilder","text":"<p>TreeBuilder that creates a hierarchical tree structure from text input using an LLM. The hierarchy goes from document -&gt; sections -&gt; paragraphs -&gt; sentences.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>Client - An instance of an LLM client (e.g., GeminiClient)</p> required"},{"location":"API%20Reference/Modules/treebuilder/#datapizza.modules.treebuilder.LLMTreeBuilder.invoke","title":"invoke","text":"<pre><code>invoke(file_path)\n</code></pre> <p>Invoke the tree builder on the input text using an LLM.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the file to process</p> required <p>Returns:</p> Type Description <code>Node</code> <p>A Node representing the document with hierarchical structure</p>"},{"location":"API%20Reference/Modules/treebuilder/#datapizza.modules.treebuilder.LLMTreeBuilder.parse","title":"parse","text":"<pre><code>parse(text)\n</code></pre> <p>Build a tree from the input text using an LLM.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text to process</p> required <p>Returns:</p> Type Description <code>Node</code> <p>A Node representing the document with hierarchical structure</p>"},{"location":"API%20Reference/Modules/treebuilder/#usage-examples","title":"Usage Examples","text":""},{"location":"API%20Reference/Modules/treebuilder/#basic-tree-structure-creation","title":"Basic Tree Structure Creation","text":"<pre><code>from datapizza.modules.treebuilder import LLMTreeBuilder\nfrom datapizza.clients.openai import OpenAIClient\n\nclient = OpenAIClient(api_key=\"your-openai-key\")\n\n# Create basic treebuilder\ntreebuilder = LLMTreeBuilder(client=client)\n\n# Unstructured content\nflat_content = \"\"\"\nIntroduction to Machine Learning\n\nMachine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. There are three main types of machine learning: supervised learning, unsupervised learning, and reinforcement learning.\n\nSupervised Learning\nIn supervised learning, algorithms learn from labeled training data. The goal is to predict outcomes for new data based on patterns learned from the training set. Common examples include classification and regression tasks.\n\nClassification algorithms predict discrete categories or classes. For example, email spam detection classifies emails as either spam or not spam.\n\nRegression algorithms predict continuous numerical values. For instance, predicting house prices based on features like location and size.\n\nUnsupervised Learning\nUnsupervised learning finds patterns in data without labeled examples. The algorithm identifies hidden structures in input data.\n\nClustering groups similar data points together. Customer segmentation is a common application.\n\nDimensionality reduction reduces the number of features while preserving important information.\n\nReinforcement Learning\nThis approach learns through interaction with an environment, receiving rewards or penalties for actions taken.\n\"\"\"\n\n# Build hierarchical structure\nstructured_document = treebuilder.parse(flat_content)\n\n# Navigate the structure\ndef print_structure(node, depth=0):\n    indent = \"  \" * depth\n    print(f\"{indent}{node.node_type.value}: {node.content[:50]}...\")\n    for child in node.children:\n        print_structure(child, depth + 1)\n\nprint_structure(structured_document)\n</code></pre>"},{"location":"API%20Reference/Modules/Parsers/","title":"Parsers","text":"<p>Parsers are pipeline components that convert documents into structured hierarchical Node representations. They extract text, layout information, and metadata from various document formats to create tree-like data structures for further processing.</p> <p>Each parser should return a Node object, which is a hierarchical representation of the document content.</p> <p>If you write a custom parser that returns a different type of object (for example, the plain text of the document content), you must use a TreeBuilder to convert it into a Node.</p>"},{"location":"API%20Reference/Modules/Parsers/#available-parsers","title":"Available Parsers","text":""},{"location":"API%20Reference/Modules/Parsers/#core-parsers-included-by-default","title":"Core Parsers (Included by Default)","text":"<ul> <li>TextParser - Simple text parser for plain text content</li> </ul>"},{"location":"API%20Reference/Modules/Parsers/#optional-parsers-separate-installation-required","title":"Optional Parsers (Separate Installation Required)","text":"<ul> <li>AzureParser - Azure AI Document Intelligence parser for PDFs and documents</li> <li>DoclingParser - Docling-based parser for PDFs with layout preservation and media extraction</li> </ul>"},{"location":"API%20Reference/Modules/Parsers/#common-usage-patterns","title":"Common Usage Patterns","text":""},{"location":"API%20Reference/Modules/Parsers/#basic-text-processing","title":"Basic Text Processing","text":"<pre><code>from datapizza.modules.parsers.text_parser import parse_text\n\n# Process plain text\ndocument = parse_text(\"Your text content here\")\n</code></pre>"},{"location":"API%20Reference/Modules/Parsers/#document-processing-pipeline","title":"Document Processing Pipeline","text":"<pre><code>from datapizza.modules.parsers import TextParser\nfrom datapizza.modules.splitters import RecursiveSplitter\n\n# Create processing pipeline\nparser = TextParser()\nsplitter = RecursiveSplitter(chunk_size=1000)\n\n# Process document\ndocument = parser.parse(text_content)\nchunks = splitter(document.content)\n</code></pre>"},{"location":"API%20Reference/Modules/Parsers/azure_parser/","title":"AzureParser","text":"<p>A document parser that uses Azure AI Document Intelligence to extract structured content from PDFs and other documents.</p>"},{"location":"API%20Reference/Modules/Parsers/azure_parser/#installation","title":"Installation","text":"<pre><code>pip install datapizza-ai-parsers-azure\n</code></pre>"},{"location":"API%20Reference/Modules/Parsers/azure_parser/#datapizza.modules.parsers.azure.AzureParser","title":"datapizza.modules.parsers.azure.AzureParser","text":"<p>               Bases: <code>Parser</code></p> <p>Parser that creates a hierarchical tree structure from Azure AI Document Intelligence response. The hierarchy goes from document -&gt; pages -&gt; paragraphs/tables -&gt; lines/cells -&gt; words.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>str</p> required <code>endpoint</code> <code>str</code> <p>str</p> required <code>result_type</code> <code>str</code> <p>str = \"markdown\", \"text\"</p> <code>'text'</code>"},{"location":"API%20Reference/Modules/Parsers/azure_parser/#datapizza.modules.parsers.azure.AzureParser.parse","title":"parse","text":"<pre><code>parse(file_path)\n</code></pre> <p>Parse a Document with Azure AI Document Intelligence into a Node structure.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the document</p> required <p>Returns:</p> Type Description <code>Node</code> <p>A Node representing the document with hierarchical structure</p>"},{"location":"API%20Reference/Modules/Parsers/azure_parser/#datapizza.modules.parsers.azure.AzureParser.parse_with_azure_ai","title":"parse_with_azure_ai","text":"<pre><code>parse_with_azure_ai(file_path)\n</code></pre> <p>Parse a Document with Azure AI Document Intelligence into a json dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the document</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary with the Azure AI Document Intelligence response</p>"},{"location":"API%20Reference/Modules/Parsers/azure_parser/#usage","title":"Usage","text":"<pre><code>from datapizza.modules.parsers.azure import AzureParser\n\nparser = AzureParser(\n    api_key=\"your-azure-key\",\n    endpoint=\"https://your-endpoint.cognitiveservices.azure.com/\",\n    result_type=\"text\"\n)\n\ndocument_node = parser.parse(\"document.pdf\")\n</code></pre>"},{"location":"API%20Reference/Modules/Parsers/azure_parser/#parameters","title":"Parameters","text":"<ul> <li><code>api_key</code> (str): Azure AI Document Intelligence API key</li> <li><code>endpoint</code> (str): Azure service endpoint URL</li> <li><code>result_type</code> (str): Output format - \"text\" or \"markdown\" (default: \"text\")</li> </ul>"},{"location":"API%20Reference/Modules/Parsers/azure_parser/#features","title":"Features","text":"<ul> <li>Creates hierarchical document structure: document \u2192 sections \u2192 paragraphs/tables/figures</li> <li>Extracts bounding regions and spatial layout information</li> <li>Handles tables, figures, and complex document layouts</li> <li>Preserves metadata including page numbers and coordinates</li> <li>Supports both sync and async processing</li> <li>Converts media elements to base64 images with coordinates</li> </ul>"},{"location":"API%20Reference/Modules/Parsers/azure_parser/#node-types-created","title":"Node Types Created","text":"<ul> <li><code>DOCUMENT</code>: Root document container</li> <li><code>SECTION</code>: Document sections</li> <li><code>PARAGRAPH</code>: Text paragraphs with content</li> <li><code>TABLE</code>: Tables with markdown representation</li> <li><code>FIGURE</code>: Images and figures with media data</li> </ul>"},{"location":"API%20Reference/Modules/Parsers/azure_parser/#examples","title":"Examples","text":""},{"location":"API%20Reference/Modules/Parsers/azure_parser/#basic-document-processing","title":"Basic Document Processing","text":"<pre><code>from datapizza.modules.parsers.azure import AzureParser\nimport os\n\nparser = AzureParser(\n    api_key=os.getenv(\"AZURE_DOC_INTELLIGENCE_KEY\"),\n    endpoint=os.getenv(\"AZURE_DOC_INTELLIGENCE_ENDPOINT\"),\n    result_type=\"markdown\"\n)\n\n# Parse document\ndocument = parser.parse(\"complex_document.pdf\")\n\n# Access hierarchical structure\nfor section in document.children:\n    for paragraph in section.children:\n        print(f\"Content: {paragraph.content}\")\n        print(f\"Bounding regions: {paragraph.metadata.get('boundingRegions', [])}\")\n</code></pre>"},{"location":"API%20Reference/Modules/Parsers/azure_parser/#async-processing","title":"Async Processing","text":"<pre><code>async def process_document():\n    document = await parser.a_run(\"document.pdf\")\n    return document\n\n# Usage in async context\ndocument = await process_document()\n</code></pre>"},{"location":"API%20Reference/Modules/Parsers/docling_parser/","title":"DoclingParser","text":"<p>A document parser that uses Docling to convert PDF files into structured hierarchical Node representations with preserved layout information and media extraction.</p>"},{"location":"API%20Reference/Modules/Parsers/docling_parser/#installation","title":"Installation","text":"<pre><code>pip install datapizza-ai-parsers-docling\n</code></pre>"},{"location":"API%20Reference/Modules/Parsers/docling_parser/#usage","title":"Usage","text":"<pre><code>from datapizza.modules.parsers.docling import DoclingParser\n\n# Basic usage\nparser = DoclingParser()\ndocument_node = parser.parse(\"sample.pdf\")\n\nprint(document_node)\n</code></pre>"},{"location":"API%20Reference/Modules/Parsers/docling_parser/#parameters","title":"Parameters","text":"<ul> <li><code>json_output_dir</code> (str, optional): Directory to save intermediate Docling JSON results for debugging and inspection</li> </ul>"},{"location":"API%20Reference/Modules/Parsers/docling_parser/#features","title":"Features","text":"<ul> <li>PDF Processing: Converts PDF files using Docling's DocumentConverter with OCR and table structure detection</li> <li>Hierarchical Structure: Creates logical document hierarchy (document \u2192 sections \u2192 paragraphs/tables/figures)</li> <li>Media Extraction: Extracts images and tables as base64-encoded media with bounding box coordinates</li> <li>Layout Preservation: Maintains spatial layout information including page numbers and bounding regions</li> <li>Markdown Generation: Converts tables to markdown format and handles list structures</li> <li>Metadata Rich: Preserves full Docling metadata in <code>docling_raw</code> with convenience fields</li> </ul>"},{"location":"API%20Reference/Modules/Parsers/docling_parser/#configuration","title":"Configuration","text":"<p>The parser automatically configures Docling with:</p> <ul> <li>Table structure detection enabled</li> <li>Full page OCR with EasyOCR</li> <li>PyPdfium backend for PDF processing</li> </ul>"},{"location":"API%20Reference/Modules/Parsers/docling_parser/#examples","title":"Examples","text":""},{"location":"API%20Reference/Modules/Parsers/docling_parser/#basic-document-processing","title":"Basic Document Processing","text":"<pre><code>from datapizza.modules.parsers.docling import DoclingParser\n\nparser = DoclingParser()\ndocument = parser.parse(\"research_paper.pdf\")\n\n# Access hierarchical structure\nfor section in document.children:\n    print(f\"Section: {section.metadata.get('docling_label', 'Unknown')}\")\n    for child in section.children:\n        if child.node_type.name == \"PARAGRAPH\":\n            print(f\"  Paragraph: {child.content[:100]}...\")\n        elif child.node_type.name == \"TABLE\":\n            print(f\"  Table with {len(child.children)} rows\")\n        elif child.node_type.name == \"FIGURE\":\n            print(f\"  Figure: {child.metadata.get('docling_label', 'Image')}\")\n</code></pre>"},{"location":"API%20Reference/Modules/Parsers/text_parser/","title":"TextParser","text":""},{"location":"API%20Reference/Modules/Parsers/text_parser/#datapizza.modules.parsers.TextParser","title":"datapizza.modules.parsers.TextParser","text":"<p>               Bases: <code>Parser</code></p> <p>Parser that creates a hierarchical tree structure from text. The hierarchy goes from document -&gt; paragraphs -&gt; sentences.</p>"},{"location":"API%20Reference/Modules/Parsers/text_parser/#datapizza.modules.parsers.TextParser.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize the TextParser.</p>"},{"location":"API%20Reference/Modules/Parsers/text_parser/#datapizza.modules.parsers.TextParser.parse","title":"parse","text":"<pre><code>parse(text, metadata=None)\n</code></pre> <p>Parse text into a hierarchical tree structure.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to parse</p> required <code>metadata</code> <code>dict | None</code> <p>Optional metadata for the root node</p> <code>None</code> <p>Returns:</p> Type Description <code>Node</code> <p>A Node representing the document with paragraph and sentence nodes</p>"},{"location":"API%20Reference/Modules/Parsers/text_parser/#usage","title":"Usage","text":"<pre><code>from datapizza.modules.parsers.text_parser import TextParser, parse_text\n\n# Using the class\nparser = TextParser()\ndocument_node = parser.parse(\"Your text content here\", metadata={\"source\": \"example\"})\n\n# Using the convenience function\ndocument_node = parse_text(\"Your text content here\")\n</code></pre>"},{"location":"API%20Reference/Modules/Parsers/text_parser/#parameters","title":"Parameters","text":"<p>The TextParser class takes no initialization parameters.</p> <p>The <code>parse</code> method accepts: - <code>text</code> (str): The text content to parse - <code>metadata</code> (dict, optional): Additional metadata to attach to the document</p>"},{"location":"API%20Reference/Modules/Parsers/text_parser/#features","title":"Features","text":"<ul> <li>Splits text into paragraphs based on double newlines</li> <li>Breaks paragraphs into sentences using regex patterns</li> <li>Creates three-level hierarchy: document \u2192 paragraphs \u2192 sentences</li> <li>Preserves original text content in sentence nodes</li> <li>Adds index metadata for paragraphs and sentences</li> </ul>"},{"location":"API%20Reference/Modules/Parsers/text_parser/#node-types-created","title":"Node Types Created","text":"<ul> <li><code>DOCUMENT</code>: Root document container</li> <li><code>PARAGRAPH</code>: Text paragraphs</li> <li><code>SENTENCE</code>: Individual sentences with content</li> </ul>"},{"location":"API%20Reference/Modules/Parsers/text_parser/#examples","title":"Examples","text":""},{"location":"API%20Reference/Modules/Parsers/text_parser/#basic-usage","title":"Basic Usage","text":"<pre><code>from datapizza.modules.parsers.text_parser import parse_text\n\ntext_content = \"\"\"\nThis is the first paragraph.\nIt contains multiple sentences.\n\nThis is the second paragraph.\nIt also has content.\n\"\"\"\n\ndocument = parse_text(text_content, metadata={\"source\": \"user_input\"})\n\n# Navigate structure\nfor i, paragraph in enumerate(document.children):\n    print(f\"Paragraph {i}:\")\n    for j, sentence in enumerate(paragraph.children):\n        print(f\"  Sentence {j}: {sentence.content}\")\n</code></pre>"},{"location":"API%20Reference/Modules/Parsers/text_parser/#class-based-usage","title":"Class-Based Usage","text":"<pre><code>from datapizza.modules.parsers.text_parser import TextParser\n\nparser = TextParser()\n\n# Parse with custom metadata\ndocument = parser.parse(\n    text=\"Sample text content here.\",\n    metadata={\n        \"source\": \"api_input\",\n        \"timestamp\": \"2024-01-01\",\n        \"language\": \"en\"\n    }\n)\n\n# Access document metadata\nprint(f\"Source: {document.metadata['source']}\")\nprint(f\"Number of paragraphs: {len(document.children)}\")\n</code></pre>"},{"location":"API%20Reference/Modules/Parsers/text_parser/#pipeline-integration","title":"Pipeline Integration","text":"<pre><code>from datapizza.modules.parsers.text_parser import TextParser\nfrom datapizza.modules.splitters import RecursiveSplitter\n\n# Create processing pipeline\nparser = TextParser()\nsplitter = RecursiveSplitter(chunk_size=500)\n\ndef process_text_document(text):\n    # Parse into hierarchical structure\n    document = parser.parse(text)\n\n    # Convert back to flat text for splitting\n    full_text = document.content\n\n    # Split into chunks\n    chunks = splitter(full_text)\n\n    return document, chunks\n\n# Process document\nstructured_doc, chunks = process_text_document(long_text)\n</code></pre>"},{"location":"API%20Reference/Modules/Parsers/text_parser/#best-practices","title":"Best Practices","text":"<ol> <li>Use for Simple Text: Best suited for plain text content without complex formatting</li> <li>Preprocessing: Clean text of unwanted characters before parsing if needed</li> <li>Metadata: Add relevant metadata during parsing for downstream processing</li> <li>Pipeline Integration: Combine with other modules for complete text processing workflows</li> </ol>"},{"location":"API%20Reference/Modules/Prompt/ChatPromptTemplate/","title":"ChatPromptTemplate","text":"<p>The ChatPromptTemplate class provides utilities for managing prompts, prompt templates, and conversation memory for AI interactions. It helps structure and format prompts for various AI tasks and maintain conversation context.</p>"},{"location":"API%20Reference/Modules/Prompt/ChatPromptTemplate/#datapizza.modules.prompt.ChatPromptTemplate","title":"datapizza.modules.prompt.ChatPromptTemplate","text":"<p>               Bases: <code>Prompt</code></p> <p>It takes as input a Memory, Chunks, Prompt and creates a Memory with all existing messages + the user's qry + function_call_retrieval + chunks retrieval. args:     user_prompt_template: str # The user prompt jinja template     retrieval_prompt_template: str # The retrieval prompt jinja template</p>"},{"location":"API%20Reference/Modules/Prompt/ChatPromptTemplate/#datapizza.modules.prompt.ChatPromptTemplate.format","title":"format","text":"<pre><code>format(\n    memory=None,\n    chunks=None,\n    user_prompt=\"\",\n    retrieval_query=\"\",\n)\n</code></pre> <p>Creates a new memory object that includes: - Existing memory messages - User's query - Function call retrieval results - Chunks retrieval results</p> <p>Parameters:</p> Name Type Description Default <code>memory</code> <code>Memory | None</code> <p>The memory object to add the new messages to.</p> <code>None</code> <code>chunks</code> <code>list[Chunk] | None</code> <p>The chunks to add to the memory.</p> <code>None</code> <code>user_prompt</code> <code>str</code> <p>The user's query.</p> <code>''</code> <code>retrieval_query</code> <code>str</code> <p>The query to search the vectorstore for.</p> <code>''</code> <p>Returns:</p> Type Description <code>Memory</code> <p>A new memory object with the new messages.</p>"},{"location":"API%20Reference/Modules/Prompt/ChatPromptTemplate/#overview","title":"Overview","text":"<p>The ChatPromptTemplate module provides utilities for managing prompts and prompt templates in AI pipelines.</p> <pre><code>from datapizza.modules.prompt import prompt\n</code></pre> <p>Features:</p> <ul> <li>Prompt template management and formatting</li> <li>Context-aware prompt construction</li> <li>Integration with memory systems for conversation history</li> <li>Structured prompt formatting for different AI tasks</li> </ul>"},{"location":"API%20Reference/Modules/Prompt/ChatPromptTemplate/#usage-examples","title":"Usage Examples","text":""},{"location":"API%20Reference/Modules/Prompt/ChatPromptTemplate/#basic-prompt-management","title":"Basic Prompt Management","text":"<pre><code>import uuid\n\nfrom datapizza.modules.prompt import ChatPromptTemplate\nfrom datapizza.type import Chunk\n\n# Create structured prompts for different tasks\nsystem_prompt = ChatPromptTemplate(\n    user_prompt_template=\"You are helping with data analysis tasks, this is the user prompt: {{ user_prompt }}\",\n    retrieval_prompt_template=\"Retrieved content:\\n{% for chunk in chunks %}{{ chunk.text }}\\n{% endfor %}\"\n)\n\nprint(system_prompt.format(user_prompt=\"Hello, how are you?\", chunks=[Chunk(id=str(uuid.uuid4()), text=\"This is a chunk\"), Chunk(id=str(uuid.uuid4()), text=\"This is another chunk\")]))\n</code></pre>"},{"location":"API%20Reference/Modules/Prompt/ImageRAGPrompt/","title":"ImageRAGPrompt","text":"<p>Specialized prompting utilities for Retrieval-Augmented Generation (RAG) with image content. The ImageRAGPrompt class provides multimodal content integration for vision-language models.</p>"},{"location":"API%20Reference/Modules/Prompt/ImageRAGPrompt/#datapizza.modules.prompt.ImageRAGPrompt","title":"datapizza.modules.prompt.ImageRAGPrompt","text":"<p>               Bases: <code>Prompt</code></p> <p>Create a memory for a image RAG system.</p>"},{"location":"API%20Reference/Modules/Prompt/ImageRAGPrompt/#datapizza.modules.prompt.ImageRAGPrompt.__init__","title":"__init__","text":"<pre><code>__init__(\n    user_prompt_template,\n    image_prompt_presentation,\n    each_image_template,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>user_prompt_template</code> <code>str</code> <p>str # The user prompt jinja template</p> required <code>image_prompt_presentation</code> <code>str</code> <p>str # The image prompt jinja template</p> required <code>each_image_template</code> <code>str</code> <p>str # The each image jinja template</p> required"},{"location":"API%20Reference/Modules/Prompt/ImageRAGPrompt/#datapizza.modules.prompt.ImageRAGPrompt.format","title":"format","text":"<pre><code>format(chunks, user_query, retrieval_query, memory=None)\n</code></pre> <p>Creates a new memory object that includes: - Existing memory messages - User's query - Function call retrieval results - Chunks retrieval results</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list[Chunk]</code> <p>The chunks to add to the memory.</p> required <code>user_query</code> <code>str</code> <p>The user's query.</p> required <code>retrieval_query</code> <code>str</code> <p>The query to search the vectorstore for.</p> required <code>memory</code> <code>Memory | None</code> <p>The memory object to add the new messages to.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>memory</code> <code>Memory</code> <p>A new memory object with the new messages.</p>"},{"location":"API%20Reference/Modules/Prompt/ImageRAGPrompt/#overview","title":"Overview","text":"<pre><code>from datapizza.modules.prompt.image_rag import ImageRAGPrompt\n\n# Initialize image RAG prompt handler\nimage_rag = ImageRAGPrompt()\n</code></pre> <p>Features:</p> <ul> <li>Image-aware RAG prompt construction</li> <li>Multimodal content integration</li> <li>Context preservation for image-text interactions</li> <li>Optimized prompting for vision-language models</li> </ul>"},{"location":"API%20Reference/Modules/Prompt/ImageRAGPrompt/#usage-examples","title":"Usage Examples","text":""},{"location":"API%20Reference/Modules/Prompt/ImageRAGPrompt/#basic-image-rag-usage","title":"Basic Image RAG Usage","text":"<pre><code>from datapizza.modules.prompt.image_rag import ImageRAGPrompt\nfrom datapizza.type import Media\n\n# Initialize image RAG prompt\nimage_rag = ImageRAGPrompt()\n\n# Create multimodal RAG prompt\nmedia_content = Media(data=image_data, media_type=\"image/png\")\nrag_prompt = image_rag.create_rag_prompt(\n    query=\"What does this chart show?\",\n    retrieved_context=text_context,\n    images=[media_content],\n    instructions=\"Analyze both the text context and image content\"\n)\n</code></pre>"},{"location":"API%20Reference/Modules/Rerankers/","title":"Rerankers","text":"<p>Rerankers are pipeline components that reorder and score retrieved content based on relevance to a query. They improve retrieval quality by applying more sophisticated ranking algorithms after initial retrieval, helping surface the most relevant content for user queries.</p>"},{"location":"API%20Reference/Modules/Rerankers/#installation","title":"Installation","text":"<p>All rerankers require separate installation via pip and are not included by default with <code>datapizza-ai-core</code>.</p>"},{"location":"API%20Reference/Modules/Rerankers/#available-rerankers","title":"Available Rerankers","text":""},{"location":"API%20Reference/Modules/Rerankers/#optional-rerankers-separate-installation-required","title":"Optional Rerankers (Separate Installation Required)","text":"<ul> <li>CohereReranker - Uses Cohere's reranking API for high-quality semantic reranking</li> <li>TogetherReranker - Uses Together AI's API with various model options</li> </ul>"},{"location":"API%20Reference/Modules/Rerankers/#common-features","title":"Common Features","text":"<ul> <li>High-quality semantic reranking using specialized models</li> <li>Configurable result count and score thresholds</li> <li>Support for both sync and async processing</li> <li>Automatic relevance scoring for retrieved content</li> <li>Integration with various reranking model providers</li> </ul>"},{"location":"API%20Reference/Modules/Rerankers/#usage-patterns","title":"Usage Patterns","text":""},{"location":"API%20Reference/Modules/Rerankers/#basic-reranking-pipeline","title":"Basic Reranking Pipeline","text":"<pre><code>from datapizza.modules.rerankers.cohere import CohereReranker\n\nreranker = CohereReranker(\n    api_key=\"your-cohere-key\",\n    endpoint=\"https://api.cohere.ai/v1\",\n    top_n=5,\n    threshold=0.6\n)\n\nquery = \"What is deep learning?\"\nreranked_chunks = reranker(query, chunks)\n</code></pre>"},{"location":"API%20Reference/Modules/Rerankers/#rag-pipeline-integration","title":"RAG Pipeline Integration","text":"<pre><code>from datapizza.modules.rerankers.together import TogetherReranker\nfrom datapizza.vectorstores import QdrantVectorStore\n\n# Initial broad retrieval\nvectorstore = QdrantVectorStore(collection_name=\"documents\")\ninitial_results = vectorstore.similarity_search(query, k=20)\n\n# Rerank for better relevance\nreranker = TogetherReranker(api_key=\"together-key\", model=\"rerank-model\")\nreranked_results = reranker(query, initial_results)\n</code></pre>"},{"location":"API%20Reference/Modules/Rerankers/#best-practices","title":"Best Practices","text":"<ol> <li>Choose the Right Model: Select reranker models based on your domain and language requirements</li> <li>Tune Thresholds: Experiment with relevance score thresholds to balance precision and recall</li> <li>Initial Retrieval Size: Retrieve more documents initially (k=20-50) before reranking to improve final quality</li> <li>Performance Considerations: Use async processing for high-throughput applications</li> <li>Cost Management: Monitor API usage, especially for high-volume applications</li> <li>Evaluation: Test different rerankers on your specific data to find the best performance</li> </ol>"},{"location":"API%20Reference/Modules/Rerankers/cohere_reranker/","title":"CohereReranker","text":"<p>A reranker that uses Cohere's reranking API to score and reorder documents based on query relevance.</p>"},{"location":"API%20Reference/Modules/Rerankers/cohere_reranker/#installation","title":"Installation","text":"<pre><code>pip install datapizza-ai-rerankers-cohere\n</code></pre>"},{"location":"API%20Reference/Modules/Rerankers/cohere_reranker/#datapizza.modules.rerankers.cohere.CohereReranker","title":"datapizza.modules.rerankers.cohere.CohereReranker","text":"<p>               Bases: <code>Reranker</code></p> <p>A reranker that uses the Cohere API to rerank documents.</p>"},{"location":"API%20Reference/Modules/Rerankers/cohere_reranker/#datapizza.modules.rerankers.cohere.CohereReranker.__init__","title":"__init__","text":"<pre><code>__init__(\n    api_key,\n    endpoint,\n    top_n=10,\n    threshold=None,\n    model=\"model\",\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key for the Cohere API.</p> required <code>endpoint</code> <code>str</code> <p>The endpoint for the Cohere API.</p> required <code>top_n</code> <code>int</code> <p>The number of documents to return.</p> <code>10</code> <code>threshold</code> <code>float | None</code> <p>The threshold for the reranker.</p> <code>None</code>"},{"location":"API%20Reference/Modules/Rerankers/cohere_reranker/#datapizza.modules.rerankers.cohere.CohereReranker.a_rerank","title":"a_rerank  <code>async</code>","text":"<pre><code>a_rerank(query, documents)\n</code></pre> <p>Rerank documents based on query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to rerank documents by.</p> required <code>documents</code> <code>list[Chunk]</code> <p>The documents to rerank.</p> required <p>Returns:</p> Type Description <code>list[Chunk]</code> <p>The reranked documents.</p>"},{"location":"API%20Reference/Modules/Rerankers/cohere_reranker/#datapizza.modules.rerankers.cohere.CohereReranker.rerank","title":"rerank","text":"<pre><code>rerank(query, documents)\n</code></pre> <p>Rerank documents based on query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to rerank documents by.</p> required <code>documents</code> <code>list[Chunk]</code> <p>The documents to rerank.</p> required <p>Returns:</p> Type Description <code>list[Chunk]</code> <p>The reranked documents.</p>"},{"location":"API%20Reference/Modules/Rerankers/cohere_reranker/#usage","title":"Usage","text":"<pre><code>from datapizza.modules.rerankers.cohere import CohereReranker\n\nreranker = CohereReranker(\n    api_key=\"your-cohere-api-key\",\n    endpoint=\"https://api.cohere.ai/v1\",\n    top_n=10,\n    threshold=0.5,\n    model=\"rerank-v3.5\",\n)\n\n# Rerank chunks based on query\nquery = \"What are the benefits of machine learning?\"\nreranked_chunks = reranker.rerank(query, chunks)\n</code></pre>"},{"location":"API%20Reference/Modules/Rerankers/cohere_reranker/#features","title":"Features","text":"<ul> <li>High-quality semantic reranking using Cohere's models</li> <li>Configurable result count and score thresholds</li> <li>Support for both sync and async processing</li> <li>Automatic relevance scoring for retrieved content</li> <li>Integration with Cohere's latest reranking models</li> <li>Flexible endpoint configuration for different Cohere services</li> </ul>"},{"location":"API%20Reference/Modules/Rerankers/cohere_reranker/#examples","title":"Examples","text":""},{"location":"API%20Reference/Modules/Rerankers/cohere_reranker/#basic-usage","title":"Basic Usage","text":"<pre><code>import uuid\n\nfrom datapizza.modules.rerankers.cohere import CohereReranker\nfrom datapizza.type import Chunk\n\n# Initialize reranker\nreranker = CohereReranker(\n    api_key=\"COHERE_API_KEY\",\n    endpoint=\"https://api.cohere.ai/v1\",\n    top_n=5,\n    threshold=0.6,\n    model=\"rerank-v3.5\",\n)\n\n# Sample retrieved chunks\nchunks = [\n    Chunk(id=str(uuid.uuid4()), text=\"Machine learning enables computers to learn from data...\"),\n    Chunk(id=str(uuid.uuid4()), text=\"Deep learning is a subset of machine learning...\"),\n    Chunk(id=str(uuid.uuid4()), text=\"Neural networks consist of interconnected nodes...\"),\n    Chunk(id=str(uuid.uuid4()), text=\"Supervised learning uses labeled training data...\"),\n    Chunk(id=str(uuid.uuid4()), text=\"The weather forecast shows rain tomorrow...\")\n]\n\nquery = \"What is deep learning and how does it work?\"\n\n# Rerank based on relevance to query\nreranked_chunks = reranker.rerank(query, chunks)\n\n# Display results with scores\nfor i, chunk in enumerate(reranked_chunks):\n    score = chunk.metadata.get('relevance_score', 'N/A')\n    print(f\"Rank {i+1} (Score: {score}): {chunk.text[:80]}...\")\n</code></pre>"},{"location":"API%20Reference/Modules/Rerankers/together_reranker/","title":"TogetherReranker","text":"<p>A reranker that uses Together AI's API for document reranking with various model options.</p>"},{"location":"API%20Reference/Modules/Rerankers/together_reranker/#installation","title":"Installation","text":"<pre><code>pip install datapizza-ai-rerankers-together\n</code></pre>"},{"location":"API%20Reference/Modules/Rerankers/together_reranker/#datapizza.modules.rerankers.together.TogetherReranker","title":"datapizza.modules.rerankers.together.TogetherReranker","text":"<p>               Bases: <code>Reranker</code></p> <p>A reranker that uses the Together API to rerank documents.</p>"},{"location":"API%20Reference/Modules/Rerankers/together_reranker/#datapizza.modules.rerankers.together.TogetherReranker.__init__","title":"__init__","text":"<pre><code>__init__(api_key, model, top_n=10, threshold=None)\n</code></pre> <p>Initialize the TogetherReranker.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>Together API key</p> required <code>model</code> <code>str</code> <p>Model name to use for reranking</p> required <code>top_n</code> <code>int</code> <p>Number of top documents to return</p> <code>10</code> <code>threshold</code> <code>Optional[float]</code> <p>Minimum relevance score threshold. If None, no filtering is applied.</p> <code>None</code>"},{"location":"API%20Reference/Modules/Rerankers/together_reranker/#datapizza.modules.rerankers.together.TogetherReranker.rerank","title":"rerank","text":"<pre><code>rerank(query, documents)\n</code></pre> <p>Rerank documents based on query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to rerank documents by.</p> required <code>documents</code> <code>list[Chunk]</code> <p>The documents to rerank.</p> required <p>Returns:</p> Type Description <code>list[Chunk]</code> <p>The reranked documents.</p>"},{"location":"API%20Reference/Modules/Rerankers/together_reranker/#usage","title":"Usage","text":"<pre><code>from datapizza.modules.rerankers.together import TogetherReranker\n\nreranker = TogetherReranker(\n    api_key=\"your-together-api-key\",\n    model=\"sentence-transformers/msmarco-bert-base-dot-v5\",\n    top_n=15,\n    threshold=0.3\n)\n\n# Rerank documents\nquery = \"How to implement neural networks?\"\nreranked_results = reranker.rerank(query, document_chunks)\n</code></pre>"},{"location":"API%20Reference/Modules/Rerankers/together_reranker/#features","title":"Features","text":"<ul> <li>Access to multiple reranking model options</li> <li>Flexible model selection for different use cases</li> <li>Score-based filtering with configurable thresholds</li> <li>Support for various domain-specific models</li> <li>Integration with Together AI's model ecosystem</li> <li>Automatic model initialization and management</li> </ul>"},{"location":"API%20Reference/Modules/Rerankers/together_reranker/#available-models","title":"Available Models","text":"<p>Common reranking models available through Together AI:</p> <ul> <li><code>sentence-transformers/msmarco-bert-base-dot-v5</code></li> <li><code>sentence-transformers/all-MiniLM-L6-v2</code></li> <li><code>sentence-transformers/all-mpnet-base-v2</code></li> <li>Custom fine-tuned models for specific domains</li> </ul>"},{"location":"API%20Reference/Modules/Rerankers/together_reranker/#examples","title":"Examples","text":""},{"location":"API%20Reference/Modules/Rerankers/together_reranker/#basic-usage","title":"Basic Usage","text":"<pre><code>import uuid\n\nfrom datapizza.modules.rerankers.together import TogetherReranker\nfrom datapizza.type import Chunk\n\n# Initialize with specific model\nreranker = TogetherReranker(\n    api_key=\"TOGETHER_API_KEY\",\n    model=\"Salesforce/Llama-Rank-V1\",\n    top_n=10,\n    threshold=0.4\n)\n\n# Sample chunks\nchunks = [\n    Chunk(id=str(uuid.uuid4()), text=\"Neural networks are computational models inspired by biological brains...\"),\n    Chunk(id=str(uuid.uuid4()), text=\"Deep learning uses multiple layers to learn complex patterns...\"),\n    Chunk(id=str(uuid.uuid4()), text=\"Backpropagation is the algorithm used to train neural networks...\"),\n    Chunk(id=str(uuid.uuid4()), text=\"The weather is sunny today with mild temperatures...\"),\n    Chunk(id=str(uuid.uuid4()), text=\"Convolutional neural networks excel at image recognition tasks...\")\n]\n\nquery = \"How do neural networks learn?\"\n\n# Rerank based on relevance\nreranked_results = reranker.rerank(query, chunks)\n\n# Display results\nfor i, chunk in enumerate(reranked_results):\n    score = chunk.metadata.get('relevance_score', 'N/A')\n    print(f\"Rank {i+1} (Score: {score}): {chunk.text[:70]}...\")\n</code></pre>"},{"location":"API%20Reference/Modules/Splitters/","title":"Splitters","text":"<p>Splitters are pipeline components that divide large text content into smaller, manageable chunks. They help optimize content for processing, storage, and retrieval in AI applications by creating appropriately sized segments while preserving context and meaning.</p>"},{"location":"API%20Reference/Modules/Splitters/#installation","title":"Installation","text":"<p>All splitters are included with <code>datapizza-ai-core</code> and require no additional installation.</p>"},{"location":"API%20Reference/Modules/Splitters/#available-splitters","title":"Available Splitters","text":""},{"location":"API%20Reference/Modules/Splitters/#core-splitters-included-by-default","title":"Core Splitters (Included by Default)","text":"<ul> <li>RecursiveSplitter - Recursively divides text using multiple splitting strategies</li> <li>TextSplitter - Basic text splitter for general-purpose chunking</li> <li>NodeSplitter - Splitter for Node objects preserving hierarchical structure</li> <li>PDFImageSplitter - Specialized splitter for PDF content with images</li> </ul>"},{"location":"API%20Reference/Modules/Splitters/#common-features","title":"Common Features","text":"<ul> <li>Multiple splitting strategies for different content types</li> <li>Configurable chunk sizes and overlap</li> <li>Context preservation through overlapping</li> <li>Support for structured content (nodes, PDFs, etc.)</li> <li>Metadata preservation during splitting</li> <li>Spatial layout awareness for document content</li> </ul>"},{"location":"API%20Reference/Modules/Splitters/#usage-patterns","title":"Usage Patterns","text":""},{"location":"API%20Reference/Modules/Splitters/#basic-text-splitting","title":"Basic Text Splitting","text":"<pre><code>from datapizza.modules.splitters import RecursiveSplitter\n\nsplitter = RecursiveSplitter(chunk_size=1000, chunk_overlap=200)\nchunks = splitter(long_text_content)\n</code></pre>"},{"location":"API%20Reference/Modules/Splitters/#document-processing-pipeline","title":"Document Processing Pipeline","text":"<pre><code>from datapizza.modules.parsers import TextParser\nfrom datapizza.modules.splitters import NodeSplitter\n\nparser = TextParser()\nsplitter = NodeSplitter(chunk_size=800, preserve_structure=True)\n\ndocument = parser.parse(text_content)\nstructured_chunks = splitter(document)\n</code></pre>"},{"location":"API%20Reference/Modules/Splitters/#choosing-the-right-splitter","title":"Choosing the Right Splitter","text":"<ul> <li>RecursiveSplitter: Best for general text content, articles, and most use cases</li> <li>TextSplitter: Simple splitting for basic text without complex requirements</li> <li>NodeSplitter: When working with structured Node objects from parsers</li> <li>PDFImageSplitter: Specifically for PDF content with images and complex layouts</li> <li>BBoxMerger: Utility for processing documents with spatial layout information</li> </ul>"},{"location":"API%20Reference/Modules/Splitters/node_splitter/","title":"NodeSplitter","text":""},{"location":"API%20Reference/Modules/Splitters/node_splitter/#datapizza.modules.splitters.NodeSplitter","title":"datapizza.modules.splitters.NodeSplitter","text":"<p>               Bases: <code>Splitter</code></p> <p>A splitter that traverses a document tree from the root node. If the root node's content is smaller than max_chars, it becomes a single chunk. Otherwise, it recursively processes the node's children, creating chunks from the first level of children that fit within max_chars, continuing deeper into the tree structure as needed.</p>"},{"location":"API%20Reference/Modules/Splitters/node_splitter/#datapizza.modules.splitters.NodeSplitter.__init__","title":"__init__","text":"<pre><code>__init__(max_char=5000)\n</code></pre> <p>Initialize the NodeSplitter.</p> <p>Parameters:</p> Name Type Description Default <code>max_char</code> <code>int</code> <p>The maximum number of characters per chunk</p> <code>5000</code>"},{"location":"API%20Reference/Modules/Splitters/node_splitter/#datapizza.modules.splitters.NodeSplitter.split","title":"split","text":"<pre><code>split(node)\n</code></pre> <p>Split the node into chunks.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The node to split</p> required <p>Returns:</p> Type Description <code>list[Chunk]</code> <p>A list of chunks</p>"},{"location":"API%20Reference/Modules/Splitters/node_splitter/#usage","title":"Usage","text":"<pre><code>from datapizza.modules.splitters import NodeSplitter\n\nsplitter = NodeSplitter(\n    max_char=800,\n)\n\nnode_chunks = splitter.split(document_node)\n</code></pre>"},{"location":"API%20Reference/Modules/Splitters/node_splitter/#features","title":"Features","text":"<ul> <li>Maintains Node object structure and hierarchy</li> <li>Preserves metadata from original nodes</li> <li>Respects node boundaries when possible</li> <li>Supports both structure-preserving and flattened chunking</li> <li>Handles nested node relationships intelligently</li> </ul>"},{"location":"API%20Reference/Modules/Splitters/node_splitter/#examples","title":"Examples","text":""},{"location":"API%20Reference/Modules/Splitters/node_splitter/#basic-node-splitting","title":"Basic Node Splitting","text":"<pre><code>from datapizza.modules.parsers import TextParser\nfrom datapizza.modules.splitters import NodeSplitter\n\n# Parse text into nodes\nparser = TextParser()\ndocument = parser.parse(\"\"\"\nThis is the first section of the document.\nIt contains important information about the topic.\n\nThis is the second section with more details.\nIt provides additional context and examples.\n\nThe final section concludes the document.\nIt summarizes the key points discussed.\n\"\"\")\n\nsplitter = NodeSplitter(\n    max_char=150,\n)\n\nchunks = splitter.split(document)\n\n# Examine the structured chunks\nfor i, chunk in enumerate(chunks):\n    print(f\"Chunk {i+1}:\")\n    print(f\"  Content length: {len(chunk.text)}\")\n    print(f\"  Content preview: {chunk.text[:80]}...\")\n    print(\"---\")\n</code></pre>"},{"location":"API%20Reference/Modules/Splitters/pdf_image_splitter/","title":"PDFImageSplitter","text":""},{"location":"API%20Reference/Modules/Splitters/pdf_image_splitter/#datapizza.modules.splitters.PDFImageSplitter","title":"datapizza.modules.splitters.PDFImageSplitter","text":"<p>               Bases: <code>Splitter</code></p> <p>Splits a PDF document into individual pages, saves each page as an image using fitz, and returns metadata about each page as a Chunk object.</p>"},{"location":"API%20Reference/Modules/Splitters/pdf_image_splitter/#datapizza.modules.splitters.PDFImageSplitter.__init__","title":"__init__","text":"<pre><code>__init__(\n    image_format=\"png\",\n    output_base_dir=\"output_images\",\n    dpi=300,\n)\n</code></pre> <p>Initializes the Splitter.</p> <p>Parameters:</p> Name Type Description Default <code>image_format</code> <code>Literal['png', 'jpeg']</code> <p>The format to save the images in ('png' or 'jpeg'). Defaults to 'png'.</p> <code>'png'</code> <code>output_base_dir</code> <code>str | Path</code> <p>The base directory where images for processed PDFs will be saved.               A subdirectory will be created for each PDF. Defaults to 'output_images'.</p> <code>'output_images'</code> <code>dpi</code> <code>int</code> <p>Dots Per Inch for rendering the PDF page to an image. Higher values increase resolution and file size. Defaults to 300.</p> <code>300</code>"},{"location":"API%20Reference/Modules/Splitters/pdf_image_splitter/#datapizza.modules.splitters.PDFImageSplitter.split","title":"split","text":"<pre><code>split(pdf_path)\n</code></pre> <p>Processes the PDF using fitz: converts pages to images and returns Chunk objects.</p> <p>Parameters:</p> Name Type Description Default <code>pdf_path</code> <code>str | Path</code> <p>The path to the input PDF file.</p> required <p>Returns:</p> Type Description <code>list[Chunk]</code> <p>A list of Chunk objects, one for each page of the PDF.</p>"},{"location":"API%20Reference/Modules/Splitters/pdf_image_splitter/#usage","title":"Usage","text":"<pre><code>from datapizza.modules.splitters import PDFImageSplitter\n\nsplitter = PDFImageSplitter()\n\npdf_chunks = splitter(\"pdf_path\")\n</code></pre>"},{"location":"API%20Reference/Modules/Splitters/pdf_image_splitter/#features","title":"Features","text":"<ul> <li>Specialized handling of PDF document structure</li> <li>Preserves image data and visual elements</li> <li>Maintains spatial layout information</li> <li>Includes page-level metadata and coordinates</li> <li>Handles complex document layouts with mixed content</li> <li>Optimized for PDF content from document intelligence services</li> </ul>"},{"location":"API%20Reference/Modules/Splitters/pdf_image_splitter/#examples","title":"Examples","text":""},{"location":"API%20Reference/Modules/Splitters/pdf_image_splitter/#basic-pdf-content-splitting","title":"Basic PDF Content Splitting","text":"<pre><code>from datapizza.modules.splitters import PDFImageSplitter\n\n# Split while preserving images and layout\npdf_splitter = PDFImageSplitter()\n\npdf_chunks = pdf_splitter(\"pdf_path\")\n\n# Examine chunks with visual content\nfor i, chunk in enumerate(pdf_chunks):\n    print(f\"Chunk {i+1}:\")\n    print(f\"  Content length: {len(chunk.content)}\")\n    print(f\"  Page: {chunk.metadata.get('page_number', 'unknown')}\")\n\n    if hasattr(chunk, 'media') and chunk.media:\n        print(f\"  Media elements: {len(chunk.media)}\")\n        for media in chunk.media:\n            print(f\"    Type: {media.media_type}\")\n\n    if 'boundingRegions' in chunk.metadata:\n        print(f\"  Bounding regions: {len(chunk.metadata['boundingRegions'])}\")\n\n    print(\"---\")\n</code></pre>"},{"location":"API%20Reference/Modules/Splitters/recursive_splitter/","title":"RecursiveSplitter","text":""},{"location":"API%20Reference/Modules/Splitters/recursive_splitter/#datapizza.modules.splitters.RecursiveSplitter","title":"datapizza.modules.splitters.RecursiveSplitter","text":"<p>               Bases: <code>Splitter</code></p> <p>The RecursiveSplitter takes leaf nodes from a tree document structure and groups them into Chunk objects until reaching the maximum character limit. Each leaf Node represents the smallest unit of content that can be grouped.</p>"},{"location":"API%20Reference/Modules/Splitters/recursive_splitter/#datapizza.modules.splitters.RecursiveSplitter.__init__","title":"__init__","text":"<pre><code>__init__(max_char=5000, overlap=0)\n</code></pre> <p>Initialize the RecursiveSplitter.</p> <p>Parameters:</p> Name Type Description Default <code>max_char</code> <code>int</code> <p>The maximum number of characters per chunk</p> <code>5000</code> <code>overlap</code> <code>int</code> <p>The number of characters to overlap between chunks</p> <code>0</code>"},{"location":"API%20Reference/Modules/Splitters/recursive_splitter/#datapizza.modules.splitters.RecursiveSplitter.split","title":"split","text":"<pre><code>split(node)\n</code></pre> <p>Split the node into chunks.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The node to split</p> required <p>Returns:</p> Type Description <code>list[Chunk]</code> <p>A list of chunks</p>"},{"location":"API%20Reference/Modules/Splitters/recursive_splitter/#usage","title":"Usage","text":"<pre><code>from datapizza.modules.parsers import TextParser\nfrom datapizza.modules.splitters import RecursiveSplitter\n\nsplitter = RecursiveSplitter(\n    max_char=10,\n    overlap=1,\n)\n\n# Parse text into nodes because RecursiveSplitter need Node\nparser = TextParser()\ndocument = parser.parse(\"\"\"\nThis is the first section of the document.\nIt contains important information about the topic.\n\nThis is the second section with more details.\nIt provides additional context and examples.\n\nThe final section concludes the document.\nIt summarizes the key points discussed.\n\"\"\")\n\nchunks = splitter.split(document)\nprint(chunks)\n</code></pre>"},{"location":"API%20Reference/Modules/Splitters/recursive_splitter/#features","title":"Features","text":"<ul> <li>Uses multiple separator strategies in order of preference</li> <li>Recursive approach ensures optimal chunk boundaries</li> <li>Configurable chunk size and overlap for context preservation</li> <li>Handles various content types with appropriate separator selection</li> <li>Preserves content structure while maintaining size limits</li> </ul>"},{"location":"API%20Reference/Modules/Splitters/text_splitter/","title":"TextSplitter","text":""},{"location":"API%20Reference/Modules/Splitters/text_splitter/#datapizza.modules.splitters.TextSplitter","title":"datapizza.modules.splitters.TextSplitter","text":"<p>               Bases: <code>Splitter</code></p> <p>A basic text splitter that operates directly on strings rather than Node objects. Unlike other splitters that work with Node types, this splitter takes raw text input and splits it into chunks while maintaining configurable size and overlap parameters.</p>"},{"location":"API%20Reference/Modules/Splitters/text_splitter/#datapizza.modules.splitters.TextSplitter.__init__","title":"__init__","text":"<pre><code>__init__(max_char=5000, overlap=0)\n</code></pre> <p>Initialize the TextSplitter.</p> <p>Parameters:</p> Name Type Description Default <code>max_char</code> <code>int</code> <p>The maximum number of characters per chunk</p> <code>5000</code> <code>overlap</code> <code>int</code> <p>The number of characters to overlap between chunks</p> <code>0</code>"},{"location":"API%20Reference/Modules/Splitters/text_splitter/#datapizza.modules.splitters.TextSplitter.split","title":"split","text":"<pre><code>split(text)\n</code></pre> <p>Split the text into chunks.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to split</p> required <p>Returns:</p> Type Description <code>list[Chunk]</code> <p>A list of chunks</p>"},{"location":"API%20Reference/Modules/Splitters/text_splitter/#usage","title":"Usage","text":"<pre><code>from datapizza.modules.splitters import TextSplitter\n\nsplitter = TextSplitter(\n    max_char=500,\n    overlap=50\n)\n\nchunks = splitter.split(text_content)\n</code></pre>"},{"location":"API%20Reference/Modules/Splitters/text_splitter/#features","title":"Features","text":"<ul> <li>Simple, straightforward text splitting algorithm</li> <li>Configurable chunk size and overlap</li> <li>Lightweight implementation for basic splitting needs</li> <li>Preserves character-level accuracy in chunk boundaries</li> <li>Minimal overhead for high-performance applications</li> </ul>"},{"location":"API%20Reference/Modules/Splitters/text_splitter/#examples","title":"Examples","text":""},{"location":"API%20Reference/Modules/Splitters/text_splitter/#basic-usage","title":"Basic Usage","text":"<pre><code>from datapizza.modules.splitters import TextSplitter\n\nsplitter = TextSplitter(max_char=50, overlap=5)\n\ntext = \"\"\"\nThis is a sample text that we want to split into smaller chunks.\nThe TextSplitter will divide this content based on the specified\nchunk size and overlap parameters. This ensures that information\nis preserved while creating manageable pieces of content.\n\"\"\"\n\nchunks = splitter.split(text)\n\nfor i, chunk in enumerate(chunks):\n    print(f\"Chunk {i+1}: {len(chunk.text)} chars\")\n    print(f\"Content: {chunk.text}\")\n    print(\"---\")\n</code></pre>"},{"location":"API%20Reference/Pipelines/dag/","title":"Dag","text":""},{"location":"API%20Reference/Pipelines/dag/#datapizza.pipeline.dag_pipeline.DagPipeline","title":"datapizza.pipeline.dag_pipeline.DagPipeline","text":"<p>A pipeline that runs a graph of a dependency graph.</p>"},{"location":"API%20Reference/Pipelines/dag/#datapizza.pipeline.dag_pipeline.DagPipeline.a_run","title":"a_run  <code>async</code>","text":"<pre><code>a_run(data)\n</code></pre> <p>Run the pipeline asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>The input data to the pipeline.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The results of the pipeline.</p>"},{"location":"API%20Reference/Pipelines/dag/#datapizza.pipeline.dag_pipeline.DagPipeline.add_module","title":"add_module","text":"<pre><code>add_module(node_name, node)\n</code></pre> <p>Add a module to the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>node_name</code> <code>str</code> <p>The name of the module.</p> required <code>node</code> <code>PipelineComponent</code> <p>The module to add.</p> required"},{"location":"API%20Reference/Pipelines/dag/#datapizza.pipeline.dag_pipeline.DagPipeline.connect","title":"connect","text":"<pre><code>connect(\n    source_node, target_node, target_key, source_key=None\n)\n</code></pre> <p>Connect two nodes in the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>source_node</code> <code>str</code> <p>The name of the source node.</p> required <code>target_node</code> <code>str</code> <p>The name of the target node.</p> required <code>target_key</code> <code>str</code> <p>The key to store the result of the target node in the source node.</p> required <code>source_key</code> <code>str</code> <p>The key to retrieve the result of the source node from the target node. Defaults to None.</p> <code>None</code>"},{"location":"API%20Reference/Pipelines/dag/#datapizza.pipeline.dag_pipeline.DagPipeline.from_yaml","title":"from_yaml","text":"<pre><code>from_yaml(config_path)\n</code></pre> <p>Load the pipeline from a YAML configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the YAML configuration file.</p> required <p>Returns:</p> Name Type Description <code>DagPipeline</code> <code>DagPipeline</code> <p>The pipeline instance.</p>"},{"location":"API%20Reference/Pipelines/dag/#datapizza.pipeline.dag_pipeline.DagPipeline.run","title":"run","text":"<pre><code>run(data)\n</code></pre> <p>Run the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>The input data to the pipeline.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The results of the pipeline.</p>"},{"location":"API%20Reference/Pipelines/functional/","title":"Functional","text":""},{"location":"API%20Reference/Pipelines/functional/#datapizza.pipeline.functional_pipeline.FunctionalPipeline","title":"datapizza.pipeline.functional_pipeline.FunctionalPipeline","text":"<p>Pipeline for executing a series of nodes with dependencies.</p>"},{"location":"API%20Reference/Pipelines/functional/#datapizza.pipeline.functional_pipeline.FunctionalPipeline.branch","title":"branch","text":"<pre><code>branch(condition, if_true, if_false, dependencies=None)\n</code></pre> <p>Branch execution based on a condition.</p> <p>Parameters:</p> Name Type Description Default <code>condition</code> <code>Callable</code> <p>The condition to evaluate.</p> required <code>if_true</code> <code>FunctionalPipeline</code> <p>The pipeline to execute if the condition is True.</p> required <code>if_false</code> <code>FunctionalPipeline</code> <p>The pipeline to execute if the condition is False.</p> required <code>dependencies</code> <code>list[Dependency]</code> <p>List of dependencies for the node. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>FunctionalPipeline</code> <code>FunctionalPipeline</code> <p>The pipeline instance.</p>"},{"location":"API%20Reference/Pipelines/functional/#datapizza.pipeline.functional_pipeline.FunctionalPipeline.execute","title":"execute","text":"<pre><code>execute(initial_data=None, context=None)\n</code></pre> <p>Execute the pipeline and return the results.</p> <p>Parameters:</p> Name Type Description Default <code>initial_data</code> <code>dict[str, Any] | None</code> <p>Dictionary where keys are node names and values are the data          to be passed to those nodes when they execute.</p> <code>None</code> <code>context</code> <code>dict | None</code> <p>Dictionary where keys are node names and values are the data      to be passed to those nodes when they execute.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any]</code> <p>The results of the pipeline.</p>"},{"location":"API%20Reference/Pipelines/functional/#datapizza.pipeline.functional_pipeline.FunctionalPipeline.foreach","title":"foreach","text":"<pre><code>foreach(name, do, dependencies=None)\n</code></pre> <p>Execute a sub-pipeline for each item in a collection.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the node.</p> required <code>do</code> <code>PipelineComponent</code> <p>The sub-pipeline to execute for each item.</p> required <code>dependencies</code> <code>list[Dependency]</code> <p>List of dependencies for the node. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>FunctionalPipeline</code> <code>FunctionalPipeline</code> <p>The pipeline instance.</p>"},{"location":"API%20Reference/Pipelines/functional/#datapizza.pipeline.functional_pipeline.FunctionalPipeline.from_yaml","title":"from_yaml  <code>staticmethod</code>","text":"<pre><code>from_yaml(yaml_path)\n</code></pre> <p>Constructs a FunctionalPipeline from a YAML configuration file. The YAML should contain 'modules' (optional) defining reusable components and 'pipeline' defining the sequence of steps.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_path</code> <code>str</code> <p>Path to the YAML configuration file.</p> required <p>Returns:</p> Type Description <code>FunctionalPipeline</code> <p>A configured FunctionalPipeline instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the YAML format is invalid, a module cannot be loaded,         or a referenced node/condition name is not found.</p> <code>KeyError</code> <p>If a required key is missing in the YAML structure.</p> <code>FileNotFoundError</code> <p>If the yaml_path does not exist.</p> <code>YAMLError</code> <p>If the YAML file cannot be parsed.</p> <code>ImportError</code> <p>If a specified module cannot be imported.</p> <code>AttributeError</code> <p>If a specified class/function is not found in the module.</p>"},{"location":"API%20Reference/Pipelines/functional/#datapizza.pipeline.functional_pipeline.FunctionalPipeline.get","title":"get","text":"<pre><code>get(name)\n</code></pre> <p>Get the result of a node.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the node.</p> required <p>Returns:</p> Name Type Description <code>FunctionalPipeline</code> <code>FunctionalPipeline</code> <p>The pipeline instance.</p>"},{"location":"API%20Reference/Pipelines/functional/#datapizza.pipeline.functional_pipeline.FunctionalPipeline.run","title":"run","text":"<pre><code>run(name, node, dependencies=None, kwargs=None)\n</code></pre> <p>Add a node to the pipeline with optional dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the node.</p> required <code>node</code> <code>PipelineComponent</code> <p>The node to add.</p> required <code>dependencies</code> <code>list[Dependency]</code> <p>List of dependencies for the node. Defaults to None.</p> <code>None</code> <code>kwargs</code> <code>dict[str, Any]</code> <p>Additional keyword arguments to pass to the node. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>FunctionalPipeline</code> <code>FunctionalPipeline</code> <p>The pipeline instance.</p>"},{"location":"API%20Reference/Pipelines/functional/#datapizza.pipeline.functional_pipeline.FunctionalPipeline.then","title":"then","text":"<pre><code>then(\n    name, node, target_key, dependencies=None, kwargs=None\n)\n</code></pre> <p>Add a node to execute after the previous node.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the node.</p> required <code>node</code> <code>PipelineComponent</code> <p>The node to add.</p> required <code>target_key</code> <code>str</code> <p>The key to store the result of the node in the previous node.</p> required <code>dependencies</code> <code>list[Dependency]</code> <p>List of dependencies for the node. Defaults to None.</p> <code>None</code> <code>kwargs</code> <code>dict[str, Any]</code> <p>Additional keyword arguments to pass to the node. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>FunctionalPipeline</code> <code>FunctionalPipeline</code> <p>The pipeline instance.</p>"},{"location":"API%20Reference/Pipelines/ingestion/","title":"Ingestion","text":""},{"location":"API%20Reference/Pipelines/ingestion/#datapizza.pipeline.pipeline.IngestionPipeline","title":"datapizza.pipeline.pipeline.IngestionPipeline","text":"<p>A pipeline for ingesting data into a vector store.</p>"},{"location":"API%20Reference/Pipelines/ingestion/#datapizza.pipeline.pipeline.IngestionPipeline.__init__","title":"__init__","text":"<pre><code>__init__(\n    modules=None, vector_store=None, collection_name=None\n)\n</code></pre> <p>Initialize the ingestion pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>modules</code> <code>list[PipelineComponent]</code> <p>List of pipeline components. Defaults to None.</p> <code>None</code> <code>vector_store</code> <code>Vectorstore</code> <p>Vector store to store the ingested data. Defaults to None.</p> <code>None</code> <code>collection_name</code> <code>str</code> <p>Name of the vector store collection to store the ingested data. Defaults to None.</p> <code>None</code>"},{"location":"API%20Reference/Pipelines/ingestion/#datapizza.pipeline.pipeline.IngestionPipeline.a_run","title":"a_run  <code>async</code>","text":"<pre><code>a_run(file_path, metadata=None)\n</code></pre> <p>Run the ingestion pipeline asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | list[str]</code> <p>The file path or list of file paths to ingest.</p> required <code>metadata</code> <code>dict</code> <p>Metadata to add to the ingested chunks. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Chunk] | None</code> <p>if vector_store is set does not return anything, otherwise returns the last result of the pipeline.</p>"},{"location":"API%20Reference/Pipelines/ingestion/#datapizza.pipeline.pipeline.IngestionPipeline.from_yaml","title":"from_yaml","text":"<pre><code>from_yaml(config_path)\n</code></pre> <p>Load the ingestion pipeline from a YAML configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the YAML configuration file.</p> required <p>Returns:</p> Name Type Description <code>IngestionPipeline</code> <code>IngestionPipeline</code> <p>The ingestion pipeline instance.</p>"},{"location":"API%20Reference/Pipelines/ingestion/#datapizza.pipeline.pipeline.IngestionPipeline.run","title":"run","text":"<pre><code>run(file_path, metadata=None)\n</code></pre> <p>Run the ingestion pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | list[str]</code> <p>The file path or list of file paths to ingest.</p> required <code>metadata</code> <code>dict</code> <p>Metadata to add to the ingested chunks. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Chunk] | None</code> <p>if vector_store is set does not return anything, otherwise returns the last result of the pipeline.</p>"},{"location":"API%20Reference/Tools/duckduckgo/","title":"DuckDuckGo","text":"<pre><code>pip install datapizza-ai-tools-duckduckgo\n</code></pre>"},{"location":"API%20Reference/Tools/duckduckgo/#datapizza.tools.duckduckgo.DuckDuckGoSearchTool","title":"datapizza.tools.duckduckgo.DuckDuckGoSearchTool","text":"<p>               Bases: <code>Tool</code></p> <p>The DuckDuckGo Search tool. It allows you to search the web for the given query.</p>"},{"location":"API%20Reference/Tools/duckduckgo/#datapizza.tools.duckduckgo.DuckDuckGoSearchTool.__call__","title":"__call__","text":"<pre><code>__call__(query)\n</code></pre> <p>Invoke the tool.</p>"},{"location":"API%20Reference/Tools/duckduckgo/#datapizza.tools.duckduckgo.DuckDuckGoSearchTool.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initializes the DuckDuckGoSearch tool.</p>"},{"location":"API%20Reference/Tools/duckduckgo/#datapizza.tools.duckduckgo.DuckDuckGoSearchTool.search","title":"search","text":"<pre><code>search(query)\n</code></pre> <p>Search the web for the given query.</p>"},{"location":"API%20Reference/Tools/duckduckgo/#overview","title":"Overview","text":"<p>The DuckDuckGoSearchTool provides web search capabilities using the DuckDuckGo search engine. This tool enables AI models to search for real-time information from the web, making it particularly useful for grounding model responses with current data.</p>"},{"location":"API%20Reference/Tools/duckduckgo/#features","title":"Features","text":"<ul> <li>Web Search: Search the web using DuckDuckGo's search engine</li> <li>Privacy-focused: Uses DuckDuckGo which doesn't track users</li> <li>Simple Integration: Easy to integrate with AI agents and pipelines</li> <li>Real-time Results: Get current web search results</li> </ul>"},{"location":"API%20Reference/Tools/duckduckgo/#usage-example","title":"Usage Example","text":"<pre><code>from datapizza.tools.duckduckgo import DuckDuckGoSearchTool\n\n# Initialize the tool\nsearch_tool = DuckDuckGoSearchTool()\n\n# Perform a search\nresults = search_tool.search(\"latest AI developments 2024\")\n\n# Process results\nfor result in results:\n    print(f\"Title: {result.get('title', 'N/A')}\")\n    print(f\"URL: {result.get('href', 'N/A')}\")\n    print(f\"Body: {result.get('body', 'N/A')}\")\n    print(\"---\")\n</code></pre>"},{"location":"API%20Reference/Tools/duckduckgo/#integration-with-agents","title":"Integration with Agents","text":"<pre><code>from datapizza.agents import Agent\nfrom datapizza.clients.openai import OpenAIClient\nfrom datapizza.tools.duckduckgo import DuckDuckGoSearchTool\n\nagent = Agent(\n    name=\"agent\",\n    tools=[DuckDuckGoSearchTool()],\n    client=OpenAIClient(api_key=\"OPENAI_API_KEY\", model=\"gpt-4.1\"),\n)\n\nresponse = agent.run(\"What is datapizza? and who are the founders?\", tool_choice=\"required_first\")\nprint(response)\n</code></pre>"},{"location":"API%20Reference/Type/block/","title":"Blocks","text":""},{"location":"API%20Reference/Type/block/#datapizza.type.Block","title":"datapizza.type.Block","text":"<p>A class for storing the response from a client.</p>"},{"location":"API%20Reference/Type/block/#datapizza.type.Block.to_dict","title":"to_dict  <code>abstractmethod</code>","text":"<pre><code>to_dict()\n</code></pre> <p>Convert the block to a dictionary for JSON serialization.</p>"},{"location":"API%20Reference/Type/block/#datapizza.type.TextBlock","title":"datapizza.type.TextBlock","text":"<p>               Bases: <code>Block</code></p> <p>A class for storing the text response from a client.</p>"},{"location":"API%20Reference/Type/block/#datapizza.type.TextBlock.__init__","title":"__init__","text":"<pre><code>__init__(content, type='text')\n</code></pre> <p>Initialize a TextBlock object.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content of the text block.</p> required <code>type</code> <code>str</code> <p>The type of the text block. Defaults to \"text\".</p> <code>'text'</code>"},{"location":"API%20Reference/Type/block/#datapizza.type.MediaBlock","title":"datapizza.type.MediaBlock","text":"<p>               Bases: <code>Block</code></p> <p>A class for storing the media response from a client.</p>"},{"location":"API%20Reference/Type/block/#datapizza.type.MediaBlock.__init__","title":"__init__","text":"<pre><code>__init__(media, type='media')\n</code></pre> <p>Initialize a MediaBlock object.</p> <p>Parameters:</p> Name Type Description Default <code>media</code> <code>Media</code> <p>The media of the media block.</p> required <code>type</code> <code>str</code> <p>The type of the media block. Defaults to \"media\".</p> <code>'media'</code>"},{"location":"API%20Reference/Type/block/#datapizza.type.ThoughtBlock","title":"datapizza.type.ThoughtBlock","text":"<p>               Bases: <code>Block</code></p> <p>A class for storing the thought from a client.</p>"},{"location":"API%20Reference/Type/block/#datapizza.type.ThoughtBlock.__init__","title":"__init__","text":"<pre><code>__init__(content, type='thought')\n</code></pre> <p>Initialize a ThoughtBlock object.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content of the thought block.</p> required <code>type</code> <code>str</code> <p>The type of the thought block. Defaults to \"thought\".</p> <code>'thought'</code>"},{"location":"API%20Reference/Type/block/#datapizza.type.FunctionCallBlock","title":"datapizza.type.FunctionCallBlock","text":"<p>               Bases: <code>Block</code></p> <p>A class for storing the function call from a client.</p>"},{"location":"API%20Reference/Type/block/#datapizza.type.FunctionCallBlock.__init__","title":"__init__","text":"<pre><code>__init__(id, arguments, name, tool, type='function')\n</code></pre> <p>Initialize a FunctionCallBlock object.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>The id of the function call block.</p> required <code>arguments</code> <code>dict[str, Any]</code> <p>The arguments of the function call block.</p> required <code>name</code> <code>str</code> <p>The name of the function call block.</p> required <code>tool</code> <code>Tool</code> <p>The tool of the function call block.</p> required"},{"location":"API%20Reference/Type/block/#datapizza.type.FunctionCallResultBlock","title":"datapizza.type.FunctionCallResultBlock","text":"<p>               Bases: <code>Block</code></p> <p>A class for storing the function call response from a client.</p>"},{"location":"API%20Reference/Type/block/#datapizza.type.FunctionCallResultBlock.__init__","title":"__init__","text":"<pre><code>__init__(id, tool, result, type='function_call_result')\n</code></pre> <p>Initialize a FunctionCallResultBlock object.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>The id of the function call result block.</p> required <code>tool</code> <code>Tool</code> <p>The tool of the function call result block.</p> required <code>result</code> <code>str</code> <p>The result of the function call result block.</p> required"},{"location":"API%20Reference/Type/block/#datapizza.type.StructuredBlock","title":"datapizza.type.StructuredBlock","text":"<p>               Bases: <code>Block</code></p> <p>A class for storing the structured response from a client.</p>"},{"location":"API%20Reference/Type/block/#datapizza.type.StructuredBlock.__init__","title":"__init__","text":"<pre><code>__init__(content, type='structured')\n</code></pre> <p>Initialize a StructuredBlock object.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>BaseModel</code> <p>The content of the structured block.</p> required <code>type</code> <code>str</code> <p>The type of the structured block. Defaults to \"structured\".</p> <code>'structured'</code>"},{"location":"API%20Reference/Type/chunk/","title":"Chunk","text":""},{"location":"API%20Reference/Type/chunk/#datapizza.type.Chunk","title":"datapizza.type.Chunk  <code>dataclass</code>","text":"<p>A class for storing the chunk response from a client.</p>"},{"location":"API%20Reference/Type/chunk/#datapizza.type.Chunk.__init__","title":"__init__","text":"<pre><code>__init__(id, text, embeddings=None, metadata=None)\n</code></pre> <p>Initialize a Chunk object.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>The id of the chunk.</p> required <code>text</code> <code>str</code> <p>The text of the chunk.</p> required <code>embeddings</code> <code>list[Embedding]</code> <p>The embeddings of the chunk. Defaults to [].</p> <code>None</code> <code>metadata</code> <code>dict</code> <p>The metadata of the chunk. Defaults to {}.</p> <code>None</code>"},{"location":"API%20Reference/Type/chunk/#overview","title":"Overview","text":"<p>The <code>Chunk</code> class represents a unit of text content that has been segmented from a larger document. It's a fundamental data structure in datapizza-ai used throughout the RAG pipeline for text processing, embedding, and retrieval operations.  Serializable: Can be easily stored and retrieved from databases</p>"},{"location":"API%20Reference/Type/media/","title":"Media","text":""},{"location":"API%20Reference/Type/media/#datapizza.type.Media","title":"datapizza.type.Media","text":"<p>A class for storing the media response from a client.</p>"},{"location":"API%20Reference/Type/media/#datapizza.type.Media.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    extension=None,\n    media_type,\n    source_type,\n    source,\n    detail=\"high\",\n)\n</code></pre> <p>A class for storing the media response from a client.</p> <p>Parameters:</p> Name Type Description Default <code>extension</code> <code>str</code> <p>The file extension of the media. Defaults to None.</p> <code>None</code> <code>media_type</code> <code>Literal['image', 'video', 'audio', 'pdf']</code> <p>The type of media. Defaults to \"image\".</p> required <code>source_type</code> <code>Literal['url', 'base64', 'path', 'pil', 'raw']</code> <p>The source type of the media. Defaults to \"url\".</p> required <code>source</code> <code>Any</code> <p>The source of the media. Defaults to None.</p> required"},{"location":"API%20Reference/Type/media/#datapizza.type.Media.to_dict","title":"to_dict","text":"<pre><code>to_dict()\n</code></pre> <p>Convert the media to a dictionary for JSON serialization.</p>"},{"location":"API%20Reference/Type/node/","title":"Node","text":""},{"location":"API%20Reference/Type/node/#datapizza.type.Node","title":"datapizza.type.Node","text":"<p>Class representing a node in a document graph.</p>"},{"location":"API%20Reference/Type/node/#datapizza.type.Node.content","title":"content  <code>property</code>","text":"<pre><code>content\n</code></pre> <p>Get the textual content of this node and its children.</p>"},{"location":"API%20Reference/Type/node/#datapizza.type.Node.is_leaf","title":"is_leaf  <code>property</code>","text":"<pre><code>is_leaf\n</code></pre> <p>Check if the node is a leaf node (has no children).</p>"},{"location":"API%20Reference/Type/node/#datapizza.type.Node.__eq__","title":"__eq__","text":"<pre><code>__eq__(other)\n</code></pre> <p>Check if two nodes are equal.</p>"},{"location":"API%20Reference/Type/node/#datapizza.type.Node.__hash__","title":"__hash__","text":"<pre><code>__hash__()\n</code></pre> <p>Hash the node.</p>"},{"location":"API%20Reference/Type/node/#datapizza.type.Node.__init__","title":"__init__","text":"<pre><code>__init__(\n    children=None,\n    metadata=None,\n    node_type=NodeType.SECTION,\n    content=None,\n)\n</code></pre> <p>Initialize a Node object.</p> <p>Parameters:</p> Name Type Description Default <code>children</code> <code>list[Node] | None</code> <p>List of child nodes</p> <code>None</code> <code>metadata</code> <code>dict | None</code> <p>Dictionary of metadata</p> <code>None</code> <code>content</code> <code>str | None</code> <p>Content object for leaf nodes</p> <code>None</code>"},{"location":"API%20Reference/Type/node/#datapizza.type.Node.add_child","title":"add_child","text":"<pre><code>add_child(child)\n</code></pre> <p>Add a child node to this node.</p>"},{"location":"API%20Reference/Type/node/#datapizza.type.Node.remove_child","title":"remove_child","text":"<pre><code>remove_child(child)\n</code></pre> <p>Remove a child node from this node.</p>"},{"location":"API%20Reference/Type/node/#datapizza.type.MediaNode","title":"datapizza.type.MediaNode","text":"<p>               Bases: <code>Node</code></p> <p>Class representing a media node in a document graph.</p>"},{"location":"API%20Reference/Type/tool/","title":"Tool","text":""},{"location":"API%20Reference/Type/tool/#datapizza.tools.Tool","title":"datapizza.tools.Tool","text":"<p>Class that wraps a function while preserving its behavior and adding attributes.</p>"},{"location":"API%20Reference/Type/tool/#datapizza.tools.Tool.__init__","title":"__init__","text":"<pre><code>__init__(\n    func=None,\n    name=None,\n    description=None,\n    end=False,\n    properties=None,\n    required=None,\n    strict=False,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable | None</code> <p>The function to wrap.</p> <code>None</code> <code>name</code> <code>str | None</code> <p>The name of the tool.</p> <code>None</code> <code>description</code> <code>str | None</code> <p>The description of the tool.</p> <code>None</code> <code>end</code> <code>bool</code> <p>Whether the tool ends a chain of operations.</p> <code>False</code> <code>properties</code> <code>dict[str, dict[str, Any]] | None</code> <p>The properties of the tool.</p> <code>None</code> <code>required</code> <code>list[str] | None</code> <p>The required parameters of the tool.</p> <code>None</code> <code>strict</code> <code>bool</code> <p>Whether the tool is strict.</p> <code>False</code>"},{"location":"API%20Reference/Type/tool/#datapizza.tools.Tool.to_dict","title":"to_dict","text":"<pre><code>to_dict()\n</code></pre> <p>Convert the tool to a dictionary for JSON serialization.</p>"},{"location":"API%20Reference/Vectorstore/qdrant_vectorstore/","title":"QdrantVectorstore","text":"<pre><code>pip install datapizza-ai-vectorstores-qdrant\n</code></pre>"},{"location":"API%20Reference/Vectorstore/qdrant_vectorstore/#datapizza.vectorstores.qdrant.QdrantVectorstore","title":"datapizza.vectorstores.qdrant.QdrantVectorstore","text":"<p>               Bases: <code>Vectorstore</code></p> <p>datapizza-ai implementation of a Qdrant vectorstore.</p>"},{"location":"API%20Reference/Vectorstore/qdrant_vectorstore/#datapizza.vectorstores.qdrant.QdrantVectorstore.__init__","title":"__init__","text":"<pre><code>__init__(host=None, port=6333, api_key=None, **kwargs)\n</code></pre> <p>Initialize the QdrantVectorstore.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>The host to use for the Qdrant client. Defaults to None.</p> <code>None</code> <code>port</code> <code>int</code> <p>The port to use for the Qdrant client. Defaults to 6333.</p> <code>6333</code> <code>api_key</code> <code>str</code> <p>The API key to use for the Qdrant client. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the Qdrant client.</p> <code>{}</code>"},{"location":"API%20Reference/Vectorstore/qdrant_vectorstore/#datapizza.vectorstores.qdrant.QdrantVectorstore.a_search","title":"a_search  <code>async</code>","text":"<pre><code>a_search(\n    collection_name,\n    query_vector,\n    k=10,\n    vector_name=None,\n    **kwargs,\n)\n</code></pre> <p>Search for chunks in a collection by their query vector.</p>"},{"location":"API%20Reference/Vectorstore/qdrant_vectorstore/#datapizza.vectorstores.qdrant.QdrantVectorstore.add","title":"add","text":"<pre><code>add(chunk, collection_name=None)\n</code></pre> <p>Add a single chunk or list of chunks to the vectorstore. Args:     chunk (Chunk | list[Chunk]): The chunk or list of chunks to add.     collection_name (str, optional): The name of the collection to add the chunks to. Defaults to None.</p>"},{"location":"API%20Reference/Vectorstore/qdrant_vectorstore/#datapizza.vectorstores.qdrant.QdrantVectorstore.create_collection","title":"create_collection","text":"<pre><code>create_collection(collection_name, vector_config, **kwargs)\n</code></pre> <p>Create a new collection in Qdrant if it doesn't exist with the specified vector configurations</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>Name of the collection to create</p> required <code>vector_config</code> <code>list[VectorConfig]</code> <p>List of vector configurations specifying dimensions and distance metrics</p> required <code>**kwargs</code> <p>Additional arguments to pass to Qdrant's create_collection</p> <code>{}</code>"},{"location":"API%20Reference/Vectorstore/qdrant_vectorstore/#datapizza.vectorstores.qdrant.QdrantVectorstore.delete_collection","title":"delete_collection","text":"<pre><code>delete_collection(collection_name, **kwargs)\n</code></pre> <p>Delete a collection in Qdrant.</p>"},{"location":"API%20Reference/Vectorstore/qdrant_vectorstore/#datapizza.vectorstores.qdrant.QdrantVectorstore.dump_collection","title":"dump_collection","text":"<pre><code>dump_collection(\n    collection_name, page_size=100, with_vectors=False\n)\n</code></pre> <p>Dumps all points from a collection in a chunk-wise manner.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>Name of the collection to dump.</p> required <code>page_size</code> <code>int</code> <p>Number of points to retrieve per batch.</p> <code>100</code> <code>with_vectors</code> <code>bool</code> <p>Whether to include vectors in the dumped chunks.</p> <code>False</code> <p>Yields:</p> Name Type Description <code>Chunk</code> <code>Chunk</code> <p>A chunk object from the collection.</p>"},{"location":"API%20Reference/Vectorstore/qdrant_vectorstore/#datapizza.vectorstores.qdrant.QdrantVectorstore.get_collections","title":"get_collections","text":"<pre><code>get_collections()\n</code></pre> <p>Get all collections in Qdrant.</p>"},{"location":"API%20Reference/Vectorstore/qdrant_vectorstore/#datapizza.vectorstores.qdrant.QdrantVectorstore.remove","title":"remove","text":"<pre><code>remove(collection_name, ids, **kwargs)\n</code></pre> <p>Remove chunks from a collection by their IDs. Args:     collection_name (str): The name of the collection to remove the chunks from.     ids (list[str]): The IDs of the chunks to remove.     **kwargs: Additional keyword arguments to pass to the Qdrant client.</p>"},{"location":"API%20Reference/Vectorstore/qdrant_vectorstore/#datapizza.vectorstores.qdrant.QdrantVectorstore.retrieve","title":"retrieve","text":"<pre><code>retrieve(collection_name, ids, **kwargs)\n</code></pre> <p>Retrieve chunks from a collection by their IDs. Args:     collection_name (str): The name of the collection to retrieve the chunks from.     ids (list[str]): The IDs of the chunks to retrieve.     **kwargs: Additional keyword arguments to pass to the Qdrant client. Returns:     list[Chunk]: The list of chunks retrieved from the collection.</p>"},{"location":"API%20Reference/Vectorstore/qdrant_vectorstore/#datapizza.vectorstores.qdrant.QdrantVectorstore.search","title":"search","text":"<pre><code>search(\n    collection_name,\n    query_vector,\n    k=10,\n    vector_name=None,\n    **kwargs,\n)\n</code></pre> <p>Search for chunks in a collection by their query vector.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>The name of the collection to search in.</p> required <code>query_vector</code> <code>list[float]</code> <p>The query vector to search for.</p> required <code>k</code> <code>int</code> <p>The number of results to return. Defaults to 10.</p> <code>10</code> <code>vector_name</code> <code>str</code> <p>The name of the vector to search for. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the Qdrant client.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[Chunk]</code> <p>list[Chunk]: The list of chunks found in the collection.</p>"},{"location":"API%20Reference/Vectorstore/qdrant_vectorstore/#usage","title":"Usage","text":"<pre><code>from datapizza.vectorstores.qdrant import QdrantVectorstore\n\n# Connect to Qdrant server\nvectorstore = QdrantVectorstore(\n    host=\"localhost\",\n    port=6333,\n    api_key=\"your-api-key\"  # Optional\n)\n\n# Or use in-memory/file storage\nvectorstore = QdrantVectorstore(\n    location=\":memory:\"  # Or path to file\n)\n</code></pre>"},{"location":"API%20Reference/Vectorstore/qdrant_vectorstore/#features","title":"Features","text":"<ul> <li>Connect to Qdrant server or use local storage</li> <li>Support for both dense and sparse embeddings</li> <li>Named vector configurations for multi-vector collections</li> <li>Batch operations for efficient processing</li> <li>Collection management (create, delete, list)</li> <li>Chunk-based operations with metadata preservation</li> <li>Async support for all operations</li> <li>Point-level operations (add, update, remove, retrieve)</li> </ul>"},{"location":"API%20Reference/Vectorstore/qdrant_vectorstore/#examples","title":"Examples","text":""},{"location":"API%20Reference/Vectorstore/qdrant_vectorstore/#basic-setup-and-collection-creation","title":"Basic Setup and Collection Creation","text":"<pre><code>from datapizza.core.vectorstore import Distance, VectorConfig\nfrom datapizza.type import EmbeddingFormat\nfrom datapizza.vectorstores.qdrant import QdrantVectorstore\n\nvectorstore = QdrantVectorstore(location=\":memory:\")\n\n# Create collection with vector configuration\nvector_config = [\n    VectorConfig(\n        name=\"text_embeddings\",\n        dimensions=3,\n        format=EmbeddingFormat.DENSE,\n        distance=Distance.COSINE\n    )\n]\n\nvectorstore.create_collection(\n    collection_name=\"documents\",\n    vector_config=vector_config\n)\n\n# Add nodes and search\n\nimport uuid\nfrom datapizza.type import Chunk, DenseEmbedding\nfrom datapizza.vectorstores.qdrant import QdrantVectorstore\n\n# Create chunks with embeddings\nchunks = [\n    Chunk(\n        id=str(uuid.uuid4()),\n        text=\"First document content\",\n        metadata={\"source\": \"doc1.txt\"},\n        embeddings=[DenseEmbedding(name=\"text_embeddings\", vector=[0.1, 0.2, 0.3])]\n    ),\n    Chunk(\n        id=str(uuid.uuid4()),\n        text=\"Second document content\",\n        metadata={\"source\": \"doc2.txt\"},\n        embeddings=[DenseEmbedding(name=\"text_embeddings\", vector=[0.4, 0.5, 0.6])]\n    )\n]\n\n# Add chunks to collection\nvectorstore.add(chunks, collection_name=\"documents\")\n\n# Search for similar chunks\nquery_vector = [0.1, 0.2, 0.3]\nresults = vectorstore.search(\n    collection_name=\"documents\",\n    query_vector=query_vector,\n    k=5\n)\n\nfor chunk in results:\n    print(f\"Text: {chunk.text}\")\n    print(f\"Metadata: {chunk.metadata}\")\n</code></pre>"},{"location":"Guides/agent/","title":"Agent","text":"<p>The <code>Agent</code> class is the core component for creating autonomous AI agents in Datapizza AI. It handles task execution, tool management, memory, and planning.</p>"},{"location":"Guides/agent/#basic-usage","title":"Basic Usage","text":"<pre><code>from datapizza.agents import Agent\nfrom datapizza.clients.openai import OpenAIClient\nfrom datapizza.memory import Memory\nfrom datapizza.tools import Tool\n\nagent = Agent(\n    name=\"my_agent\",\n    system_prompt=\"You are a helpful assistant\",\n    client=OpenAIClient(api_key=\"YOUR_API_KEY\", model=\"gpt-4o-mini\"),\n    # tools=[],\n    # max_steps=10,\n    # terminate_on_text=True,  # Terminate execution when the client return a plain text \n    # memory=memory,\n    # stream=False,\n    # planning_interval=0\n)\n\nres = agent.run(\"Hi\")\nprint(res.text)\n</code></pre>"},{"location":"Guides/agent/#use-tools","title":"Use Tools","text":"<p>The above agent is quite basic, so let's make it more functional by adding tools.</p> <pre><code>from datapizza.agents import Agent\nfrom datapizza.clients.openai import OpenAIClient\nfrom datapizza.memory import Memory\nfrom datapizza.tools import Tool\nfrom datapizza.tools import tool\n\n\n@tool\ndef get_weather(location: str, when: str) -&gt; str:\n    \"\"\"Retrieves weather information for a specified location and time.\"\"\"\n    return \"25 \u00b0C\"\n\nagent = Agent(name= \"weather_agent\",tools=[get_weather], client=OpenAIClient(api_key=\"YOUR_API_KEY\", model=\"gpt-4o-mini\"))\nresponse = agent.run(\"What's the weather tomorrow in Milan?\")\n\nprint(response.text)\n# Output:\n# Tomorrow in Milan, the temperature will be 25 \u00b0C.\n</code></pre>"},{"location":"Guides/agent/#tool_choice","title":"tool_choice","text":"<p>You can set the parameter <code>tool_choice</code> at invoke time.</p> <p>The accepted values are :  <code>auto</code>, <code>required</code>, <code>none</code>, <code>required_first</code>,  <code>list[\"tool_name\"]</code></p> <pre><code>res = master_agent.run(\n    task_input=\"what is the weather in milan?\", tool_choice=\"required_first\"\n)\n</code></pre> <ul> <li><code>auto</code> : the model will decide if use a tool or not.</li> <li><code>required_first</code> : force to use a tool only at the first step, then auto.</li> <li><code>required</code> : force to use a tool at every step.</li> <li><code>none</code> : force to not use any tool.</li> </ul>"},{"location":"Guides/agent/#core-methods","title":"Core Methods","text":""},{"location":"Guides/agent/#sync-run","title":"Sync run","text":"<p><code>run(task_input: str, tool_choice = \"auto\", **kwargs) -&gt; str</code> Execute a task and return the final result.</p> <pre><code>result = agent.run(\"What's the weather like today?\")\nprint(result.text)  # \"The weather is sunny with 25\u00b0C\"\n</code></pre>"},{"location":"Guides/agent/#stream-invoke","title":"Stream invoke","text":"<p>Stream the agent's execution process, yielding intermediate steps. (Do not stream the single answer)</p> <pre><code>from datapizza.agents.agent import Agent, StepResult\nfrom datapizza.clients.openai import OpenAIClient\nfrom datapizza.memory import Memory\nfrom datapizza.tools import Tool\nfrom datapizza.tools import tool\n\n@tool\ndef get_weather(location: str, when: str) -&gt; str:\n    \"\"\"Retrieves weather information for a specified location and time.\"\"\"\n    return \"25 \u00b0C\"\n\nagent = Agent(name= \"weather_agent\",tools=[get_weather], client=OpenAIClient(api_key=\"YOUR_API_KEY\", model=\"gpt-4o-mini\"))\n\nfor step in agent.stream_invoke(\"What's the weather tomorrow in Milan?\"):\n    print(f\"Step {step.index} starting...\")\n    print(step.text)\n</code></pre>"},{"location":"Guides/agent/#async-run","title":"Async run","text":"<p><code>a_run(task_input: str, **kwargs) -&gt; str</code> Async version of run.</p> <pre><code>import asyncio\n\nasync def main():\n\n    agent = Agent(name= \"agent\", client=OpenAIClient(api_key=\"YOUR_API_KEY\", model=\"gpt-4o-mini\"))\n    return await agent.a_run(\"Process this request\")\n\n\nres = asyncio.run(main())\nprint(res.text)\n</code></pre>"},{"location":"Guides/agent/#async-stream-invoke","title":"Async stream invoke","text":"<p><code>a_stream_invoke(task_input: str, **kwargs) -&gt; AsyncGenerator[str | StepResult, None]</code> Stream the agent's execution process, yielding intermediate steps. (Do not stream the single answer)</p> <pre><code>from datapizza.agents.agent import Agent\nfrom datapizza.clients.openai import OpenAIClient\nimport asyncio\n\nasync def get_response():\n    client = OpenAIClient(api_key=\"YOUR_API_KEY\", model=\"gpt-4o-mini\")\n    agent = Agent(name= \"joke_agent\",client=client)\n    async for step in agent.a_stream_invoke(\"tell me a joke\"):\n        print(f\"Step {step.index} starting...\")\n        print(step.text)\n\nasyncio.run(get_response())\n</code></pre>"},{"location":"Guides/agent/#multi-agent-communication","title":"Multi-Agent Communication","text":"<p>An agent can call another ones using <code>can_call</code> method</p> <pre><code>from datapizza.agents.agent import Agent\nfrom datapizza.clients.openai import OpenAIClient\nfrom datapizza.tools import tool\n\nclient = OpenAIClient(api_key=\"YOUR_API_KEY\", model=\"gpt-4.1\")\n\n@tool\ndef get_weather(city: str) -&gt; str:\n    return f\"\"\" Monday's weather in {city} is cloudy. \n                Tuesday's weather in {city} is rainy.\n                Wednesday's weather in {city} is sunny\n                Thursday's weather in {city} is cloudy, \n                Friday's weather in {city} is rainy, \n                Saturday's weather in {city} is sunny \n                and Sunday's weather in {city} is cloudy.\"\"\"\n\nweather_agent = Agent(\n    name=\"weather_expert\",\n    client=client,\n    system_prompt=\"You are a weather expert. Provide detailed weather information and forecasts.\",\n    tools=[get_weather]\n)\n\nplanner_agent = Agent(\n    name=\"planner\",\n    client=client, \n    system_prompt=\"You are a trip planner. Use weather and analysis info to make recommendations.\"\n)\n\nplanner_agent.can_call(weather_agent)\n\nresponse = planner_agent.run(\n    \"I need to plan a hiking trip in Seattle next week. Can you help analyze the weather and make recommendations?\"\n)\nprint(response.text)\n</code></pre> <p>Alternatively, you can define a tool that manually calls the agent. The two solutions are more or less identical.</p> <pre><code>from datapizza.agents import Agent\nfrom datapizza.clients.openai import OpenAIClient\nfrom datapizza.tools import tool\n\nclass MasterAgent(Agent):\n    system_prompt=\"You are a master agent. You can call the weather expert to get weather information.\"\n    name=\"master_agent\"\n\n    @tool\n    def call_weather_expert(self, task_to_ask: str) -&gt; str:\n        @tool\n        def get_weather(city: str) -&gt; str:\n            return f\"\"\" Monday's weather in {city} is cloudy. \n                        Tuesday's weather in {city} is rainy.\n                        Wednesday's weather in {city} is sunny\n                        Thursday's weather in {city} is cloudy, \n                        Friday's weather in {city} is rainy, \n                        Saturday's weather in {city} is sunny \n                        and Sunday's weather in {city} is cloudy.\"\"\"\n\n        weather_agent = Agent(\n            name=\"weather_expert\",\n            client=OpenAIClient(api_key=\"YOUR_API_KEY\", model=\"gpt-4.1\"),\n            system_prompt=\"You are a weather expert. Provide detailed weather information and forecasts.\",\n            tools=[get_weather]\n        )\n        res = weather_agent.run(task_to_ask)\n        return res.text\n\nmaster_agent = MasterAgent(\n    client=OpenAIClient(api_key=\"YOUR_API_KEY\", model=\"gpt-4.1\"),\n)\n\nmaster_agent.run(\"What is the weather in Rome?\")\n</code></pre>"},{"location":"Guides/agent/#planning-system","title":"Planning System","text":"<p>When <code>planning_interval &gt; 0</code>, the agent creates execution plans at regular intervals:</p> <p>During the planning stages, the agent spends time thinking about what the next steps are to be taken to achieve the task.</p> <pre><code>agent = Agent(\n    name=\"Agent_with_plan\",\n    client=client,\n    planning_interval=3,  # Plan every 3 steps\n)\n</code></pre> <p>The planning system generates structured plans that help the agent organize complex tasks.</p>"},{"location":"Guides/agent/#stream-output-response","title":"Stream output response","text":"<pre><code>from datapizza.agents import Agent\nfrom datapizza.clients.openai import OpenAIClient\nfrom datapizza.core.clients import ClientResponse\nfrom datapizza.tools import tool\n\nclient = OpenAIClient(api_key=\"YOUR_API_KEY\", model=\"gpt-4.1\")\n\nagent = Agent(\n    name=\"Big_boss\",\n    client=client,\n    system_prompt=\"You are a helpful assistant that answers questions based on the provided context.\",\n    stream=True, # With stream=True, the agent will stream the client resposne, not only the intermediate steps\n\n)\n\nfor r in agent.stream_invoke(\"What is the weather in Milan?\"):\n    if isinstance(r, ClientResponse):\n        print(r.delta, end=\"\", flush=True)\n</code></pre>"},{"location":"Guides/Clients/chatbot/","title":"Real example: Chatbot","text":"<p>Learn how to build conversational AI applications using the OpenAI client with memory management, context awareness, and advanced chatbot patterns.</p>"},{"location":"Guides/Clients/chatbot/#basic-chatbot","title":"Basic Chatbot","text":"<p>Clients need memory to maintain context and have meaningful conversations. The Memory class stores and manages conversation history, allowing the AI to reference previous exchanges and maintain coherent dialogue.</p> <p>Here's a simple example of a chatbot with memory:</p> <pre><code>from datapizza.clients.openai import OpenAIClient\nfrom datapizza.memory import Memory\nfrom datapizza.type import ROLE, TextBlock\n\nclient = OpenAIClient(\n    api_key=\"your-api-key\",\n    model=\"gpt-4o-mini\",\n    system_prompt=\"You are a helpful assistant\"\n)\n\ndef simple_chatbot():\n    \"\"\"Basic chatbot with conversation memory.\"\"\"\n\n    memory = Memory()\n\n    print(\"Chatbot: Hello! I'm here to help. Type 'quit' to exit.\")\n\n    while True:\n        user_input = input(\"\\nYou: \")\n\n        if user_input.lower() in ['quit', 'exit', 'bye']:\n            print(\"Chatbot: Goodbye!\")\n            break\n\n        # Get AI response with memory context\n        response = client.invoke(user_input, memory=memory)\n        print(f\"Chatbot: {response.text}\")\n\n        # Update conversation memory\n        memory.add_turn(TextBlock(content=user_input), role=ROLE.USER)\n        memory.add_turn(response.content, role=ROLE.ASSISTANT)\n\n# Run the chatbot\nsimple_chatbot()\n</code></pre>"},{"location":"Guides/Clients/local_model/","title":"Running with Ollama","text":"<p>Datapizza AI supports running with local models through Ollama, providing you with complete control over your AI infrastructure while maintaining privacy and reducing costs.</p>"},{"location":"Guides/Clients/local_model/#prerequisites","title":"Prerequisites","text":"<p>Before getting started, you'll need to have Ollama installed and running on your system.</p>"},{"location":"Guides/Clients/local_model/#installing-ollama","title":"Installing Ollama","text":"<ol> <li>Download and Install Ollama</li> <li>Visit ollama.ai and download the installer for your operating system</li> <li> <p>Follow the installation instructions for your platform</p> </li> <li> <p>Start Ollama Service <pre><code># Ollama typically starts automatically after installation\n# If not, you can start it manually:\nollama serve\n</code></pre></p> </li> <li> <p>Pull a Model <pre><code># Pull the Gemma 2B model (lightweight option)\nollama pull gemma2:2b\n\n# Or pull Gemma 7B for better performance\nollama pull gemma2:7b\n\n# Or pull Llama 3.1 8B\nollama pull llama3.1:8b\n</code></pre></p> </li> </ol>"},{"location":"Guides/Clients/local_model/#installation","title":"Installation","text":"<p>Install the Datapizza AI OpenAI-like client:</p> <pre><code>pip install datapizza-ai-clients-openai-like\n</code></pre>"},{"location":"Guides/Clients/local_model/#basic-usage","title":"Basic Usage","text":"<p>Here's a simple example of how to use Datapizza AI with Ollama:</p> <pre><code>import os\nfrom datapizza.clients.openai_like import OpenAILikeClient\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Create client for Ollama\nclient = OpenAILikeClient(\n    api_key=\"\",  # Ollama doesn't require an API key\n    model=\"gemma2:2b\",  # Use any model you've pulled with Ollama\n    system_prompt=\"You are a helpful assistant.\",\n    base_url=\"http://localhost:11434/v1\",  # Default Ollama API endpoint\n)\n\n# Simple query\nresponse = client.invoke(\"What is the capital of France?\")\nprint(response.content)\n</code></pre>"},{"location":"Guides/Clients/multimodality/","title":"Multimodality","text":"<p>The clients supports various media types including images and PDFs, allowing you to create rich multimodal applications.</p>"},{"location":"Guides/Clients/multimodality/#supported-media-types","title":"Supported Media Types","text":"Media Type Supported Formats Source Types Images PNG, JPEG, GIF, WebP File path, URL, base64 PDFs PDF documents File path, base64"},{"location":"Guides/Clients/multimodality/#basic-image-input","title":"Basic Image Input","text":""},{"location":"Guides/Clients/multimodality/#single-image-from-file","title":"Single Image from File","text":"<pre><code>from datapizza.clients.openai import OpenAIClient\nfrom datapizza.type import Media, MediaBlock, TextBlock\n\nclient = OpenAIClient(\n    api_key=\"your-api-key\",\n    model=\"gpt-4o\"  # Vision models required for images\n)\n\n# Create image media object\nimage = Media(\n    media_type=\"image\",\n    source_type=\"path\",\n    source=\"image.png\", # Use the correct path\n    extension=\"png\"\n)\n\n# Create media block\nmedia_block = MediaBlock(media=image)\ntext_block = TextBlock(content=\"What do you see in this image?\")\n\n# Send multimodal input\nresponse = client.invoke(\n    input=[text_block, media_block],\n    max_tokens=200\n)\n\nprint(response.text)\n</code></pre>"},{"location":"Guides/Clients/multimodality/#image-from-url","title":"Image from URL","text":"<pre><code># Image from URL\nimage_url = Media(\n    media_type=\"image\",\n    source_type=\"url\",\n    source=\"https://example.com/image.png\",\n    extension=\"png\"\n)\n\nresponse = client.invoke(\n    input=[\n        TextBlock(content=\"Describe this image\"),\n        MediaBlock(media=image_url)\n    ]\n)\nprint(response.text)\n</code></pre>"},{"location":"Guides/Clients/multimodality/#image-from-base64","title":"Image from Base64","text":"<pre><code>import base64\n\n# Read and encode image\nwith open(\"image.jpg\", \"rb\") as image_file:\n    base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n\nimage_b64 = Media(\n    media_type=\"image\",\n    source_type=\"base64\",\n    source=base64_image,\n    extension=\"png\"\n)\n\nresponse = client.invoke(\n    input=[\n        TextBlock(content=\"Analyze this image\"),\n        MediaBlock(media=image_b64)\n    ]\n)\nprint(response.text)\n</code></pre>"},{"location":"Guides/Clients/multimodality/#multiple-images","title":"Multiple Images","text":"<p>Compare or analyze multiple images in a single request:</p> <pre><code># Multiple images for comparison\nimage1 = Media(\n    media_type=\"image\",\n    source_type=\"path\",\n    source=\"before.png\",\n    extension=\"png\"\n)\n\nimage2 = Media(\n    media_type=\"image\",\n    source_type=\"path\",\n    source=\"after.png\",\n    extension=\"png\"\n)\n\nresponse = client.invoke(\n    input=[\n        TextBlock(content=\"Compare these two images and describe the differences\"),\n        MediaBlock(media=image1),\n        MediaBlock(media=image2)\n    ],\n    max_tokens=300\n)\n\nprint(response.text)\n</code></pre>"},{"location":"Guides/Clients/multimodality/#working-with-pdfs","title":"Working with PDFs","text":"<pre><code># PDF from file path\npdf_doc = Media(\n    media_type=\"pdf\",\n    source_type=\"path\",\n    source=\"document.pdf\",\n    extension=\"pdf\"\n)\n\nresponse = client.invoke(\n    input=[\n        TextBlock(content=\"Summarize the key points from this document\"),\n        MediaBlock(media=pdf_doc)\n    ],\n    max_tokens=500\n)\n\nprint(response.text)\n</code></pre>"},{"location":"Guides/Clients/multimodality/#working-with-audio","title":"Working with Audio","text":"<p>Google handle audio inline</p> <pre><code>pip install datapizza-ai-clients-google\n</code></pre> <pre><code>from datapizza.clients.google import GoogleClient\nfrom datapizza.type import Media, MediaBlock, TextBlock\n\nclient = GoogleClient(\n    api_key=\"YOUR_API_KEY\",\n    model=\"gemini-2.0-flash-exp\"\n)\n# PDF from file path\nmedia = Media(\n    media_type=\"audio\",\n    source_type=\"path\",\n    source=\"sample.mp3\",\n    extension=\"mp3\"\n)\n\nresponse = client.invoke(\n    input=[\n        TextBlock(content=\"Summarize the key points from this audio file\"),\n        MediaBlock(media=media)\n    ],\n)\n\nprint(response.text)\n</code></pre>"},{"location":"Guides/Clients/quick_start/","title":"Quick Start","text":"<p>This guide will help you get started with the <code>OpenAIClient</code> in datapizza-ai. For specialized topics, check out our detailed guides on multimodality, streaming and building chatbots.</p>"},{"location":"Guides/Clients/quick_start/#installation","title":"Installation","text":"<p>First, make sure you have datapizza-ai installed:</p> <pre><code>pip install datapizza-ai\n</code></pre>"},{"location":"Guides/Clients/quick_start/#basic-setup","title":"Basic Setup","text":"<pre><code>from datapizza.clients.openai import OpenAIClient\n\n# Initialize the client with your API key\nclient = OpenAIClient(\n    api_key=\"your-openai-api-key\",\n    model=\"gpt-4o-mini\",  # Default model\n    system_prompt=\"You are a helpful assistant\",  # Optional\n    temperature=0.7  # Optional, controls randomness (0-2)\n)\n</code></pre> <pre><code># Basic text response\nresponse = client.invoke(\"What is the capital of France?\")\nprint(response.text)\n# Output: \"The capital of France is Paris.\"\n</code></pre>"},{"location":"Guides/Clients/quick_start/#core-methods","title":"Core Methods","text":"<pre><code>response = client.invoke(\n    input=\"Explain quantum computing in simple terms\",\n    temperature=0.5,  # Override default temperature\n    max_tokens=200,   # Limit response length\n    system_prompt=\"You are a physics teacher\"  # Override system prompt\n)\n\nprint(response.text)\nprint(f\"Tokens used: {response.completion_tokens_used}\")\n</code></pre>"},{"location":"Guides/Clients/quick_start/#async-invoke","title":"Async invoke","text":"<pre><code>import asyncio\n\nasync def main():\n    return await client.a_invoke(\n        input=\"Explain quantum computing in simple terms\",\n        temperature=0.5,  # Override default temperature\n        max_tokens=200,   # Limit response length\n        system_prompt=\"You are a physics teacher\"  # Override system prompt\n    )\n\nresponse = asyncio.run(main())\n\nprint(response.text)\nprint(f\"Tokens used: {response.completion_tokens_used}\")\n</code></pre>"},{"location":"Guides/Clients/quick_start/#working-with-memory","title":"Working with Memory","text":"<p>Memory allows you to maintain conversation context:</p> <pre><code>from datapizza.memory import Memory\nfrom datapizza.type import ROLE, TextBlock\n\nmemory = Memory()\n\n# First interaction\nresponse1 = client.invoke(\"My name is Alice\", memory=memory)\nmemory.add_turn(TextBlock(content=\"My name is Alice\"), role=ROLE.USER)\nmemory.add_turn(response1.content, role=ROLE.ASSISTANT)\n\n# Second interaction - the model remembers Alice\nresponse2 = client.invoke(\"What's my name?\", memory=memory)\nprint(response2.text)  # Should mention Alice\n</code></pre>"},{"location":"Guides/Clients/quick_start/#token-management","title":"Token Management","text":"<p>Monitor your usage:</p> <pre><code>response = client.invoke(\"Explain AI\")\nprint(f\"Tokens used: {response.completion_tokens_used}\")\nprint(f\"Prompt token used: {response.prompt_tokens_used}\")\nprint(f\"Cached token used: {response.cached_tokens_used}\")\n</code></pre> <p>That's it! You're ready to start building with the OpenAI client. Check out the specialized guides above for advanced features and patterns.</p>"},{"location":"Guides/Clients/quick_start/#whats-next","title":"What's Next?","text":"<p>Now that you know the basics, explore our specialized guides:</p>"},{"location":"Guides/Clients/quick_start/#multimodality-guide","title":"\ud83d\udcf8 Multimodality Guide","text":"<p>Work with images, PDFs, and other media types for visual AI applications.</p>"},{"location":"Guides/Clients/quick_start/#streaming-guide","title":"\ud83c\udf0a Streaming Guide","text":"<p>Build responsive applications with real-time text generation and streaming.</p>"},{"location":"Guides/Clients/quick_start/#tools-guide","title":"\ud83d\udee0\ufe0f Tools Guide","text":"<p>Extend AI capabilities by integrating external functions and tools.</p>"},{"location":"Guides/Clients/quick_start/#structured-responses-guide","title":"\ud83d\udcca Structured Responses Guide","text":"<p>Work with strongly-typed outputs using JSON schemas and Pydantic models.</p>"},{"location":"Guides/Clients/quick_start/#chatbot-guide","title":"\ud83e\udd16 Chatbot Guide","text":"<p>Create sophisticated conversational AI with memory and context management.</p>"},{"location":"Guides/Clients/streaming/","title":"Streaming","text":"<p>Streaming allows you to receive responses in real-time as they're generated, providing a better user experience for long responses and interactive applications.</p>"},{"location":"Guides/Clients/streaming/#why-use-streaming","title":"Why Use Streaming?","text":"<ul> <li>Real-time feedback: Users see responses as they're generated</li> <li>Better UX: Reduces perceived latency for long responses</li> <li>Progressive display: Show partial results immediately</li> <li>Interruptible: Can stop generation early if needed</li> </ul>"},{"location":"Guides/Clients/streaming/#basic-streaming","title":"Basic Streaming","text":""},{"location":"Guides/Clients/streaming/#synchronous-streaming","title":"Synchronous Streaming","text":"<pre><code>from datapizza.clients.openai import OpenAIClient\n\nclient = OpenAIClient(\n    api_key=\"your-api-key\",\n    model=\"gpt-4o-mini\"\n)\n\n# Basic streaming\nfor chunk in client.stream_invoke(\"Write a short story about a robot learning to paint\"):\n    if chunk.delta:\n        print(chunk.delta, end=\"\", flush=True)\nprint()  # New line when complete\n</code></pre>"},{"location":"Guides/Clients/streaming/#asynchronous-streaming","title":"Asynchronous Streaming","text":"<pre><code>import asyncio\n\nasync def async_stream_example():\n    async for chunk in client.a_stream_invoke(\"Explain quantum computing in simple terms\"):\n        if chunk.delta:\n            print(chunk.delta, end=\"\", flush=True)\n    print()  # New line when complete\n\n# Run the async function\nasyncio.run(async_stream_example())\n</code></pre>"},{"location":"Guides/Clients/structured_responses/","title":"Structured Responses","text":"<p>Generate structured, typed data from AI responses using Pydantic models. This ensures consistent output format and enables easy data validation.</p>"},{"location":"Guides/Clients/structured_responses/#basic-usage","title":"Basic Usage","text":""},{"location":"Guides/Clients/structured_responses/#simple-model","title":"Simple Model","text":"<pre><code>from datapizza.clients.openai import OpenAIClient\nfrom pydantic import BaseModel\n\nclient = OpenAIClient(api_key=\"your-api-key\", model=\"gpt-4o-mini\")\n\nclass Person(BaseModel):\n    name: str\n    age: int\n    occupation: str\n\nresponse = client.structured_response(\n    input=\"Create a profile for a software engineer\",\n    output_cls=Person\n)\n\nperson = response.structured_data[0]\nprint(f\"Name: {person.name}\")\nprint(f\"Age: {person.age}\")\nprint(f\"Occupation: {person.occupation}\")\n</code></pre>"},{"location":"Guides/Clients/structured_responses/#complex-models","title":"Complex Models","text":"<pre><code>from typing import List\nfrom pydantic import BaseModel, Field\n\nclass Product(BaseModel):\n    name: str\n    price: float = Field(gt=0, description=\"Price must be positive\")\n    tags: List[str]\n    in_stock: bool\n\nclass Store(BaseModel):\n    name: str\n    location: str\n    products: List[Product]\n\nresponse = client.structured_response(\n    input=\"Create a tech store with 3 products\",\n    output_cls=Store\n)\n\nstore = response.structured_data[0]\nprint(f\"Store: {store.name}\")\nfor product in store.products:\n    print(f\"- {product.name}: ${product.price}\")\n</code></pre>"},{"location":"Guides/Clients/structured_responses/#data-extraction","title":"Data Extraction","text":""},{"location":"Guides/Clients/structured_responses/#extract-information-from-text","title":"Extract Information from Text","text":"<pre><code>class ContactInfo(BaseModel):\n    name: str\n    email: str\n    phone: str\n    company: str\n\ntext = \"\"\"\nHi, I'm John Smith from TechCorp. \nYou can reach me at john.smith@techcorp.com or call 555-0123.\n\"\"\"\n\nresponse = client.structured_response(\n    input=f\"Extract contact information from this text: {text}\",\n    output_cls=ContactInfo\n)\n\ncontact = response.structured_data[0]\nprint(f\"Contact: {contact.name} at {contact.company}\")\n</code></pre>"},{"location":"Guides/Clients/structured_responses/#analyze-and-categorize","title":"Analyze and Categorize","text":"<pre><code>from enum import Enum\n\nclass Sentiment(str, Enum):\n    POSITIVE = \"positive\"\n    NEGATIVE = \"negative\"\n    NEUTRAL = \"neutral\"\n\nclass TextAnalysis(BaseModel):\n    sentiment: Sentiment\n    confidence: float = Field(ge=0, le=1)\n    key_topics: List[str]\n    summary: str\n\nreview = \"This product is amazing! Great quality and fast shipping.\"\n\nresponse = client.structured_response(\n    input=f\"Analyze this review: {review}\",\n    output_cls=TextAnalysis\n)\n\nanalysis = response.structured_data[0]\nprint(f\"Sentiment: {analysis.sentiment}\")\nprint(f\"Confidence: {analysis.confidence}\")\nprint(f\"Topics: {', '.join(analysis.key_topics)}\")\n</code></pre>"},{"location":"Guides/Clients/tools/","title":"Tools","text":"<p>Tools allow AI models to call external functions, enabling them to perform actions, retrieve data, and interact with external systems.</p>"},{"location":"Guides/Clients/tools/#basic-tool-usage","title":"Basic Tool Usage","text":""},{"location":"Guides/Clients/tools/#simple-tool","title":"Simple Tool","text":"<pre><code>from datapizza.clients.openai import OpenAIClient\nfrom datapizza.tools import tool\n\nclient = OpenAIClient(api_key=\"your-api-key\", model=\"gpt-4o-mini\")\n\n@tool\ndef get_weather(location: str) -&gt; str:\n    \"\"\"Get the current weather for a location.\"\"\"\n    # Simulate weather API call\n    return f\"The weather in {location} is sunny and 72\u00b0F\"\n\n# Use the tool\nresponse = client.invoke(\n    \"What's the weather in New York?\",\n    tools=[get_weather]\n)\n\n# Execute tool calls\nfor func_call in response.function_calls:\n    result = func_call.tool(**func_call.arguments)\n    print(f\"Tool result: {result}\")\n\nprint(response.text)\n</code></pre>"},{"location":"Guides/Clients/tools/#multiple-tools","title":"Multiple Tools","text":"<pre><code>@tool\ndef calculate(expression: str) -&gt; str:\n    \"\"\"Calculate a mathematical expression.\"\"\"\n    try:\n        result = eval(expression)  # Note: Use safe evaluation in production\n        return str(result)\n    except Exception as e:\n        return f\"Error: {e}\"\n\n@tool\ndef get_time() -&gt; str:\n    \"\"\"Get the current time.\"\"\"\n    from datetime import datetime\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n# Use multiple tools\nresponse = client.invoke(\n    \"What time is it and what's 15 * 8?\",\n    tools=[get_time, calculate]\n)\n\n# Execute all tool calls\nfor func_call in response.function_calls:\n    result = func_call.tool(**func_call.arguments)\n    print(f\"{func_call.name}: {result}\")\n</code></pre>"},{"location":"Guides/Clients/tools/#tool-choice-control","title":"Tool Choice Control","text":""},{"location":"Guides/Clients/tools/#auto-default","title":"Auto (Default)","text":"<p>Let the model decide when to use tools:</p> <pre><code>response = client.invoke(\n    \"Hello, how are you?\",\n    tools=[get_weather],\n    tool_choice=\"auto\"  # Model may or may not use tools\n)\n</code></pre>"},{"location":"Guides/Clients/tools/#required","title":"Required","text":"<p>Force the model to use a tool:</p> <pre><code>response = client.invoke(\n    \"Get weather information\",\n    tools=[get_weather],\n    tool_choice=\"required\"  # Model must use a tool\n)\n</code></pre>"},{"location":"Guides/Clients/tools/#none","title":"None","text":"<p>Disable tool usage:</p> <pre><code>response = client.invoke(\n    \"What's the weather like?\",\n    tools=[get_weather],\n    tool_choice=\"none\"  # Model won't use tools\n)\n</code></pre>"},{"location":"Guides/Clients/tools/#specific-tool","title":"Specific Tool","text":"<p>Force a specific tool:</p> <pre><code>response = client.invoke(\n    \"Check the weather\",\n    tools=[get_weather, get_time],\n    tool_choice=[\"get_weather\"]  # Only use this specific tool\n)\n</code></pre>"},{"location":"Guides/Monitoring/log/","title":"Log level","text":"<p>With the variables <code>DATAPIZZA_LOG_LEVEL</code> and <code>DATAPIZZA_AGENT_LOG_LEVEL</code> you can change the log levels of the master logger and the agent logger</p> <p>Allowed values are:</p> <ul> <li><code>DEBUG</code></li> <li><code>INFO</code></li> <li><code>WARN</code></li> <li><code>ERROR</code></li> </ul> <p>The default values are:</p> <ul> <li><code>DATAPIZZA_LOG_LEVEL=INFO</code></li> <li><code>DATAPIZZA_AGENT_LOG_LEVEL=INFO</code></li> </ul>"},{"location":"Guides/Monitoring/tracing/","title":"Tracing","text":"<p>The tracing module provides an easy-to-use interface for collecting and displaying OpenTelemetry traces with rich console output. It's designed to help developers monitor performance and understand the execution flow of their applications.</p>"},{"location":"Guides/Monitoring/tracing/#features","title":"Features","text":"<ul> <li>In-memory trace collection - Stores spans in memory for fast access</li> <li>Context-aware tracking - Only collects spans for explicitly tracked operations</li> <li>Thread-safe operations - Safe for use in multi-threaded applications</li> <li>OpenTelemetry integration - Works with standard OpenTelemetry instrumentation</li> </ul>"},{"location":"Guides/Monitoring/tracing/#quick-start","title":"Quick Start","text":"<p>The simplest way to use tracing is with the <code>tracer</code> context manager:</p> <pre><code>from datapizza.tracing import ContextTracing\n\n\n# Basic tracing\nwith ContextTracing().trace(\"trace_name\"):\n    # Your code here\n    result = some_datapizza_operations()\n\n# Output will show:\n# \u256d\u2500 Trace Summary of my_operation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u256e\n# \u2502 Total Spans: 3                                                    \u2502\n# \u2502 Duration: 2.45s                                                   \u2502\n# \u2502 \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n# \u2502 \u2503 Model       \u2503 Prompt Tokens \u2503 Completion Tokens \u2503 Cached Tokens \u2503\n# \u2502 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n# \u2502 \u2502 gpt-4o-mini \u2502 31            \u2502 27                \u2502 0             \u2502\n# \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n# \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"Guides/Monitoring/tracing/#clients-trace-inputoutputmemory","title":"Clients trace input/output/memory","text":"<p>If you want to log the input/output and the memory passed to client invoke you should set the env variable</p> <p><code>DATAPIZZA_TRACE_CLIENT_IO=TRUE</code></p> <p>default is <code>FALSE</code></p>"},{"location":"Guides/Monitoring/tracing/#manual-span-creation","title":"Manual Span Creation","text":"<p>For more granular control, create spans manually:</p> <pre><code>from opentelemetry import trace\nfrom datapizza.tracing import ContextTracing\n\ntracer = trace.get_tracer(__name__)\n\nwith ContextTracing().trace(\"trace_name\"):\n    with tracer.start_as_current_span(\"database_query\"):\n        # Database operation\n        data = fetch_from_database()\n\n    with tracer.start_as_current_span(\"data_validation\"):\n        # Validation logic\n        validate_data(data)\n\n    with tracer.start_as_current_span(\"business_logic\"):\n        # Core business logic\n        result = process_business_rules(data)\n</code></pre>"},{"location":"Guides/Monitoring/tracing/#adding-external-exporters","title":"Adding External Exporters","text":"<p>The tracing module uses in-memory storage by default, but you can easily add external exporters to send traces to other systems.</p>"},{"location":"Guides/Monitoring/tracing/#create-the-resource","title":"Create the resource","text":"<p>First of all you should set the trace provider</p> <pre><code>from opentelemetry.sdk.resources import Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor, SimpleSpanProcessor\nfrom opentelemetry.semconv.resource import ResourceAttributes\n\nresource = Resource.create(\n   {\n       ResourceAttributes.SERVICE_NAME: \"your_service_name\",\n   }\n)\ntrace.set_tracer_provider(TracerProvider(resource=resource))\n</code></pre>"},{"location":"Guides/Monitoring/tracing/#zipkin-integration","title":"Zipkin Integration","text":"<p>Export traces to Zipkin for visualization and analysis:</p> <p><code>pip install opentelemetry-exporter-zipkin</code></p> <p>After setting the trace provider you can add the exporters</p> <pre><code>from opentelemetry import trace\nfrom opentelemetry.exporter.zipkin.json import ZipkinExporter\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.semconv.resource import ResourceAttributes\n\nzipkin_url = \"http://localhost:9411/api/v2/spans\"\n\nzipkin_exporter = ZipkinExporter(\n    endpoint=zipkin_url,\n)\n\ntracer_provider = trace.get_tracer_provider()\n\nspan_processor = SimpleSpanProcessor(zipkin_exporter)\ntrace.get_tracer_provider().add_span_processor(span_processor)\n\n# Now all traces will be sent to both in-memory storage and Zipkin\n</code></pre>"},{"location":"Guides/Monitoring/tracing/#otlp-opentelemetry-protocol","title":"OTLP (OpenTelemetry Protocol)","text":"<p>Export to any OTLP-compatible backend (Grafana, Datadog, etc.):</p> <pre><code>from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\n\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.semconv.resource import ResourceAttributes\n\notlp_exporter = OTLPSpanExporter(\n    endpoint=\"http://localhost:4317\",\n    headers={\"authorization\": \"Bearer your-token\"}\n)\n\nspan_processor = BatchSpanProcessor(otlp_exporter)\ntrace.get_tracer_provider().add_span_processor(span_processor)\n</code></pre>"},{"location":"Guides/Monitoring/tracing/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Use <code>BatchSpanProcessor</code> for external exporters in production</li> <li>Set reasonable limits on span attributes and events</li> <li>Monitor memory usage with many active traces</li> </ul> <pre><code># Production configuration\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\n\n# Batch spans for better performance\nbatch_processor = BatchSpanProcessor(\n    exporter,\n    max_queue_size=2048,\n    schedule_delay_millis=5000,\n    max_export_batch_size=512,\n)\n\ntrace.get_tracer_provider().add_span_processor(batch_processor)\n</code></pre>"},{"location":"Guides/Pipeline/functional_pipeline/","title":"Functional Pipeline","text":"<p>WARNING:  This module is in beta. Signatures and interfaces may change in future releases.</p> <p>The <code>FunctionalPipeline</code> module provides a flexible way to build data processing pipelines with complex dependency graphs. It allows you to define reusable processing nodes and connect them in various patterns including sequential execution, branching, parallel execution, and foreach loops.</p>"},{"location":"Guides/Pipeline/functional_pipeline/#core-components","title":"Core Components","text":""},{"location":"Guides/Pipeline/functional_pipeline/#dependency","title":"Dependency","text":"<p>Defines how data flows between Nodes:</p> <pre><code>@dataclass\nclass Dependency:\n    node_name: str\n    input_key: str | None = None\n    target_key: str | None = None\n</code></pre> <ul> <li><code>node_name</code>: The name of the node to get data from</li> <li><code>input_key</code>: Optional key for extracting a specific part of the node's output</li> <li><code>target_key</code>: The key under which to store the data in the receiving node's input</li> </ul>"},{"location":"Guides/Pipeline/functional_pipeline/#functionalpipeline","title":"FunctionalPipeline","text":"<p>The main class for building and executing pipelines:</p> <pre><code>class FunctionalPipeline:\n    def __init__(self):\n        self.nodes = []\n</code></pre>"},{"location":"Guides/Pipeline/functional_pipeline/#building-pipelines","title":"Building Pipelines","text":""},{"location":"Guides/Pipeline/functional_pipeline/#sequential-execution","title":"Sequential Execution","text":"<pre><code>pipeline = FunctionalPipeline()\npipeline.run(\"load_data\", DataLoader(), kwargs={\"filepath\": \"data.csv\"})\npipeline.then(\"transform\", Transformer(), target_key=\"data\")\npipeline.then(\"save\", Saver(), target_key=\"transformed_data\")\n</code></pre>"},{"location":"Guides/Pipeline/functional_pipeline/#branching","title":"Branching","text":"<pre><code>pipeline.branch(\n    condition=is_valid_data,\n    if_true=valid_data_pipeline,\n    if_false=invalid_data_pipeline,\n    dependencies=[Dependency(node_name=\"validate\", target_key=\"validation_result\")]\n)\n</code></pre>"},{"location":"Guides/Pipeline/functional_pipeline/#foreach-loop","title":"Foreach Loop","text":"<pre><code>pipeline.foreach(\n    name=\"process_items\",\n    do=item_processing_pipeline,\n    dependencies=[Dependency(node_name=\"get_items\")]\n)\n</code></pre>"},{"location":"Guides/Pipeline/functional_pipeline/#executing-pipelines","title":"Executing Pipelines","text":"<pre><code>result = pipeline.execute(\n    initial_data={\"load_data\": {\"filepath\": \"override.csv\"}},\n    context={\"existing_data\": {...}}\n)\n</code></pre>"},{"location":"Guides/Pipeline/functional_pipeline/#yaml-configuration","title":"YAML Configuration","text":"<p>You can define pipelines in YAML and load them at runtime: This is useful for separating pipeline structure from code</p> <pre><code>modules:\n  - name: data_loader\n    module: my_package.loaders\n    type: CSVLoader\n    params:\n      encoding: \"utf-8\"\n\n  - name: transformer\n    module: my_package.transformers\n    type: StandardTransformer\n\npipeline:\n  - type: run\n    name: load_data\n    node: data_loader\n    kwargs:\n      filepath: \"data.csv\"\n\n  - type: then\n    name: transform\n    node: transformer\n    target_key: data\n</code></pre> <p>Load the pipeline:</p> <pre><code>pipeline = FunctionalPipeline.from_yaml(\"pipeline_config.yaml\")\nresult = pipeline.execute()\n</code></pre>"},{"location":"Guides/Pipeline/functional_pipeline/#real-world-examples","title":"Real-world Examples","text":""},{"location":"Guides/Pipeline/functional_pipeline/#question-answering-pipeline","title":"Question Answering Pipeline","text":"<p>Here's an example of a question answering pipeline that uses embeddings to retrieve relevant information and an LLM to generate a response:</p> <p>Define the components: <pre><code>from datapizza.clients.google import GoogleClient\nfrom datapizza.clients.openai import OpenAIClient\nfrom datapizza.core.vectorstore import VectorConfig\nfrom datapizza.embedders.openai import OpenAIEmbedder\nfrom datapizza.modules.prompt import ChatPromptTemplate\nfrom datapizza.modules.rewriters import ToolRewriter\nfrom datapizza.pipeline import Dependency, FunctionalPipeline\nfrom datapizza.vectorstores.qdrant import QdrantVectorstore\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nrewriter = ToolRewriter(\n    client=OpenAIClient(\n        model=\"gpt-4o\",\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        system_prompt=\"Use only 1 time the tool to answer the user prompt.\",\n    )\n)\nembedder = OpenAIEmbedder(\n    api_key=os.getenv(\"OPENAI_API_KEY\"), model_name=\"text-embedding-3-small\"\n)\n\nvector_store = QdrantVectorstore(host=\"localhost\", port=6333)\nvector_store.create_collection(collection_name=\"my_documents\", vector_config=[VectorConfig(dimensions=1536, name=\"vector_name\")])\nvector_store = vector_store.as_module_component() # required to use the vectorstore in the pipeline\n\nprompt_template = ChatPromptTemplate(\n    user_prompt_template=\"this is a user prompt: {{ user_prompt }}\",\n    retrieval_prompt_template=\"{% for chunk in chunks %} Relevant chunk: {{ chunk.text }} \\n\\n {% endfor %}\",\n)\ngenerator = GoogleClient(\n    api_key=os.getenv(\"GOOGLE_API_KEY\"),\n    system_prompt=\"You are a senior Software Engineer. You are given a user prompt and you need to answer it given the context of the chunks.\",\n).as_module_component()\n</code></pre></p> <p>And now create and execute the pipeline</p> <pre><code>pipeline = (FunctionalPipeline()\n    .run(name=\"rewriter\", node=rewriter, kwargs={\"user_prompt\": \"tell me something about this document\"})\n    .then(name=\"embedder\", node=embedder, target_key=\"text\")\n    .then(name=\"vector_store\", node=vector_store, target_key=\"query_vector\", \n          kwargs={\"collection_name\": \"my_documents\", \"k\": 4})\n    .then(name=\"prompt_template\", node=prompt_template, target_key=\"chunks\" , kwargs={\"user_prompt\": \"tell me something about this document\"})\n    .then(name=\"generator\", node=generator, target_key=\"memory\", kwargs={\"input\": \"tell me something about this document\"})\n    .get(\"generator\")\n)\n\nresult = pipeline.execute()\nprint(result)\n</code></pre> <p>When using <code>.then()</code>, the <code>target_key</code> parameter specifies the input parameter name for the current node's <code>run()</code> method that will receive the output from the previous node. In other words, <code>target_key</code> defines how the previous node's output gets mapped into the current node's <code>run()</code> method parameters.</p> <p>This pipeline:</p> <ol> <li>Rewrites/processes the user query</li> <li>Creates embeddings from the processed query</li> <li>Retrieves relevant chunks from a vector database</li> <li>Creates a prompt template with the retrieved context</li> <li>Generates a response using an LLM</li> <li>Returns the generated response</li> </ol>"},{"location":"Guides/Pipeline/functional_pipeline/#branch-and-loop-usage-example","title":"Branch and loop usage example","text":"<pre><code>from datapizza.core.models import PipelineComponent\nfrom datapizza.pipeline import Dependency, FunctionalPipeline\n\n\nclass Scraper(PipelineComponent):\n    def _run(self, number_of_links: int = 1):\n        return [\"example.com\"] * number_of_links\n\nclass UpperComponent(PipelineComponent):\n    def _run(self, item):\n        return item.upper()\n\nclass SendNotification(PipelineComponent):\n    def _run(self ):\n        return \"No Url found, Notification sent\"\n\nsend_notification = FunctionalPipeline().run(name=\"send_notification\", node=SendNotification())\n\nupper_elements = FunctionalPipeline().foreach(\n    name=\"loop_links\",\n    dependencies=[Dependency(node_name=\"get_link\")],\n    do=UpperComponent(),\n)\n\npipeline = (\n    FunctionalPipeline()\n    .run(name=\"get_link\", node=Scraper())\n    .branch(\n        condition=lambda pipeline_context: len(pipeline_context.get(\"get_link\")) &gt; 0,\n        dependencies=[Dependency(node_name=\"get_link\")],\n        if_true=upper_elements,\n        if_false=send_notification,\n    )\n)\n\nresults = pipeline.execute(initial_data={\"get_link\": {\"number_of_links\": 0}}) # put 1 to test the other branch\nprint(results)\n</code></pre>"},{"location":"Guides/Pipeline/ingestion_pipeline/","title":"Ingestion Pipeline","text":"<p>The <code>IngestionPipeline</code> provides a streamlined way to process documents, transform them into nodes (chunks of text with metadata), generate embeddings, and optionally store them in a vector database. It allows chaining various components like parsers, captioners, splitters, and embedders to create a customizable document processing workflow.</p>"},{"location":"Guides/Pipeline/ingestion_pipeline/#core-concepts","title":"Core Concepts","text":"<ul> <li>Components: These are the processing steps in the pipeline, typically inheriting from <code>datapizza.core.models.PipelineComponent</code>. Each component implements a <code>process</code> method to perform a specific task like parsing a document, splitting text, or generating embeddings. Components are executed sequentially via their <code>__call__</code> method in the order they are provided.</li> <li>Vector Store: An optional component responsible for storing the final nodes and their embeddings.</li> <li>Nodes: The fundamental unit of data passed between components. A node usually represents a chunk of text (e.g., a paragraph, a table summary) along with its associated metadata and embeddings.</li> </ul>"},{"location":"Guides/Pipeline/ingestion_pipeline/#available-components","title":"Available Components","text":"<p>The pipeline typically supports components for:</p> <ol> <li>Parsers: Convert raw documents (PDF, DOCX, etc.) into structured <code>Node</code> objects (e.g., <code>AzureParser</code>, <code>UnstructuredParser</code>).</li> <li>Captioners: Enhance nodes representing images or tables with textual descriptions using models like LLMs (e.g., <code>LLMCaptioner</code>).</li> <li>Splitters: Divide nodes into smaller chunks based on their content (e.g., <code>NodeSplitter</code>, <code>PdfImageSplitter</code>).</li> <li>Embedders: Create chunk embeddings for semantic search and similarity matching (e.g., <code>NodeEmbedder</code>, <code>ClientEmbedder</code>).<ul> <li><code>ChunkEmbedder</code>: Batch processing for efficient embedding of multiple nodes.</li> </ul> </li> <li>Vector Stores: Store and retrieve embeddings efficiently using vector databases (e.g., <code>QdrantVectorstore</code>).</li> </ol> <p>Refer to the specific documentation for each component type (e.g., in <code>datapizza.parsers</code>, <code>datapizza.embedders</code>) for details on their specific parameters and usage. Remember that pipeline components typically inherit from <code>PipelineComponent</code> and implement the <code>_run</code> method.</p>"},{"location":"Guides/Pipeline/ingestion_pipeline/#configuration-methods","title":"Configuration Methods","text":"<p>There are two main ways to configure and use the <code>IngestionPipeline</code>:</p>"},{"location":"Guides/Pipeline/ingestion_pipeline/#1-programmatic-configuration","title":"1. Programmatic Configuration","text":"<p>Define and configure the pipeline directly within your Python code. This offers maximum flexibility.</p> <pre><code>from datapizza.clients.openai import OpenAIClient\nfrom datapizza.core.vectorstore import VectorConfig\nfrom datapizza.embedders import ChunkEmbedder\nfrom datapizza.modules.parsers.docling import DoclingParser\nfrom datapizza.modules.splitters import NodeSplitter\nfrom datapizza.pipeline.pipeline import IngestionPipeline\nfrom datapizza.vectorstores.qdrant import QdrantVectorstore\n\nvector_store = QdrantVectorstore(\n    location=\":memory:\" # or set host and port\n)\nvector_store.create_collection(collection_name=\"datapizza\", vector_config=[VectorConfig(dimensions=1536, name=\"vector_name\")])\n\npipeline = IngestionPipeline(\n    modules=[\n        DoclingParser(),\n        NodeSplitter(max_char=2000),\n        ChunkEmbedder(client=OpenAIClient(api_key=\"OPENAI_API_KEY\", model=\"text-embedding-3-small\"), model_name=\"text-embedding-3-small\", embedding_name=\"small\"),\n    ],\n    vector_store=vector_store,\n    collection_name=\"datapizza\",\n)\n\npipeline.run(file_path=\"sample.pdf\")\n\nprint(vector_store.search(query_vector= [0.0]*1536, collection_name=\"datapizza\", k=4))\n</code></pre>"},{"location":"Guides/Pipeline/ingestion_pipeline/#2-yaml-configuration","title":"2. YAML Configuration","text":"<p>Define the entire pipeline structure, components, and their parameters in a YAML file. This is useful for managing configurations separately from code.</p> <pre><code>from datapizza.pipeline.pipeline import IngestionPipeline\nimport os\n\n# Load pipeline from YAML\npipeline = IngestionPipeline().from_yaml(\"path/to/your/config.yaml\")\n\n# Run the pipeline (Ensure necessary ENV VARS for the YAML config are set)\npipeline.run(file_path=\"path/to/your/document.pdf\")\n</code></pre>"},{"location":"Guides/Pipeline/ingestion_pipeline/#example-yaml-configuration-configyaml","title":"Example YAML Configuration (<code>config.yaml</code>)","text":"<pre><code>constants:\n  EMBEDDING_MODEL: \"text-embedding-3-small\"\n  CHUNK_SIZE: 1000\n\ningestion_pipeline:\n  clients:\n    openai_embedder:\n      provider: openai\n      model: \"${EMBEDDING_MODEL}\"\n      api_key: \"${OPENAI_API_KEY}\"\n\n  modules:\n    - name: parser\n      type: DoclingParser\n      module: datapizza.modules.parsers.docling\n    - name: splitter\n      type: NodeSplitter\n      module: datapizza.modules.splitters\n      params:\n        max_char: ${CHUNK_SIZE}\n    - name: embedder\n      type: ChunkEmbedder\n      module: datapizza.embedders\n      params:\n        client: openai_embedder\n\n  vector_store:\n    type: QdrantVectorstore\n    module: datapizza.vectorstores.qdrant\n    params:\n      host: \"localhost\"\n      port: 6333\n\n  collection_name: \"my_documents\"\n</code></pre> <p>Key points for YAML configuration:</p> <ul> <li>Environment Variables: Use <code>${VAR_NAME}</code> syntax within strings to securely load secrets or configuration from environment variables. Ensure these variables are set in your execution environment.</li> <li>Clients: Define shared clients (like <code>OpenAIClient</code>) under the <code>clients</code> key and reference them by name within module <code>params</code>.</li> <li>Modules: List components under <code>modules</code>. Each requires <code>type</code> (class name) and <code>module</code> (Python path to the class). <code>params</code> are passed to the component's constructor (<code>__init__</code>). Components should generally inherit from <code>PipelineComponent</code>.</li> <li>Vector Store: Configure the optional vector store similarly to modules.</li> <li>Collection Name: Must be provided if a <code>vector_store</code> is configured.</li> </ul>"},{"location":"Guides/Pipeline/ingestion_pipeline/#pipeline-execution-run-method","title":"Pipeline Execution (<code>run</code> method)","text":"<pre><code>pipeline.run(file_path=f, metadata={\"name\": f, \"type\": \"md\"})\n</code></pre>"},{"location":"Guides/Pipeline/ingestion_pipeline/#async-execution-a_run-method","title":"Async Execution (<code>a_run</code> method)","text":"<p>IngestionPipeline support async run NB: Every modules should implement <code>_a_run</code> method to run the async pipeline.</p> <pre><code>await pipeline.a_run(file_path=f, metadata={\"name\": f, \"type\": \"md\"})\n</code></pre>"},{"location":"Guides/Pipeline/retrieval_pipeline/","title":"DagPipeline","text":"<p>The <code>DagPipeline</code> class allows you to define and execute a series of processing steps (modules) organized as a Directed Acyclic Graph (DAG). Modules typically inherit from <code>datapizza.core.models.PipelineComponent</code> or are simple callables. This enables complex workflows where the output of one module can be selectively used as input for others.</p>"},{"location":"Guides/Pipeline/retrieval_pipeline/#core-concepts","title":"Core Concepts","text":""},{"location":"Guides/Pipeline/retrieval_pipeline/#modules","title":"Modules","text":"<p>Modules are the building blocks of the pipeline. They are typically instances of classes inheriting from <code>datapizza.core.models.PipelineComponent</code> (which requires implementing a <code>run</code> and  <code>a_run</code> method), <code>datapizza.core.models.ChainableProducer</code> (which exposes an <code>as_module_component</code> method returning a <code>PipelineComponent</code>), or simply Python callables.</p> <pre><code>from datapizza.core.models import PipelineComponent\nfrom datapizza.pipeline import DagPipeline\n\nclass MyProcessingStep(PipelineComponent):\n    # Inheriting from PipelineComponent provides the __call__ wrapper for logging\n    def _run(self, input_data: str) -&gt; str:\n        return something\n\n    async _a_run(self, something: str) -&gt; str:\n        return await do_stuff()\n</code></pre>"},{"location":"Guides/Pipeline/retrieval_pipeline/#connections","title":"Connections","text":"<p>Connections define the flow of data between modules. You specify which module's output connects to which module's input.</p> <ul> <li><code>from_node_name</code>: The name of the source module.</li> <li><code>to_node_name</code>: The name of the target module.</li> <li><code>source_key</code> (Optional): If the source module's <code>process</code> method (or callable) returns a dictionary, this key specifies which value from the dictionary should be passed. If <code>None</code>, the entire output of the source module is passed.</li> <li><code>target_key</code> : This key specifies the argument name in the target module's <code>process</code> method (or callable) that should receive the data. If <code>None</code>, and the source output is not a dictionary, the data is passed as the first non-<code>self</code> argument to the target's <code>_run</code> method/callable. If <code>None</code> and the source output is a dictionary, its key-value pairs are merged into the target's input keyword arguments.</li> </ul> <pre><code>from datapizza.clients.openai import OpenAIClient\nfrom datapizza.core.models import PipelineComponent\nfrom datapizza.core.vectorstore import VectorConfig\nfrom datapizza.embedders.openai import OpenAIEmbedder\nfrom datapizza.modules.prompt import ChatPromptTemplate\nfrom datapizza.modules.rewriters import ToolRewriter\nfrom datapizza.pipeline import DagPipeline\nfrom datapizza.vectorstores.qdrant import QdrantVectorstore\n\nclient = OpenAIClient(api_key=\"OPENAI_API_KEY\", model=\"gpt-4o-mini\")\nvector_store = QdrantVectorstore(location=\":memory:\")\nvector_store.create_collection(collection_name=\"my_documents\", vector_config=[VectorConfig(dimensions=1536, name=\"vector_name\")])\n\npipeline = DagPipeline()\n\npipeline.add_module(\"rewriter\", ToolRewriter(client=client, system_prompt=\"rewrite the query to perform a better search in a vector database\"))\npipeline.add_module(\"embedder\", OpenAIEmbedder(api_key=\"OPENAI_API_KEY\", model_name=\"text-embedding-3-small\"))\npipeline.add_module(\"vector_store\", vector_store)\npipeline.add_module(\"prompt_template\", ChatPromptTemplate(user_prompt_template = \"this is a user prompt: {{ user_prompt }}\", retrieval_prompt_template = \"{% for chunk in chunks %} Relevant chunk: {{ chunk.text }} \\n\\n {% endfor %}\"))\npipeline.add_module(\"llm\", OpenAIClient(model = \"gpt-4o-mini\", api_key = \"OPENAI_API_KEY\"))\n\n\npipeline.connect(\"rewriter\", \"embedder\", target_key=\"text\")\npipeline.connect(\"embedder\", \"vector_store\", target_key=\"query_vector\")\npipeline.connect(\"vector_store\", \"prompt_template\", target_key=\"chunks\")\npipeline.connect(\"prompt_template\", \"llm\", target_key=\"memory\")\n</code></pre>"},{"location":"Guides/Pipeline/retrieval_pipeline/#running-the-pipeline","title":"Running the Pipeline","text":"<p>The <code>run</code> method executes the pipeline based on the defined connections. It requires an initial <code>data</code> dictionary which provides the missing input arguments for the nodes that require them.</p> <p>The keys of this dictionary should match the names of the modules requiring initial input, and the values should be dictionaries mapping argument names to values for their respective <code>process</code> methods (or callables).</p> <pre><code>user_input = \"tell me something about this document\"\nres = pipeline.run(\n    {\n        \"rewriter\": {\"user_prompt\": user_input},\n\n        # Embedder doesn't require any input because it's provided by the rewriter\n\n        \"prompt_template\": {\"user_prompt\": user_input},  # Prompt template requires user_prompt\n        \"vector_store\": {\n            \"collection_name\": \"my_documents\",\n            \"k\": 10,\n        },\n        \"llm\": {\n            \"input\": user_input,\n            \"system_prompt\": \"You are a helpful assistant. try to answer user questions given the context\",\n        },\n    }\n)\nresult = res.get(\"llm\").text\nprint(result)\n</code></pre> <p>The pipeline automatically determines the execution order based on dependencies. It executes modules by calling their <code>run</code> method only when all their prerequisites (connected <code>from_node_name</code> modules) have completed successfully.</p>"},{"location":"Guides/Pipeline/retrieval_pipeline/#async-run","title":"Async run","text":"<p>Pipeline support async run with  <code>a_run</code> With async run, the pipeline will call a_run of modules.</p> <p>This only works if you are using a remote qdrant server. The in-memory qdrant function does not work with asynchronous execution. <pre><code>res = await pipeline.a_run(\n    {\n        \"rewriter\": {\"user_prompt\": user_input},\n        \"prompt_template\": {\"user_prompt\": user_input},\n        \"vector_store\": {\n            \"collection_name\": \"datapizza\",\n            \"k\": 10,\n        },\n        \"llm\": {\n            \"input\": user_input,\n            \"system_prompt\": \"You are a helpful assistant. try to answer user questions given the context\",\n        },\n    }\n)\n</code></pre></p>"},{"location":"Guides/Pipeline/retrieval_pipeline/#configuration-via-yaml","title":"Configuration via YAML","text":"<p>Pipelines can be defined entirely using a YAML configuration file, which is loaded using the <code>from_yaml</code> method. This is useful for separating pipeline structure from code.</p> <p>The YAML structure includes sections for <code>clients</code> (like LLM providers), <code>modules</code>, and <code>connections</code>.</p> <pre><code>from datapizza.pipeline import DagPipeline\n\npipeline = DagPipeline().from_yaml(\"dag_pipeline.yaml\")\nuser_input = \"tell me something about this document\"\nres = pipeline.run(\n    {\n        \"rewriter\": {\"user_prompt\": user_input},\n        \"prompt_template\": {\"user_prompt\": user_input},\n        \"vector_store\": {\"collection_name\": \"my_documents\",\"k\": 10,},\n        \"llm\": {\"input\": user_input,\"system_prompt\": \"You are a helpful assistant. try to answer user questions given the context\",},\n    }\n)\nresult = res.get(\"llm\").text\nprint(result)\n</code></pre>"},{"location":"Guides/Pipeline/retrieval_pipeline/#example-yaml-dag_configyaml","title":"Example YAML (<code>dag_config.yaml</code>)","text":"<pre><code>dag_pipeline:\n  clients:\n    openai_client:\n      provider: openai\n      model: \"gpt-4o-mini\"\n      api_key: ${OPENAI_API_KEY}\n    google_client:\n      provider: google\n      model: \"gemini-2.0\"\n      api_key: ${GOOGLE_API_KEY}\n    openai_embedder:\n      provider: openai\n      model: \"text-embedding-3-small\"\n      api_key: ${OPENAI_API_KEY}\n\n  modules:\n    - name: rewriter\n      type: ToolRewriter\n      module: datapizza.modules.rewriters\n      params:\n        client: openai_client\n        system_prompt: \"rewrite the query to perform a better search in a vector database\"\n    - name: embedder\n      type: ClientEmbedder\n      module: datapizza.embedders\n      params:\n        client: openai_embedder\n    - name: vector_store\n      type: QdrantVectorstore\n      module: datapizza.vectorstores.qdrant\n      params:\n        host: localhost\n    - name: prompt_template\n      type: ChatPromptTemplate\n      module: datapizza.modules.prompt\n      params:\n        user_prompt_template: \"this is a user prompt: {{ user_prompt }}\"\n        retrieval_prompt_template: \"{% for chunk in chunks %} Relevant chunk: {{ chunk.text }} \\n\\n {% endfor %}\"\n    - name: llm\n      type: OpenAIClient\n      module: datapizza.clients.openai\n      params:\n        model: \"gpt-4o-mini\"\n        api_key: ${OPENAI_API_KEY}\n\n  connections:\n\n    - from: rewriter\n      to: embedder\n      target_key: text\n    - from: embedder\n      to: vector_store\n      target_key: query_vector\n    - from: vector_store\n      to: prompt_template\n      target_key: chunks\n    - from: prompt_template\n      to: llm\n      target_key: memory\n</code></pre> <p>Key points for YAML configuration:</p> <ul> <li>Environment Variables: Use <code>${VAR_NAME}</code> syntax to load sensitive information like API keys from environment variables.</li> <li>Clients: Define clients once and reference them by name in module <code>params</code>.</li> <li>Module Loading: Specify the <code>module</code> path and <code>type</code> (class name) for dynamic loading. The class should generally be a <code>PipelineComponent</code>.</li> <li>Parameters: <code>params</code> are passed directly to the module's constructor.</li> <li>Connections: Define data flow similarly to the programmatic <code>connect</code> method.</li> </ul>"},{"location":"Guides/RAG/rag/","title":"Build a RAG","text":"<p>This guide demonstrates how to build a complete RAG (Retrieval-Augmented Generation) system using datapizza-ai's pipeline architecture. We'll cover both the ingestion pipeline for processing and storing documents, and the DagPipeline for retrieval and response generation.</p>"},{"location":"Guides/RAG/rag/#overview","title":"Overview","text":"<p>A RAG system consists of two main phases:</p> <ol> <li>Ingestion: Process documents, split them into chunks, generate embeddings, and store in a vector database</li> <li>Retrieval: Query the vector database, retrieve relevant chunks, and generate responses</li> </ol> <p>datapizza-ai provides specialized pipeline components for each phase:</p> <ul> <li>IngestionPipeline: Sequential processing for document ingestion</li> <li>DagPipeline: Graph-based processing for complex retrieval workflows</li> </ul>"},{"location":"Guides/RAG/rag/#part-1-document-ingestion-pipeline","title":"Part 1: Document Ingestion Pipeline","text":"<p>The ingestion pipeline processes raw documents and stores them in a vector database. Here's a complete example:</p>"},{"location":"Guides/RAG/rag/#basic-ingestion-setup","title":"Basic Ingestion Setup","text":"<pre><code>pip install datapizza-ai-parsers-docling\n</code></pre> <pre><code>from datapizza.clients.openai import OpenAIClient\nfrom datapizza.core.vectorstore import VectorConfig\nfrom datapizza.embedders import ChunkEmbedder\nfrom datapizza.embedders.openai import OpenAIEmbedder\nfrom datapizza.modules.captioners import LLMCaptioner\nfrom datapizza.modules.parsers.docling import DoclingParser\nfrom datapizza.modules.splitters import NodeSplitter\nfrom datapizza.pipeline import IngestionPipeline\nfrom datapizza.vectorstores.qdrant import QdrantVectorstore\n\nvectorstore = QdrantVectorstore(location=\":memory:\")\nvectorstore.create_collection(\n    \"my_documents\",\n    vector_config=[VectorConfig(name=\"embedding\", dimensions=1536)]\n)\n\nembedder_client = OpenAIEmbedder(\n    api_key=\"YOUR_API_KEY\",\n    model_name=\"text-embedding-3-small\",\n)\n\ningestion_pipeline = IngestionPipeline(\n    modules=[\n        DoclingParser(), # choose between Docling, Azure or TextParser to parse plain text\n\n        #LLMCaptioner(\n        #    client=OpenAIClient(api_key=\"YOUR_API_KEY\"),\n        #), # This is optional, add it if you want to caption the media\n\n        NodeSplitter(max_char=1000),             # Split Nodes into Chunks\n        ChunkEmbedder(client=embedder_client),   # Add embeddings to Chunks\n    ],\n    vector_store=vectorstore,\n    collection_name=\"my_documents\"\n)\n\ningestion_pipeline.run(\"sample.pdf\", metadata={\"source\": \"user_upload\"})\n\nres = vectorstore.search(\n    query_vector = [0.0] * 1536,\n    collection_name=\"my_documents\",\n    k=2,\n)\nprint(res)\n</code></pre>"},{"location":"Guides/RAG/rag/#configuration-based-ingestion","title":"Configuration-Based Ingestion","text":"<p>You can also define your pipeline using YAML configuration:</p> <pre><code>constants:\n  EMBEDDING_MODEL: \"text-embedding-3-small\"\n  CHUNK_SIZE: 1000\n\ningestion_pipeline:\n  clients:\n    openai_embedder:\n      provider: openai\n      model: \"${EMBEDDING_MODEL}\"\n      api_key: \"${OPENAI_API_KEY}\"\n\n  modules:\n    - name: parser\n      type: DoclingParser\n      module: datapizza.modules.parsers.docling\n    - name: splitter\n      type: NodeSplitter\n      module: datapizza.modules.splitters\n      params:\n        max_char: ${CHUNK_SIZE}\n    - name: embedder\n      type: ChunkEmbedder\n      module: datapizza.embedders\n      params:\n        client: openai_embedder\n\n  vector_store:\n    type: QdrantVectorstore\n    module: datapizza.vectorstores.qdrant\n    params:\n      host: \"localhost\"\n      port: 6333\n\n  collection_name: \"my_documents\"\n</code></pre> <p>Load and use the configuration:</p> <pre><code>from datapizza.pipeline import IngestionPipeline\n\n# Make sure the collection exists before running the pipeline\npipeline = IngestionPipeline().from_yaml(\"ingestion_pipeline.yaml\")\npipeline.run(\"sample.pdf\")\n</code></pre>"},{"location":"Guides/RAG/rag/#part-2-retrieval-with-dagpipeline","title":"Part 2: Retrieval with DagPipeline","text":"<p>The DagPipeline enables complex retrieval workflows with query rewriting, embedding, and response generation.</p>"},{"location":"Guides/RAG/rag/#basic-retrieval-setup","title":"Basic Retrieval Setup","text":"<pre><code>from datapizza.clients.openai import OpenAIClient\nfrom datapizza.embedders.openai import OpenAIEmbedder\nfrom datapizza.modules.prompt import ChatPromptTemplate\nfrom datapizza.modules.rewriters import ToolRewriter\nfrom datapizza.pipeline import DagPipeline\nfrom datapizza.vectorstores.qdrant import QdrantVectorstore\n\nopenai_client = OpenAIClient(\n    model=\"gpt-4o-mini\",\n    api_key=\"YOUR_API_KEY\"\n)\n\nquery_rewriter = ToolRewriter(\n    client=openai_client,\n    system_prompt=\"Rewrite user queries to improve retrieval accuracy.\"\n)\n\nembedder = OpenAIEmbedder(\n    api_key=\"YOUR_API_KEY\",\n    model_name=\"text-embedding-3-small\"\n)\n\n# Use the same qdrant of ingestion (prefer host and port instead of location when possible)\nretriever = QdrantVectorstore(location=\":memory:\")\nretriever.create_collection(\n    \"my_documents\",\n    vector_config=[VectorConfig(name=\"embedding\", dimensions=1536)]\n)\n\nprompt_template = ChatPromptTemplate(\n    user_prompt_template=\"User question: {{user_prompt}}\\n:\",\n    retrieval_prompt_template=\"Retrieved content:\\n{% for chunk in chunks %}{{ chunk.text }}\\n{% endfor %}\"\n)\n\ndag_pipeline = DagPipeline()\ndag_pipeline.add_module(\"rewriter\", query_rewriter)\ndag_pipeline.add_module(\"embedder\", embedder)\ndag_pipeline.add_module(\"retriever\", retriever)\ndag_pipeline.add_module(\"prompt\", prompt_template)\ndag_pipeline.add_module(\"generator\", openai_client)\n\ndag_pipeline.connect(\"rewriter\", \"embedder\", target_key=\"text\")\ndag_pipeline.connect(\"embedder\", \"retriever\", target_key=\"query_vector\")\ndag_pipeline.connect(\"retriever\", \"prompt\", target_key=\"chunks\")\ndag_pipeline.connect(\"prompt\", \"generator\", target_key=\"memory\")\n\nquery = \"tell me something about this document\"\nresult = dag_pipeline.run({\n    \"rewriter\": {\"user_prompt\": query},\n    \"prompt\": {\"user_prompt\": query},\n    \"retriever\": {\"collection_name\": \"my_documents\", \"k\": 3},\n    \"generator\":{\"input\": query}\n})\n\nprint(f\"Generated response: {result['generator']}\")\n</code></pre>"},{"location":"Guides/RAG/Evaluation/evaluation_cookbook/","title":"Practical Guide with TechCrunch","text":"<p>This cookbook provides a step-by-step walkthrough of evaluating a Retrieval Augmented Generation (RAG) pipeline using a practical example based on the TechCrunch dataset. It demonstrates how to implement various evaluation metrics and techniques discussed in the RAG Evaluation Overview. A full version of the entire code presented in this cookbook can be downloaded as a Jupyter notebook from this link.</p> <p>The goal is to bridge the gap between theoretical evaluation concepts and their concrete application in a Python environment. We will cover data loading, ingestion, retrieval, answer generation, and a comprehensive suite of evaluation metrics.</p>"},{"location":"Guides/RAG/Evaluation/evaluation_cookbook/#0-prerequisites-and-setup","title":"0. Prerequisites and Setup","text":"<p>Before diving into the code, ensure you have the necessary environment and dependencies set up. The script relies on several libraries, including <code>datapizza-ai</code> (having <code>qdrant-client</code>, <code>openai</code>, <code>google-generativeai</code>, <code>tiktoken</code>, <code>cohere</code>), <code>tqdm</code>, <code>numpy</code>, <code>pandas</code>, <code>matplotlib</code>, and <code>seaborn</code>.</p> <p>Environment variables for API keys (OpenAI, Google, Qdrant, Cohere) must be configured, typically in a <code>.env</code> file. Ask your DevOps to get these credentials!</p> <pre><code># Ensure you have a .env file in your project root or parent directories with:\n# OPENAI_API_KEY=\"your_openai_api_key\"\n# GOOGLE_API_KEY=\"your_google_api_key\"\n# QDRANT_URL=\"your_qdrant_url_or_localhost\"\n# QDRANT_API_KEY=\"your_qdrant_api_key_if_any\"\n# AZURE_COHERE_API_KEY=\"your_cohere_api_key\"\n# AZURE_COHERE_ENDPOINT=\"your_cohere_endpoint\"\n</code></pre>"},{"location":"Guides/RAG/Evaluation/evaluation_cookbook/#1-load-dataset","title":"1. Load Dataset","text":"<p>The first step is to load our dataset. This typically consists of two main parts: 1.  Corpus: The knowledge base from which the RAG system will retrieve information. In this example, it's a JSON file containing TechCrunch articles. 2.  Golden Dataset: A set of questions (queries) with their corresponding ground truth answers and/or evidence documents. This dataset is crucial for evaluation, as explained in the \"Building a Golden Dataset\" section of the RAG Evaluation Overview.</p> <p>The dataset can be downloaded here. Within the downloaded <code>MultiHop</code> directory, you'll find several JSON files:</p> <ul> <li><code>corpus.json</code> contains the original TechCrunch articles that serve as the knowledge base.</li> <li><code>corpus_techcrunch_300.json</code> is a smaller, sampled version of <code>corpus.json</code> (300 articles) used in this cookbook for quicker processing.</li> <li><code>MultiHopRAG.json</code> is the \"golden dataset\" containing questions, ground truth answers, and evidence, formatted and preprocessed for RAG evaluation.</li> <li><code>MultiHopRAG_TechCrunch_300.json</code> is the sampled version of the golden dataset, corresponding to the 300 articles in <code>corpus_techcrunch_300.json</code>.</li> <li><code>MultiHopRAG_TechCrunch_300_results.json</code> is an example file showing the kind of output you can expect after running the retrieval and generation steps described in this cookbook on the sampled dataset.</li> </ul> <p><pre><code>import json\n\nCORPUS_PATH = \"./dataset/MultiHop/corpus_techcrunch_300.json\"\nGOLDEN_DATASET_PATH = \"./dataset/MultiHop/MultiHopRAG_TechCrunch_300.json\"\n\n# load the corpus (knowledge base)\nwith open(CORPUS_PATH, \"r\", encoding=\"utf-8\") as f:\n    corpus_data = json.load(f)\n\n# load the golden dataset (queries)\nwith open(GOLDEN_DATASET_PATH, \"r\", encoding=\"utf-8\") as f:\n    golden_dataset = json.load(f)\n</code></pre> The <code>corpus_data</code> will be ingested into th Qdrant vector store, while <code>golden_dataset</code> will be used to evaluate the RAG pipeline's performance.</p>"},{"location":"Guides/RAG/Evaluation/evaluation_cookbook/#2-ingest-dataset","title":"2. Ingest Dataset","text":"<p>Once the corpus is loaded, it needs to be processed and ingested into a vector database. This involves several steps: document parsing, splitting, embedding, and indexing (i.e. uploading to Qdrant).</p> <pre><code>import logging\nimport os\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nfrom datapizza.clients.google import GoogleClient\nfrom datapizza.clients.openai import OpenAIClient\nfrom datapizza.embedders import NodeEmbedder\nfrom datapizza.pipeline.pipeline import IngestionPipeline\nfrom datapizza.splitters import RecursiveSplitter\nfrom datapizza.vectorstores import QdrantVectorstore\nfrom datapizza.vectorstores.vectorstore import VectorConfig\nfrom dotenv import load_dotenv\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n)\n</code></pre>"},{"location":"Guides/RAG/Evaluation/evaluation_cookbook/#21-custom-llmtreebuilderrouting","title":"2.1. Custom <code>LLMTreeBuilderRouting</code>","text":"<p>This cookbook uses a custom <code>LLMTreeBuilderRouting</code> class. This class is designed to choose different language models for document summarization/structuring (tree building) based on the document's token count. Shorter documents might use a faster/cheaper model, while longer documents might benefit from a more powerful model.</p> <p><pre><code>import tiktoken\nfrom datapizza.clients import Client\nfrom datapizza.treebuilder import LLMTreeBuilder\n\n\nclass LLMTreeBuilderRouting:\n    def __init__(\n        self,\n        client_for_short_documents: Client,\n        client_for_long_documents: Client,\n        token_threshold: int = 10000,\n        tokenizer_model: str = \"gpt-4o-mini\",\n    ):\n        if not isinstance(client_for_short_documents, Client):\n            raise ValueError(\n                \"client_for_short_documents must be an instance of datapizza.clients.client.Client\"\n            )\n        if not isinstance(client_for_long_documents, Client):\n            raise ValueError(\n                \"client_for_long_documents must be an instance of datapizza.clients.client.Client\"\n            )\n\n        self.client_for_short_documents = client_for_short_documents\n        self.client_for_long_documents = client_for_long_documents\n        self.llm_treebuilder_for_short_documents = LLMTreeBuilder(\n            self.client_for_short_documents\n        )\n        self.llm_treebuilder_for_long_documents = LLMTreeBuilder(\n            self.client_for_long_documents\n        )\n        self.token_threshold = token_threshold\n        self.tokenizer_model = tokenizer_model\n        self.encoding = tiktoken.encoding_for_model(self.tokenizer_model)\n\n    def run(self, path_to_file: str):\n        with open(path_to_file, \"r\") as f:\n            text = f.read()\n\n        num_tokens = len(self.encoding.encode(text))\n\n        if num_tokens &gt; self.token_threshold:\n            return self.llm_treebuilder_for_long_documents(path_to_file)\n        else:\n            return self.llm_treebuilder_for_short_documents(path_to_file)\n\n    def __call__(self, path_to_file: str):\n        return self.run(path_to_file)\n</code></pre> This routing mechanism demonstrates a practical approach to optimizing cost and performance in an ingestion pipeline, using a custom module.</p>"},{"location":"Guides/RAG/Evaluation/evaluation_cookbook/#22-configuration-and-component-initialization","title":"2.2. Configuration and Component Initialization","text":"<p>This section loads environment variables and defines key configuration parameters for the ingestion pipeline. The <code>initialize_components</code> function sets up clients (OpenAI, Google), the <code>LLMTreeBuilderRouting</code>, splitter, embedder, and vector store.</p> <pre><code># Load environment variables from .env file\nload_dotenv()\n# --- Configuration ---\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nOPENAI_EMBEDDING_MODEL = \"text-embedding-3-large\"\nOPENAI_LLM_MODEL = \"gpt-4o-mini\"\nEMBEDDING_NAME = \"openai_large\"\nEMBEDDING_DIMENSIONS = 3072\n\nQDRANT_HOST = os.getenv(\"QDRANT_URL\", \"localhost\")\nQDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")  # Optional\nCOLLECTION_NAME = \"test-evaluation-tech-crunch-collection\"\n\nGOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\nGOOGLE_LLM_MODEL = \"gemini-2.5-pro-preview-03-25\"\n\nSPLITTER_MAX_CHAR = 3000\nSPLITTER_OVERLAP = 200\n\nWORKING_FOLDER_PATH = (\n    \"./data/single_corpus_techcrunch_300\"\n)\n\n\ndef initialize_components():\n    \"\"\"Initializes and configures all necessary components.\"\"\"\n    if not OPENAI_API_KEY:\n        raise ValueError(\"OPENAI_API_KEY environment variable not set.\")\n\n    logging.info(\"Initializing OpenAI clients...\")\n    embed_client = OpenAIClient(api_key=OPENAI_API_KEY, model=OPENAI_EMBEDDING_MODEL)\n    client_for_short_documents = OpenAIClient(\n        api_key=OPENAI_API_KEY, model=OPENAI_LLM_MODEL\n    )\n    client_for_long_documents = GoogleClient(\n        api_key=GOOGLE_API_KEY, model=GOOGLE_LLM_MODEL\n    )\n\n    logging.info(\"Initializing TreeBuilder...\")\n    treebuilder = LLMTreeBuilderRouting(\n        client_for_short_documents=client_for_short_documents,\n        client_for_long_documents=client_for_long_documents,\n    )\n\n    logging.info(\"Initializing Splitter...\")\n    splitter = RecursiveSplitter(max_char=SPLITTER_MAX_CHAR, overlap=SPLITTER_OVERLAP)\n\n    logging.info(\"Initializing Embedder...\")\n    embedder = NodeEmbedder(\n        client=embed_client,\n        model_name=OPENAI_EMBEDDING_MODEL,\n        embedding_name=EMBEDDING_NAME,\n    )\n\n    logging.info(\"Initializing Vectorstore...\")\n    vectorstore = QdrantVectorstore(\n        host=QDRANT_HOST,\n        port=None,\n        api_key=QDRANT_API_KEY,\n    )\n\n    os.makedirs(WORKING_FOLDER_PATH, exist_ok=True)\n    return treebuilder, splitter, embedder, vectorstore\n</code></pre>"},{"location":"Guides/RAG/Evaluation/evaluation_cookbook/#23-vector-store-collection-creation","title":"2.3. Vector Store Collection Creation","text":"<p>Before ingesting data, we ensure the target collection exists in Qdrant. The <code>VectorConfig</code> specifies the embedding model's name and dimensions.</p> <pre><code>def create_collection_if_not_exists(\n    vectorstore: QdrantVectorstore, collection_name: str\n):\n    \"\"\"Creates the collection if it doesn't exist.\"\"\"\n    logging.info(f\"Creating collection '{collection_name}'...\")\n    vectorstore.create_collection(\n        collection_name=collection_name,\n        vector_config=[\n            VectorConfig(\n                name=EMBEDDING_NAME,\n                dimensions=EMBEDDING_DIMENSIONS,\n            ),\n        ],\n    )\n    logging.info(f\"Collection '{collection_name}' created successfully.\")\n</code></pre>"},{"location":"Guides/RAG/Evaluation/evaluation_cookbook/#24-document-processing","title":"2.4. Document Processing","text":"<p>The <code>process_single_document</code> function handles the ingestion of individual documents. It saves the document's body to a temporary file* and then runs the ingestion pipeline on it. The <code>process_documents</code> function orchestrates this for all documents in the corpus, using <code>ThreadPoolExecutor</code> for parallel processing to speed up ingestion.</p> <p>*The temporary files are not deleted in this code, remember to clean if you want. <pre><code>def process_single_document(\n    i: int, doc: dict, pipeline: IngestionPipeline, folder_path: str\n):\n    \"\"\"Processes a single document: saves it and runs the pipeline.\"\"\"\n    title = doc.get(\"title\")\n    source = doc.get(\"source\")\n    url = doc.get(\"url\")\n    date = doc.get(\"date\")\n    author = doc.get(\"author\")\n    published_at = doc.get(\"published_at\")\n    category = doc.get(\"category\")\n\n    text = doc.get(\"body\")\n    if not text:\n        logging.warning(\n            f\"Skipping document {i + 1} ('{title}') due to missing 'body' field.\"\n        )\n        return title, False\n\n    file_path = os.path.join(folder_path, f\"document_{i}.txt\")\n    logging.info(f\"Processing document {i + 1}: '{title}' -&gt; {file_path}\")\n    try:\n        with open(file_path, \"w\") as f:\n            f.write(text)\n        pipeline.run(\n            file_path=file_path,\n            metadata={\n                \"document_name\": title,\n                \"file_path\": file_path,\n                \"source\": source,\n                \"url\": url,\n                \"date\": date,\n                \"author\": author,\n                \"published_at\": published_at,\n                \"category\": category,\n            },\n        )\n        return title, True\n    except Exception as e:\n        logging.error(f\"Error processing document {i + 1} ('{title}'): {e}\")\n        return title, False\n\n\ndef process_documents(\n    pipeline: IngestionPipeline, json_path: str, folder_path: str, max_workers: int = 10\n):\n    \"\"\"Loads documents from a JSON file and runs them through the ingestion pipeline in parallel.\"\"\"\n    logging.info(f\"Loading documents from {json_path}...\")\n    try:\n        with open(json_path, \"r\") as f:\n            data = json.load(f)\n    except FileNotFoundError:\n        logging.error(f\"Error: Input JSON file not found at {json_path}\")\n        return\n    except json.JSONDecodeError:\n        logging.error(f\"Error: Could not decode JSON from {json_path}\")\n        return\n\n    num_docs = len(data)\n    logging.info(\n        f\"Found {num_docs} documents. Starting parallel ingestion with {max_workers} workers...\"\n    )\n\n    futures = []\n    processed_count = 0\n    failed_count = 0\n    skipped_count = 0\n\n    os.makedirs(folder_path, exist_ok=True)\n\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        for i, doc in enumerate(data):\n            futures.append(\n                executor.submit(process_single_document, i, doc, pipeline, folder_path)\n            )\n\n        for future in as_completed(futures):\n            try:\n                title, success = future.result()\n                if success is None:\n                    skipped_count += 1\n                elif success:\n                    processed_count += 1\n                    logging.info(\n                        f\"Successfully processed '{title}' ({processed_count}/{num_docs})\"\n                    )\n                else:\n                    original_index = -1\n                    for idx, submitted_future in enumerate(futures):\n                        if submitted_future == future:\n                            original_index = idx\n                            break\n                    if original_index != -1 and not data[original_index].get(\"body\"):\n                        skipped_count += 1\n                    else:\n                        failed_count += 1\n                        logging.warning(f\"Failed or skipped processing for '{title}'\")\n\n            except Exception as exc:\n                failed_count += 1\n                logging.error(f\"An error occurred during processing: {exc}\")\n\n    logging.info(\"Ingestion process completed.\")\n    logging.info(\n        f\"Summary: Processed={processed_count}, Failed={failed_count}, Skipped={skipped_count} out of {num_docs} documents.\"\n    )\n</code></pre></p>"},{"location":"Guides/RAG/Evaluation/evaluation_cookbook/#25-running-the-ingestion-pipeline","title":"2.5. Running the Ingestion Pipeline","text":"<p>This block initializes the components, creates the Qdrant collection if it doesn't exist, configures the <code>IngestionPipeline</code> with the treebuilder, splitter, and embedder modules, and then starts the document processing.</p> <pre><code>treebuilder, splitter, embedder, vectorstore = initialize_components()\n\ncreate_collection_if_not_exists(vectorstore, COLLECTION_NAME)\n\nlogging.info(\"Configuring ingestion pipeline...\")\npipeline = IngestionPipeline(\n    modules=[\n        treebuilder,\n        splitter,\n        embedder,\n    ],\n    vector_store=vectorstore,\n    collection_name=COLLECTION_NAME,\n)\n\nprocess_documents(pipeline, CORPUS_PATH, WORKING_FOLDER_PATH)\n</code></pre>"},{"location":"Guides/RAG/Evaluation/evaluation_cookbook/#3-retrieve-and-generate-answers","title":"3. Retrieve and Generate Answers","text":"<p>After ingesting the corpus, the next phase is to build a RAG pipeline that can retrieve relevant chunks for a given query and then generate an answer based on these chunks.</p> <pre><code>import tqdm\nfrom datapizza.core.models import PipelineComponent\nfrom datapizza.embedders import ClientEmbedder\nfrom datapizza.pipeline.retrieval_pipeline import DagPipeline\nfrom datapizza.prompt import ChatPromptTemplate\nfrom datapizza.rerankers.cohere_reranker import CohereReranker\nfrom datapizza.type import Chunk\n</code></pre>"},{"location":"Guides/RAG/Evaluation/evaluation_cookbook/#31-custom-chunktransformer","title":"3.1. Custom <code>ChunkTransformer</code>","text":"<p>This custom pipeline component, <code>ChunkTransformer</code>, is used to reformat retrieved chunks before they are passed to the reranker and the generator. It constructs a string with the title, URL, and content of each chunk. This can help provide better context to downstream components.</p> <pre><code>class ChunkTransformer(PipelineComponent):\n    def __init__(self):\n        pass\n\n    def __call__(self, chunks: list[Chunk]):\n        return [self.process_chunk(chunk) for chunk in chunks]\n\n    def process_chunk(self, chunk: Chunk):\n        stringified_chunk = f\"\"\"Title: {chunk.metadata.get(\"document_name\")}\nUrl: {chunk.metadata.get(\"url\")}\nContent: {chunk.text}\"\"\"\n        return Chunk(\n            id=chunk.id,\n            text=stringified_chunk,\n            metadata=chunk.metadata,\n        )\n</code></pre>"},{"location":"Guides/RAG/Evaluation/evaluation_cookbook/#32-pipeline-components-and-construction","title":"3.2. Pipeline Components and Construction","text":"<p>The <code>create_components</code> function initializes all modules needed for the retrieval and generation pipeline:</p> <ul> <li><code>ClientEmbedder</code>: For embedding the input query.</li> <li><code>QdrantVectorstore</code>: For retrieving chunks from the indexed corpus.</li> <li><code>ChunkTransformer</code>: The custom component defined above.</li> <li><code>CohereReranker</code>: For reranking the retrieved chunks to improve relevance.</li> <li><code>ChatPromptTemplate</code>: For formatting the input to the generator LLM, including the retrieved context and user query.</li> <li><code>OpenAIClient</code> (as generator): The LLM that generates the final answer.</li> </ul> <p>The <code>build_pipeline</code> function then assembles these components into a <code>DagPipeline</code>, defining the flow of data between modules.</p> <pre><code>AZURE_COHERE_API_KEY = os.getenv(\"AZURE_COHERE_API_KEY\")\nAZURE_COHERE_ENDPOINT = os.getenv(\"AZURE_COHERE_ENDPOINT\")\n\ndef create_components():\n    client_embedder = OpenAIClient(\n        model=OPENAI_EMBEDDING_MODEL,\n        api_key=OPENAI_API_KEY,\n    )\n    embedder = ClientEmbedder(\n        client=client_embedder,\n        model_name=OPENAI_EMBEDDING_MODEL,\n        embedding_name=EMBEDDING_NAME,\n    )\n    retriever = QdrantVectorstore(\n        host=QDRANT_HOST,\n        port=None,\n        api_key=QDRANT_API_KEY,\n    )\n    chunk_transfomer = ChunkTransformer()\n    reranker = CohereReranker(\n        api_key=AZURE_COHERE_API_KEY,\n        endpoint=AZURE_COHERE_ENDPOINT,\n        top_n=10,\n        threshold=0.1,\n    )\n    retrieval_prompt_template = \"\"\"\nHere are the context documents:\n{% for chunk in chunks %}\n{{chunk.text}}\n{% endfor %}\n\"\"\"\n    template = ChatPromptTemplate(\n        user_prompt_template=\"{{user_prompt}}\",\n        system_prompt_template=\"You are a helpful assistant. Try to answer user questions given the context\",\n        retrieval_prompt_template=retrieval_prompt_template,\n    )\n    generator = OpenAIClient(\n        model=\"gpt-4o-mini\",\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n    )\n    return {\n        \"embedder\": embedder,\n        \"retriever\": retriever,\n        \"chunk_transformer\": chunk_transfomer,\n        \"reranker\": reranker,\n        \"prompt_template\": template,\n        \"generator\": generator,\n    }\n\n\ndef build_pipeline():\n    pipeline = DagPipeline()\n    components = create_components()\n\n    pipeline.add_module(\"embedder\", components[\"embedder\"])\n    pipeline.add_module(\"retriever\", components[\"retriever\"])\n    pipeline.add_module(\"reranker\", components[\"reranker\"])\n    pipeline.add_module(\"prompt_template\", components[\"prompt_template\"])\n    pipeline.add_module(\"generator\", components[\"generator\"])\n    pipeline.add_module(\"chunk_transformer\", components[\"chunk_transformer\"])\n\n    pipeline.connect(\"embedder\", \"retriever\", target_key=\"query_vector\")\n    pipeline.connect(\"retriever\", \"chunk_transformer\", target_key=\"chunks\")\n    pipeline.connect(\"chunk_transformer\", \"reranker\", target_key=\"documents\")\n    pipeline.connect(\"reranker\", \"prompt_template\", target_key=\"chunks\")\n    pipeline.connect(\"prompt_template\", \"generator\", target_key=\"memory\")\n\n    return pipeline\n</code></pre>"},{"location":"Guides/RAG/Evaluation/evaluation_cookbook/#33-generating-results","title":"3.3. Generating Results","text":"<p>The <code>generate_results</code> function takes a data item (containing a query and ground truth answer) and the RAG pipeline, then runs the pipeline to get a predicted answer. The <code>process_data_item</code> function is a wrapper that builds a pipeline instance (important for parallel processing where each process needs its own pipeline instance) and calls <code>generate_results</code>. It formats the output including the query, ground truth answer, predicted answer, retrieved chunks, and ground truth chunks.</p> <p>The main script then uses <code>ThreadPoolExecutor</code> to process all items in the <code>golden_dataset</code> in parallel, collecting the results.</p> <p><pre><code>def save_results(path: str, results: list[dict]):\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    with open(path, \"w\") as f:\n        json.dump(results, f)\n\ndef generate_results(data: dict, pipeline: DagPipeline):\n    user_input = data[\"query\"]\n\n    results = pipeline.run(\n        {\n            \"embedder\": {\"text\": user_input},\n            \"retriever\": {\"collection_name\": COLLECTION_NAME, \"k\": 10},\n            \"reranker\": {\"query\": user_input},\n            \"prompt_template\": {\n                \"user_prompt\": user_input,\n                \"retrieval_query\": user_input,\n            },\n            \"generator\": {\n                \"input\": None,\n                \"system_prompt\": \"You are a helpful assistant. Try to answer user questions given the context. Use as few words as possible.\",\n            },\n        }\n    )\n\n    logging.info(user_input)\n    logging.info(\"--------------------------------\")\n    logging.info(f\"GT: {data['answer']}\")\n    logging.info(\"--------------------------------\")\n    logging.info(f\"Pred: {results['generator'].text}\")\n    logging.info(\"--------------------------------\")\n    return results\n\ndef process_data_item(data_item: dict):\n    local_pipeline = build_pipeline()\n\n    try:\n        results = generate_results(data_item, local_pipeline)\n    except Exception as e:\n        logging.error(f\"Error processing item {data_item.get('query', 'UNKNOWN')}: {e}\")\n        return {\n            \"query\": data_item.get(\"query\"),\n            \"answer\": data_item.get(\"answer\"),\n            \"prediction\": f\"Error: {e}\",\n            \"chunks\": [],\n            \"gt_chunks\": [\n                {\"title\": x.get(\"title\"), \"fact\": x.get(\"fact\")}\n                for x in data_item.get(\"evidence_list\", [])\n            ],\n            \"error\": str(e),\n        }\n\n    prediction = results.get(\"generator\")\n    prediction_text = (\n        prediction.text if prediction else \"Error: Generator result missing\"\n    )\n\n    reranker_results = results.get(\"reranker\", [])\n\n    return {\n        \"query\": data_item[\"query\"],\n        \"answer\": data_item[\"answer\"],\n        \"prediction\": prediction_text,\n        \"chunks\": [\n            {\n                \"id\": chunk.id,\n                \"title\": chunk.metadata.get(\"document_name\", \"N/A\"),\n                \"text\": chunk.metadata.get(\"text\", \"N/A\"), # This should be chunk.text after transformer\n            }\n            for chunk in reranker_results # Assuming reranker returns Chunk objects with metadata\n        ],\n        \"gt_chunks\": [\n            {\"title\": x[\"title\"], \"fact\": x[\"fact\"]} for x in data_item[\"evidence_list\"]\n        ],\n    }\n\n# Main execution block for generating results\nresults_list = []\nwith ThreadPoolExecutor(max_workers=16) as executor:\n    futures = [\n        executor.submit(process_data_item, golden_dataset[i])\n        for i in range(len(golden_dataset))\n    ]\n    for future in tqdm.tqdm(\n        as_completed(futures), total=len(futures), desc=\"Processing queries\"\n    ):\n        results_list.append(future.result())\n\nsave_results(\n    path=\"./data/results/MultiHopRAG_TechCrunch_300_results.json\",\n    results=results_list,\n)\n</code></pre> For clarity in the <code>process_data_item</code> returned chunks: The <code>ChunkTransformer</code> sets <code>chunk.text</code> to the stringified version.  If <code>reranker_results</code> are these transformed <code>Chunk</code> objects, then <code>chunk.text</code> will be the transformed string.  The metadata like <code>document_name</code> is preserved.  The original text before transformation is in <code>chunk.metadata.get(\"text\")</code> which is used in the eval.</p>"},{"location":"Guides/RAG/Evaluation/evaluation_cookbook/#4-evaluation-using-datapizza-ai-metrics","title":"4. Evaluation using <code>datapizza-ai</code> metrics","text":"<p>This is the core of the evaluation process, where we use the generated results and the golden dataset to calculate various metrics. These metrics help quantify the performance of both the retrieval and generation components of the RAG pipeline. This section directly implements concepts from the RAG Evaluation Overview.</p> <pre><code>with open(\"./data/results/MultiHopRAG_TechCrunch_300_results.json\", \"r\") as f:\n    results_list = json.load(f)\n\nembed_client = OpenAIClient(api_key=OPENAI_API_KEY, model=OPENAI_EMBEDDING_MODEL)\nvectorstore = QdrantVectorstore(\n    host=QDRANT_HOST,\n    port=None,\n    api_key=QDRANT_API_KEY,\n)\n\nfrom typing import Optional\nimport numpy as np\nimport pandas as pd\nfrom datapizza.clients.google_client import GoogleClient\nfrom datapizza.evaluation import metrics\nfrom pydantic import BaseModel\n</code></pre>"},{"location":"Guides/RAG/Evaluation/evaluation_cookbook/#41-defining-relevance-preparing-embeddings-for-similarity-metrics","title":"4.1. Defining \"Relevance\": Preparing Embeddings for Similarity Metrics","text":"<p>As discussed in the RAG Evaluation Overview under \"Defining Relevance: Exact Match vs. Cosine Similarity\", similarity-based metrics require embeddings for both retrieved and ground truth chunks.</p> <p>The <code>get_embeddings_retrieved_chunks</code> function fetches embeddings for the text of retrieved chunks. It does this by dumping all chunks and their vectors from the Qdrant collection and creating a mapping from text to embedding. This is an expensive operation and can be optimized if chunk IDs are consistently used and retrievable. The <code>get_embeddings_ground_truth_chunks</code> function generates embeddings for the ground truth texts on the fly using the <code>embed_client</code>.</p> <pre><code>def get_embeddings_retrieved_chunks(\n    retrieved_chunk_texts: list[str],\n    vector_store: QdrantVectorstore,  # Expects an instance of QdrantVectorStore (or compatible)\n    collection_name: str,\n) -&gt; list[Optional[list[float]]]:\n    \"\"\"\n    Retrieves embeddings for a list of text chunks from a Qdrant collection.\n\n    This function dumps all chunks with their vectors from the specified collection,\n    maps their texts to their embeddings, and then looks up the embeddings for\n    the provided list of chunk texts.\n\n    Args:\n        retrieved_chunk_texts: A list of strings, where each string is the text\n                               of a chunk whose embedding is to be retrieved.\n        vector_store: An instance of a vector store client that has a\n                      `dump_collection` method (e.g., QdrantVectorStore).\n                      The `dump_collection` method should yield chunk-like objects\n                      that have `payload` (a dictionary) and `vector` attributes.\n        collection_name: The name of the collection in the vector store.\n        payload_text_key: The key in the chunk's payload dictionary that\n                          stores the text content. Defaults to \"text\".\n\n    Returns:\n        A list of embeddings corresponding to the retrieved_chunk_texts.\n        Each embedding is expected to be a list of floats.\n        If a text from retrieved_chunk_texts is not found in the collection\n        or if its corresponding chunk has no vector, None is placed at that\n        position in the returned list.\n    \"\"\"\n\n    all_dumped_chunks = vector_store.dump_collection(\n        collection_name=collection_name,\n        with_vectors=True,  # Ensure vectors (embeddings) are fetched\n    )\n\n    text_to_embedding_map: dict[str, list[float]] = {}\n    for chunk in all_dumped_chunks:\n        if (\n            hasattr(chunk, \"text\")\n            and hasattr(chunk, \"embeddings\")\n            and chunk.embeddings is not None\n        ):\n            if chunk.text is not None:\n                text_to_embedding_map[chunk.text] = chunk.embeddings[0].vector\n\n    retrieved_embeddings: list[Optional[list[float]]] = []\n    for text_to_find in retrieved_chunk_texts:\n        embedding = text_to_embedding_map.get(text_to_find)\n        retrieved_embeddings.append(embedding)\n\n    return retrieved_embeddings\n\ndef get_embeddings_ground_truth_chunks(ground_truth_texts, client_embedder):\n    embeddings = client_embedder.embed(ground_truth_texts)\n    return embeddings\n</code></pre>"},{"location":"Guides/RAG/Evaluation/evaluation_cookbook/#42-retrieval-evaluation-metrics","title":"4.2. Retrieval Evaluation Metrics","text":"<p>These metrics assess how well the retriever found relevant information.</p>"},{"location":"Guides/RAG/Evaluation/evaluation_cookbook/#421-exact-match-metrics","title":"4.2.1. Exact Match Metrics","text":"<p>These metrics consider a retrieved chunk relevant if its text exactly matches a ground truth chunk text. Refer to Precision@k, Recall@k, F1-score@k, Hybrid Log-Rank Score (Exact Match) in the overview.</p> <pre><code>def calculate_exact_match_metrics(retrieved_chunk_texts, ground_truth_texts, k_values):\n    metrics_dict = {}\n    for k_val in k_values:\n        metrics_dict[f\"precision_at_{k_val}_exact\"] = metrics.precision_at_k_exact(\n            retrieved_chunks=retrieved_chunk_texts,\n            ground_truth_chunks=ground_truth_texts,\n            k=k_val,\n        )\n        metrics_dict[f\"recall_at_{k_val}_exact\"] = metrics.recall_at_k_exact(\n            retrieved_chunks=retrieved_chunk_texts,\n            ground_truth_chunks=ground_truth_texts,\n            k=k_val,\n        )\n        metrics_dict[f\"f1_at_{k_val}_exact\"] = metrics.f1_at_k_exact(\n            retrieved_chunks=retrieved_chunk_texts,\n            ground_truth_chunks=ground_truth_texts,\n            k=k_val,\n        )\n    metrics_dict[\"hybrid_log_rank_score_exact\"] = metrics.hybrid_log_rank_score_exact(\n        retrieved_chunks=retrieved_chunk_texts,\n        ground_truth_chunks=ground_truth_texts,\n    )\n    return metrics_dict\n</code></pre>"},{"location":"Guides/RAG/Evaluation/evaluation_cookbook/#422-lexical-overlap-metrics-bleu-rouge","title":"4.2.2. Lexical Overlap Metrics (BLEU &amp; ROUGE)","text":"<p>While BLEU and ROUGE are traditionally used for evaluating generated text (like translations or summaries), this script applies <code>corpus_bleu_score</code> and <code>corpus_rouge_scores</code> to compare the set of retrieved chunk texts against the set of ground truth chunk texts. This can provide a measure of lexical overlap at a corpus level. Refer to BLEU Score and ROUGE Score in the overview for their typical usage.</p> <pre><code>def calculate_bleu_rouge_metrics(retrieved_chunk_texts, ground_truth_texts):\n    metrics_dict = {}\n    bleu_value = metrics.corpus_bleu_score(\n        retrieved_chunks=retrieved_chunk_texts, ground_truth_chunks=ground_truth_texts\n    )\n    metrics_dict[\"corpus_bleu\"] = bleu_value\n\n    rouge_values = metrics.corpus_rouge_scores(\n        retrieved_chunks=retrieved_chunk_texts, ground_truth_chunks=ground_truth_texts\n    )\n    for rouge_type, scores_dict in rouge_values.items():\n        metrics_dict[f\"corpus_{rouge_type}_precision\"] = scores_dict[\"precision\"]\n        metrics_dict[f\"corpus_{rouge_type}_recall\"] = scores_dict[\"recall\"]\n        metrics_dict[f\"corpus_{rouge_type}_fmeasure\"] = scores_dict[\"fmeasure\"]\n    return metrics_dict\n</code></pre>"},{"location":"Guides/RAG/Evaluation/evaluation_cookbook/#423-similarity-based-metrics","title":"4.2.3. Similarity-Based Metrics","text":"<p>These metrics use vector embeddings and cosine similarity to determine relevance, as explained in the RAG Evaluation Overview. A retrieved chunk is relevant if its embedding is semantically similar to a ground truth chunk's embedding above a certain <code>similarity_threshold</code>. Refer to Precision@k, Recall@k, F1-score@k, Hybrid Log-Rank Score (Similarity-based) in the overview.</p> <pre><code>def calculate_similarity_based_metrics(\n    retrieved_chunk_embeddings: list[np.ndarray],\n    ground_truth_embeddings: list[np.ndarray],\n    k_values: list[int],\n    similarity_threshold: float = 0.8,\n    hybrid_log_rank_gamma: float = 1.0,\n    hybrid_log_rank_alpha: float = 0.5,\n):\n    metrics_dict = {}\n    if not retrieved_chunk_embeddings or not ground_truth_embeddings:\n        for k_val in k_values:\n            metrics_dict[f\"precision_at_{k_val}_similarity\"] = None\n            metrics_dict[f\"recall_at_{k_val}_similarity\"] = None\n            metrics_dict[f\"f1_at_{k_val}_similarity\"] = None\n        metrics_dict[\"hybrid_log_rank_score_similarity\"] = None\n        return metrics_dict\n\n    for k_val in k_values:\n        try:\n            metrics_dict[f\"precision_at_{k_val}_similarity\"] = (\n                metrics.precision_at_k_similarity(\n                    retrieved_embeddings=retrieved_chunk_embeddings,\n                    ground_truth_embeddings=ground_truth_embeddings,\n                    k=k_val,\n                    similarity_threshold=similarity_threshold,\n                )\n            )\n        except Exception as e:\n            logging.error(f\"Error calculating precision_at_{k_val}_similarity: {e}\")\n            metrics_dict[f\"precision_at_{k_val}_similarity\"] = None\n        # Similar try-except blocks for recall and F1\n        try:\n            metrics_dict[f\"recall_at_{k_val}_similarity\"] = (\n                metrics.recall_at_k_similarity(\n                    retrieved_embeddings=retrieved_chunk_embeddings,\n                    ground_truth_embeddings=ground_truth_embeddings,\n                    k=k_val,\n                    similarity_threshold=similarity_threshold,\n                )\n            )\n        except Exception as e:\n            logging.error(f\"Error calculating recall_at_{k_val}_similarity: {e}\")\n            metrics_dict[f\"recall_at_{k_val}_similarity\"] = None\n\n        try:\n            metrics_dict[f\"f1_at_{k_val}_similarity\"] = metrics.f1_at_k_similarity(\n                retrieved_embeddings=retrieved_chunk_embeddings,\n                ground_truth_embeddings=ground_truth_embeddings,\n                k=k_val,\n                similarity_threshold=similarity_threshold,\n            )\n        except Exception as e:\n            logging.error(f\"Error calculating f1_at_{k_val}_similarity: {e}\")\n            metrics_dict[f\"f1_at_{k_val}_similarity\"] = None\n\n    try:\n        metrics_dict[\"hybrid_log_rank_score_similarity\"] = (\n            metrics.hybrid_log_rank_score_similarity(\n                retrieved_embeddings=retrieved_chunk_embeddings,\n                ground_truth_embeddings=ground_truth_embeddings,\n                similarity_threshold=similarity_threshold,\n                gamma=hybrid_log_rank_gamma,\n                alpha=hybrid_log_rank_alpha,\n            )\n        )\n    except Exception as e:\n        logging.error(f\"Error calculating hybrid_log_rank_score_similarity: {e}\")\n        metrics_dict[\"hybrid_log_rank_score_similarity\"] = None\n    return metrics_dict\n</code></pre>"},{"location":"Guides/RAG/Evaluation/evaluation_cookbook/#43-generation-evaluation-metrics","title":"4.3. Generation Evaluation Metrics","text":"<p>These metrics assess the quality of the answer generated by the LLM.</p>"},{"location":"Guides/RAG/Evaluation/evaluation_cookbook/#431-llm-as-judge","title":"4.3.1. LLM-as-Judge","text":"<p>This approach uses another powerful LLM (the \"judge\") to evaluate the generated answer's quality against the ground truth answer and the original query. The script defines a <code>MatchingResult</code> Pydantic model for the judge's structured output. Refer to LLM-as-judge in the overview.</p> <pre><code>def calculate_answer_generation_metrics(\n    query, predicted_answer, ground_truth_answer, client_judge\n):\n    class MatchingResult(BaseModel):\n        is_matching: bool\n        # reasoning: str | None = None # Optional\n\n    judge_system_prompt = \"\"\"You are an AI assistant acting as an impartial judge.\n    Your task is to determine if the 'PREDICTION' accurately and satisfactorily answers the 'ORIGINAL QUERY', considering the provided 'ANSWER' as a reference.\n    Respond with a JSON object containing two keys:\n    1.  `is_matching`: a boolean value (true if the prediction matches, false otherwise).\n    2.  `reasoning`: a brief explanation for your decision, especially if it's not a match.\n\n    Focus on semantic similarity and factual correctness. Minor phrasing differences are acceptable if the meaning is preserved.\n    If the prediction is too vague, incomplete, or factually incorrect compared to the answer and query, it is not matching.\n    \"\"\"\n\n    judge_input_prompt = f\"\"\"ORIGINAL QUERY:\n{query}\n\nGROUND TRUTH ANSWER:\n{ground_truth_answer}\n\nPREDICTION:\n{predicted_answer}\"\"\"\n\n    try:\n        client_response = client_judge.structured_response(\n            input=judge_input_prompt,\n            output_cls=MatchingResult,\n            system_prompt=judge_system_prompt,\n        )\n        matching_data = client_response.structured_data[0]\n        return int(matching_data.is_matching)\n    except Exception as e:\n        logging.error(f\"Error calculating answer generation metrics: {e}\")\n        return 0 # Default to not matching on error\n</code></pre>"},{"location":"Guides/RAG/Evaluation/evaluation_cookbook/#44-running-the-evaluation-loop","title":"4.4. Running the Evaluation Loop","text":"<p>This is the main loop where metrics are calculated for each item in <code>results_list</code>. It first prepares global sets of retrieved and ground truth chunk texts and their embeddings to avoid redundant computations. Then, it iterates through each evaluated query, extracts the necessary data (retrieved chunks, ground truth chunks, predicted answer, etc.), and calls the respective metric calculation functions.</p> <pre><code># Main evaluation process\nall_item_metrics = []\nK_VALUES = [1, 3, 5, 10]\n\n# Prepare embeddings for all unique chunks once\nall_retrieved_chunk_texts_set = set()\nfor item in results_list:\n    for chunk_info in item.get(\"chunks\", []):\n        # The script stores retrieved chunk texts under 'text' key inside 'chunks' list of dicts.\n        # Example: item['chunks'] = [{'id': ..., 'title': ..., 'text': 'actual chunk text from reranker'}]\n        if chunk_info.get(\"text\"): # Ensure text is not None or empty\n             all_retrieved_chunk_texts_set.add(chunk_info.get(\"text\"))\n\nall_ground_truth_texts_set = set()\nfor item in results_list:\n    for gt_chunk in item.get(\"gt_chunks\", []):\n        # Ground truth facts are stored under 'fact' key\n        if gt_chunk.get(\"fact\"): # Ensure fact is not None or empty\n            all_ground_truth_texts_set.add(gt_chunk.get(\"fact\"))\n\n# Convert sets to lists for functions expecting lists\nlist_all_retrieved_chunk_texts = list(all_retrieved_chunk_texts_set)\nlist_all_ground_truth_texts = list(all_ground_truth_texts_set)\n\n# Fetch all retrieved chunks embeddings\n# Ensure that the 'text' field used to build list_all_retrieved_chunk_texts is the same text\n# that get_embeddings_retrieved_chunks expects to find in the vector store dump.\nall_retrieved_chunk_embeddings_list = get_embeddings_retrieved_chunks(\n    list_all_retrieved_chunk_texts, vectorstore, COLLECTION_NAME\n)\n# Compute all ground truth chunks embeddings\nall_ground_truth_embeddings_list = get_embeddings_ground_truth_chunks(\n    list_all_ground_truth_texts, embed_client\n)\n\n# Reorganize into dictionaries for quick lookup: text -&gt; embedding\nretrieved_chunks_emb_dict = {\n    text: emb\n    for text, emb in zip(list_all_retrieved_chunk_texts, all_retrieved_chunk_embeddings_list)\n    if emb is not None # Only include if embedding was found\n}\nground_truth_emb_dict = {\n    text: emb\n    for text, emb in zip(list_all_ground_truth_texts, all_ground_truth_embeddings_list)\n    if emb is not None\n}\n\nclient_judge = GoogleClient(\n    api_key=GOOGLE_API_KEY,\n    model=\"gemini-2.5-flash-preview-04-17\",\n)\n\nlogging.info(\"Starting evaluation of each item...\")\n\nqueries_for_gen_eval = []\nground_truth_answers_for_gen_eval = []\npredicted_answers_for_gen_eval = []\n\nfor i, item in enumerate(results_list):\n    if i % 20 == 0 and i &gt; 0:\n        logging.info(f\"  Processed {i}/{len(results_list)} items for retrieval metrics...\")\n\n    # For retrieval metrics:\n    # Extracted from item['chunks'] which are the reranked chunks.\n    retrieved_chunk_texts_for_item = [\n        chunk.get(\"text\", \"\") for chunk in item.get(\"chunks\", []) if chunk.get(\"text\")\n    ]\n    retrieved_chunk_embeddings_for_item = [\n        retrieved_chunks_emb_dict.get(text) for text in retrieved_chunk_texts_for_item if retrieved_chunks_emb_dict.get(text) is not None\n    ]\n\n    # Ground truth texts (facts) for retrieval\n    ground_truth_texts_for_item = []\n    if item.get(\"gt_chunks\") and isinstance(item[\"gt_chunks\"], list):\n        ground_truth_texts_for_item = [\n            gt_chunk.get(\"fact\", \"\")\n            for gt_chunk in item[\"gt_chunks\"]\n            if isinstance(gt_chunk, dict) and gt_chunk.get(\"fact\")\n        ]\n    ground_truth_embeddings_for_item = [\n        ground_truth_emb_dict.get(text) for text in ground_truth_texts_for_item if ground_truth_emb_dict.get(text) is not None\n    ]\n\n    current_metrics = {\"query_id\": i, \"query\": item.get(\"query\", \"N/A\")}\n\n    exact_metrics = calculate_exact_match_metrics(\n        retrieved_chunk_texts_for_item, ground_truth_texts_for_item, K_VALUES\n    )\n    current_metrics.update(exact_metrics)\n\n    bleu_rouge_metrics = calculate_bleu_rouge_metrics(\n        retrieved_chunk_texts, ground_truth_texts\n    )\n    current_metrics.update(bleu_rouge_metrics)\n\n    similarity_metrics = calculate_similarity_based_metrics(\n        retrieved_chunk_embeddings_for_item, ground_truth_embeddings_for_item, K_VALUES,\n        similarity_threshold=0.6\n    )\n    current_metrics.update(similarity_metrics)\n\n    # Store data for batch generation metric calculation\n    queries_for_gen_eval.append(item.get(\"query\", \"\"))\n    ground_truth_answers_for_gen_eval.append(item.get(\"answer\", \"\"))\n    predicted_answers_for_gen_eval.append(item.get(\"prediction\", \"\"))\n\n    all_item_metrics.append(current_metrics)\n\n# Calculate answer generation metrics in parallel\ndef process_answer_metrics_for_item(query_pred_gt_tuple):\n    q, p, g = query_pred_gt_tuple\n    return calculate_answer_generation_metrics(q, p, g, client_judge)\n\nlogging.info(\"Calculating answer generation metrics...\")\nwith ThreadPoolExecutor(max_workers=20) as executor: # As per script\n    answer_generation_metric_results = list(\n        tqdm.tqdm(\n            executor.map(\n                process_answer_metrics_for_item,\n                zip(queries_for_gen_eval, predicted_answers_for_gen_eval, ground_truth_answers_for_gen_eval),\n            ),\n            total=len(queries_for_gen_eval),\n            desc=\"Evaluating generated answers\"\n        )\n    )\n\n# Add generation metrics to each item's metrics\nfor i, item_metrics_dict in enumerate(all_item_metrics):\n    item_metrics_dict[\"llm_as_judge_answer_metrics\"] = answer_generation_metric_results[i]\n\nlogging.info(\"\nEvaluation processing complete.\")\n# `all_item_metrics` now contains all calculated metrics for each query.\n</code></pre>"},{"location":"Guides/RAG/Evaluation/evaluation_cookbook/#45-visualizing-results","title":"4.5. Visualizing Results","text":"<p>The <code>visualize_results</code> function takes the list of all item metrics and K values to generate and log summary statistics and plots. It uses <code>pandas</code> for data manipulation and <code>matplotlib</code>/<code>seaborn</code> for plotting.</p> <p>Key visualizations include:</p> <ul> <li>A table of summary statistics (mean, median, min, max, std, count, missing_count) for all numeric metrics.</li> <li>A bar chart of average scores for evaluation metrics.</li> <li>Box plots showing the distribution of key metrics (e.g., precision@k, recall@k for different k).</li> </ul> <p>This function is extensive and primarily focuses on presentation. Below is its definition. To see the plots, you would typically run the <code>plt.show()</code> commands (commented out in the original script's logging messages) in a Jupyter environment.</p> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n# Ensure logging and os are imported if not already:\n# import logging\n# import os\n# import numpy as np # for numeric_cols selection\n# import pandas as pd # for DataFrame\n\ndef visualize_results(all_item_metrics: list[dict], k_values: list[int]):\n    if not all_item_metrics:\n        logging.info(\"\ud83d\udeab No evaluation data was provided. Skipping visualization.\")\n        return\n\n    try:\n        df_results = pd.DataFrame(all_item_metrics)\n    except Exception as e:\n        logging.error(f\"\u274c Error creating DataFrame from all_item_metrics: {e}\")\n        return\n\n    pd.set_option(\"display.max_columns\", None)\n    pd.set_option(\"display.width\", 150)\n    pd.set_option(\"display.float_format\", \"{:.4f}\".format)\n\n    logging.info(\"\n\ud83d\udcca======= Overall Evaluation Summary =======\ud83d\udcca\")\n    numeric_cols = df_results.select_dtypes(include=np.number).columns\n\n    if not numeric_cols.empty:\n        summary_stats = (\n            df_results[numeric_cols]\n            .agg([\"mean\", \"median\", \"min\", \"max\", \"std\", \"count\"])\n            .transpose()\n        )\n        summary_stats[\"missing_count\"] = len(df_results) - summary_stats[\"count\"]\n        logging.info(\"\n\ud83d\udccb Summary Statistics for Numeric Metrics:\")\n        try:\n            logging.info(f\"\n{summary_stats.to_string()}\")\n        except Exception as e:\n            logging.error(f\"Error logging.infoing summary_stats: {e}\")\n\n        plt.style.use(\"seaborn-v0_8-whitegrid\")\n        mean_metrics = df_results[numeric_cols].mean(skipna=True).dropna()\n        mean_metrics_to_plot = mean_metrics.drop(\"query_id\", errors=\"ignore\")\n\n        if not mean_metrics_to_plot.empty:\n            plt.figure(figsize=(15, 8))\n            colors = sns.color_palette(\"viridis\", len(mean_metrics_to_plot))\n            bars = mean_metrics_to_plot.sort_values(ascending=False).plot(\n                kind=\"bar\", color=colors\n            )\n            plt.title(\"Average Scores for Evaluation Metrics\", fontsize=18, fontweight=\"bold\")\n            plt.ylabel(\"Average Score\", fontsize=14)\n            plt.xlabel(\"Metric\", fontsize=14)\n            plt.xticks(rotation=45, ha=\"right\", fontsize=10)\n            plt.yticks(fontsize=10)\n            plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n            for bar in bars.patches:\n                bars.text(\n                    bar.get_x() + bar.get_width() / 2,\n                    bar.get_height() + 0.01,\n                    f\"{bar.get_height():.3f}\",\n                    ha=\"center\", va=\"bottom\", fontsize=9,\n                )\n            plt.tight_layout()\n            logging.info(\"\n\ud83d\udcc8 [Plot Code: Average Metrics Bar Chart - Copy and run in a new cell to display `plt.show()`]\")\n        else:\n            logging.info(\"\n\u26a0\ufe0f No numeric metrics with non-NaN mean values to plot.\")\n\n        key_metrics_for_boxplot = []\n        for k in k_values:\n            key_metrics_for_boxplot.extend([f\"precision_at_{k}_exact\", f\"recall_at_{k}_exact\", f\"f1_at_{k}_exact\"])\n            key_metrics_for_boxplot.extend([f\"precision_at_{k}_similarity\", f\"recall_at_{k}_similarity\", f\"f1_at_{k}_similarity\"]) # Added similarity\n\n        if \"hybrid_log_rank_score_exact\" in numeric_cols: key_metrics_for_boxplot.append(\"hybrid_log_rank_score_exact\")\n        if \"hybrid_log_rank_score_similarity\" in numeric_cols: key_metrics_for_boxplot.append(\"hybrid_log_rank_score_similarity\") # Added similarity\n        if \"llm_as_judge_answer_metrics\" in numeric_cols: key_metrics_for_boxplot.append(\"llm_as_judge_answer_metrics\") # Added judge metric\n\n        plot_metrics = [m for m in key_metrics_for_boxplot if m in df_results.columns and pd.api.types.is_numeric_dtype(df_results[m])]\n        plot_metrics_valid = [metric for metric in plot_metrics if not df_results[metric].isnull().all()]\n\n        if plot_metrics_valid:\n            plt.figure(figsize=(16, max(10, len(plot_metrics_valid) * 0.6)))\n            ax_box = plt.gca()\n            sns.boxplot(data=df_results[plot_metrics_valid], orient=\"h\", palette=\"pastel\", ax=ax_box)\n            ax_box.set_title(\"Distribution of Key Evaluation Metrics\", fontsize=18, fontweight=\"bold\")\n            ax_box.set_xlabel(\"Score\", fontsize=14); ax_box.set_ylabel(\"Metric\", fontsize=14)\n            ax_box.tick_params(axis='both', labelsize=10); ax_box.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n            plt.tight_layout()\n            logging.info(\"\n\ud83d\udcc8 [Plot Code: Key Metric Distributions (Box Plots) - Copy and run in a new cell to display `plt.show()`]\")\n        else:\n            logging.info(\"\n\u26a0\ufe0f No key metrics available or valid for box plot distribution.\")\n    else:\n        logging.info(\"\n\u26a0\ufe0f No numeric columns found to generate statistics or plots.\")\n\n    logging.info(\"\n\ud83d\udcc4======= Detailed Results (First 5 Queries) =======\ud83d\udcc4\")\n    try:\n        logging.info(f\"\n{df_results.head(5).to_string()}\")\n    except Exception as e:\n        logging.error(f\"Error logging.infoing detailed results head: {e}\")\n\n    output_csv_path = \"./data/results/evaluation_metrics_detailed.csv\" # From script\n    # Example of saving (ensure directory exists):\n    # output_dir = os.path.dirname(output_csv_path)\n    # if output_dir and not os.path.exists(output_dir): os.makedirs(output_dir, exist_ok=True)\n    # df_results.to_csv(output_csv_path, index=False)\n    # logging.info(f\"\n\ud83d\udcbe Detailed results optionally saved to: {output_csv_path}\")\n\n    logging.info(\"\n\ud83d\udca1======= Important Notes &amp; Next Steps =======\ud83d\udca1\")\n    # ... (logging messages from script) ...\n</code></pre> <p>And finally, call the visualization: <pre><code># Visualize the results\nvisualize_results(all_item_metrics, K_VALUES)\n</code></pre></p>"},{"location":"Guides/RAG/Evaluation/evaluation_cookbook/#5-conclusion","title":"5. Conclusion","text":"<p>This cookbook has walked through a comprehensive RAG evaluation pipeline, from data ingestion to metric calculation and visualization. By implementing the functions and processes described, you can:</p> <ol> <li>Prepare your data: Load and process a corpus and a golden dataset.</li> <li>Build RAG pipelines: Construct ingestion and retrieval/generation pipelines using components from the <code>datapizza-ai</code> library.</li> <li>Evaluate Retrieval Performance: Calculate exact match, lexical overlap (BLEU/ROUGE on chunks), and similarity-based metrics (Precision@k, Recall@k, F1@k, Hybrid Log-Rank Score) to assess the quality of retrieved context. These directly relate to the metrics discussed in the Retrieval Evaluation Metrics section of the overview.</li> <li>Evaluate Generation Quality: Use LLM-as-judge to assess the relevance and accuracy of generated answers, as detailed in the Generation Evaluation Metrics section.</li> <li>Analyze Results: Use visualizations and summary statistics to understand your RAG system's strengths and weaknesses.</li> </ol> <p>Remember that building a robust \"Golden Dataset\" as described in the evaluation overview is paramount for meaningful evaluation. The choice of metrics should align with your specific application's requirements. This cookbook provides a practical template that you can adapt and extend for your own RAG evaluation tasks.</p>"},{"location":"Guides/RAG/Evaluation/evaluation_overview/","title":"Metrics","text":"<p>This document outlines the evaluation metrics available for assessing the performance of Retrieval Augmented Generation (RAG) systems, particularly focusing on the retrieval and generation components. The implementations can be found in <code>datapizza/evaluation/metrics.py</code>.</p>"},{"location":"Guides/RAG/Evaluation/evaluation_overview/#retrieval-evaluation-metrics","title":"Retrieval Evaluation Metrics","text":"<p>These metrics assess the quality of the retrieved context. They typically compare a list of retrieved chunks (or their embeddings) against a \"golden\" set of ground truth relevant chunks.</p>"},{"location":"Guides/RAG/Evaluation/evaluation_overview/#defining-relevance-exact-match-vs-cosine-similarity","title":"Defining Relevance: Exact Match vs. Cosine Similarity","text":"<p>At the core of retrieval evaluation is determining whether a retrieved chunk is \"relevant\" when compared to a ground truth chunk. Two primary methods are employed:</p> <ul> <li> <p>Exact Match:</p> <ul> <li>Concept: This method considers a retrieved chunk relevant if it is an exact string match to one of the ground truth chunks.</li> <li>When to use: Exact match is suitable when the retrieved information must be precisely identical to the source. This is often the case for retrieving identifiers, codes, specific terminologies, or when the phrasing itself is critical and variations are not acceptable. It's a stringent measure that ensures verbatim retrieval.</li> </ul> </li> <li> <p>Cosine Similarity (Similarity-based):</p> <ul> <li>Concept: This method determines relevance based on the semantic similarity between chunks. First, text chunks are converted into numerical vector representations (embeddings). Then, the cosine similarity between the embedding of a retrieved chunk and the embeddings of ground truth chunks is calculated. If this similarity score exceeds a predefined <code>similarity_threshold</code> (typically between 0 and 1), the retrieved chunk is considered relevant. A higher threshold demands greater similarity.</li> <li>When to use: Cosine similarity is preferred when the meaning or semantic content is more important than the exact wording. This is useful for tasks where paraphrasing is common, synonyms are expected, or the goal is to retrieve conceptually similar information even if the surface form differs. It allows for more flexibility in matching relevant content that might be expressed in various ways.</li> </ul> </li> </ul> <p>The choice between exact match and cosine similarity depends on the specific requirements of your RAG application and the nature of the data you are working with.</p>"},{"location":"Guides/RAG/Evaluation/evaluation_overview/#precisionk","title":"Precision@k","text":"<p>Precision at k (P@k) measures the proportion of the top <code>k</code> retrieved items that are relevant. A higher P@k indicates that the items ranked highly by the retriever are indeed useful.</p> <p>It is calculated as: [ P@k = \\frac{\\text{Number of relevant items in top k}}{\\text{k}} ]</p> <p>Relevance can be determined using either exact match or cosine similarity. The corresponding functions are:</p> <pre><code>def precision_at_k_exact(\n    retrieved_chunks: list[str],\n    ground_truth_chunks: list[str],\n    k: int\n) -&gt; float:\n\ndef precision_at_k_similarity(\n    retrieved_embeddings: list[np.ndarray],\n    ground_truth_embeddings: list[np.ndarray],\n    k: int,\n    similarity_threshold: float = 0.8,\n) -&gt; float:\n</code></pre>"},{"location":"Guides/RAG/Evaluation/evaluation_overview/#recallk","title":"Recall@k","text":"<p>Recall at k (R@k) measures the proportion of all truly relevant items that are found within the top <code>k</code> retrieved items. A higher R@k indicates that the retriever is successful in finding most of the relevant information.</p> <p>It is calculated as: [ R@k = \\frac{\\text{Number of relevant items in top k}}{\\text{Total number of relevant items}} ]</p> <p>Relevance can be determined using either exact match or cosine similarity. The corresponding functions are:</p> <pre><code>def recall_at_k_exact(\n    retrieved_chunks: list[str],\n    ground_truth_chunks: list[str],\n    k: int\n) -&gt; float:\n\ndef recall_at_k_similarity(\n    retrieved_embeddings: list[np.ndarray],\n    ground_truth_embeddings: list[np.ndarray],\n    k: int,\n    similarity_threshold: float = 0.8,\n) -&gt; float:\n</code></pre>"},{"location":"Guides/RAG/Evaluation/evaluation_overview/#f1-scorek","title":"F1-score@k","text":"<p>The F1-score at k (F1@k) is the harmonic mean of Precision@k and Recall@k. It provides a single score that balances both precision and recall, useful when you want a combined measure of retrieval performance.</p> <p>It is calculated as: [ F1@k = 2 \\times \\frac{P@k \\times R@k}{P@k + R@k} ]</p> <p>Relevance can be determined using either exact match or cosine similarity. The corresponding functions are:</p> <pre><code>def f1_at_k_exact(\n    retrieved_chunks: list[str],\n    ground_truth_chunks: list[str],\n    k: int\n) -&gt; float:\n\ndef f1_at_k_similarity(\n    retrieved_embeddings: list[np.ndarray],\n    ground_truth_embeddings: list[np.ndarray],\n    k: int,\n    similarity_threshold: float = 0.8,\n) -&gt; float:\n</code></pre>"},{"location":"Guides/RAG/Evaluation/evaluation_overview/#hybrid-log-rank-score","title":"Hybrid Log-Rank Score","text":"<p>This metric provides a more nuanced evaluation by combining recall with a rank-aware scoring component. It rewards systems that not only retrieve relevant items but also place them at higher ranks. The rank-based component uses a logarithmic scoring function (<code>log_rank_score</code>) which gives higher scores to items ranked closer to the top.</p> <p>The score is a weighted average of recall and rank quality: [ \\text{Hybrid Score} = \\alpha \\times \\text{Recall} + (1 - \\alpha) \\times \\text{Rank Quality} ] where <code>alpha</code> controls the trade-off, and <code>gamma</code> in the <code>log_rank_score</code> function controls the steepness of the logarithmic curve for rank scoring.</p> <p>Relevance for the recall component can be determined using either exact match or cosine similarity. The corresponding functions are:</p> <pre><code>def hybrid_log_rank_score_exact(\n    retrieved_chunks: list[str],\n    ground_truth_chunks: list[str],\n    gamma: float = 1.0,\n    alpha: float = 0.5,\n) -&gt; float:\n\ndef hybrid_log_rank_score_similarity(\n    retrieved_embeddings: list[np.ndarray],\n    ground_truth_embeddings: list[np.ndarray],\n    similarity_threshold: float = 0.8,\n    gamma: float = 1.0,\n    alpha: float = 0.5,\n) -&gt; float:\n</code></pre>"},{"location":"Guides/RAG/Evaluation/evaluation_overview/#bleu-score-bilingual-evaluation-understudy","title":"BLEU Score (Bilingual Evaluation Understudy)","text":"<p>BLEU measures the similarity between a machine-generated text (hypothesis) and one or more high-quality reference texts. It counts matching n-grams (contiguous sequences of n words) between the hypothesis and references, penalizing for brevity. While originally for machine translation, it's adapted for other text generation tasks. Scores are typically between 0 and 1 (or 0-100, here scaled to 0-1).</p> <p><pre><code>def bleu_score(\n    retrieved_chunk: str,  # Intended to be the generated text\n    ground_truth_chunks: list[str] # List of reference texts\n) -&gt; float:\n</code></pre> Note: The parameter <code>retrieved_chunk</code> here refers to the generated text to be evaluated, and <code>ground_truth_chunks</code> are the reference texts.</p>"},{"location":"Guides/RAG/Evaluation/evaluation_overview/#rouge-score-recall-oriented-understudy-for-gisting-evaluation","title":"ROUGE Score (Recall-Oriented Understudy for Gisting Evaluation)","text":"<p>ROUGE is a set of metrics used for evaluating automatic summarization and machine translation. It works by comparing an automatically produced summary or translation against a set of reference summaries (typically human-produced).</p> <p>Key ROUGE variants include:</p> <ul> <li>ROUGE-N: Measures overlap of n-grams. <code>ROUGE-1</code> refers to unigram overlap, <code>ROUGE-2</code> to bigram overlap.</li> <li>ROUGE-L: Measures longest common subsequence (LCS) based statistics.</li> </ul> <p>The implementation returns F1-scores for ROUGE-1, ROUGE-2, and ROUGE-L.</p> <p><pre><code>def rouge_score(\n    retrieved_chunk: str, # Intended to be the generated text\n    ground_truth_chunk: str # A single reference text\n) -&gt; dict: # Returns {\"rouge1\": ..., \"rouge2\": ..., \"rougeL\": ...}\n</code></pre> Note: Similar to BLEU, <code>retrieved_chunk</code> refers to the generated text, and <code>ground_truth_chunk</code> is the reference text.</p>"},{"location":"Guides/RAG/Evaluation/evaluation_overview/#generation-evaluation-metrics","title":"Generation Evaluation Metrics","text":"<p>These metrics assess the quality of the text generated by the language model, typically an answer or a summary, based on the retrieved context.</p>"},{"location":"Guides/RAG/Evaluation/evaluation_overview/#llm-as-judge","title":"LLM-as-judge","text":"<p>Concept: LLM-as-judge is an evaluation technique where a separate, often more powerful, Language Model (the \"judge\" LLM) is used to assess the quality of the output generated by the system under evaluation. The judge LLM is given a prompt that includes the input query, the generated response, and sometimes reference answers or specific criteria. It then outputs a score or a qualitative assessment based on these inputs.</p> <p>Why and When to Use: This technique is particularly useful for evaluating aspects of generation that are subjective and difficult to capture with traditional automated metrics. These aspects can include:</p> <ul> <li>Coherence and Readability: How well-written and easy to understand is the generated text?</li> <li>Relevance: How relevant is the answer to the input query?</li> <li>Helpfulness and Instructiveness: How well does the answer satisfy the user's intent?</li> <li>Harmlessness: Does the output avoid generating biased, offensive, or unsafe content?</li> <li>Factual Accuracy: While LLMs can hallucinate, a judge LLM can sometimes cross-check facts if provided with context or if it has strong internal knowledge.</li> </ul> <p>LLM-as-judge offers a scalable alternative to human evaluation, which can be time-consuming and costly. It's beneficial when you need nuanced feedback on generation quality, especially for open-ended tasks where defining precise ground truth is challenging. However, it's important to be aware that judge LLMs can also have biases and their judgments might not always align perfectly with human evaluators. Careful prompt engineering for the judge LLM is crucial for obtaining reliable results.</p> <p>Example Implementation:</p> <p>The following example demonstrates how to use a Google Gemini model as a judge to determine if a predicted answer matches a given answer for a specific query.</p> <pre><code>import os\nfrom pydantic import BaseModel\nfrom datapizza.clients.google_client import GoogleClient\n\n# Initialize the GoogleClient\ntry:\n    client_judge = GoogleClient(\n        api_key=os.getenv(\"GOOGLE_API_KEY\"),\n        model=\"gemini-2.5-flash-preview-04-17\",\n    )\nexcept ValueError as e:\n    print(f\"Error initializing GoogleClient: {e}\")\n    print(\"Please ensure GOOGLE_API_KEY is set or Vertex AI parameters are correctly configured.\")\n    exit()\n\n\nclass MatchingResult(BaseModel):\n    is_matching: bool\n    # Optionally, add a field for the judge's reasoning\n    # reasoning: str | None = None\n\n# Sample data\ndata = [\n    {\"query\": \"What is the capital of France?\", \"answer\": \"Paris\", \"prediction\": \"Paris is the capital of France.\"},\n    {\"query\": \"What is 2+2?\", \"answer\": \"4\", \"prediction\": \"The result is three.\"},\n    {\"query\": \"Explain black holes.\", \"answer\": \"A black hole is a region of spacetime where gravity is so strong that nothing, including light and other electromagnetic waves, has enough energy to escape its event horizon.\", \"prediction\": \"Black holes are mysterious space objects.\"},\n]\n\n# System prompt for the judge LLM to guide its evaluation\n# This is crucial for getting consistent and accurate judgments.\njudge_system_prompt = \"\"\"You are an AI assistant acting as an impartial judge.\nYour task is to determine if the 'PREDICTION' accurately and satisfactorily answers the 'ORIGINAL QUERY', considering the provided 'ANSWER' as a reference.\nRespond with a JSON object containing two keys:\n1.  `is_matching`: a boolean value (true if the prediction matches, false otherwise).\n2.  `reasoning`: a brief explanation for your decision, especially if it's not a match.\n\nFocus on semantic similarity and factual correctness. Minor phrasing differences are acceptable if the meaning is preserved.\nIf the prediction is too vague, incomplete, or factually incorrect compared to the answer and query, it is not matching.\n\"\"\"\n\nfor item in data:\n    # Construct the input prompt for the judge LLM\n    judge_input_prompt = f\"\"\"ORIGINAL QUERY:\n{item['query']}\n\nANSWER:\n{item['answer']}\n\nPREDICTION:\n{item['prediction']}\"\"\"\n\n    try:\n        # The `structured_response` method in GoogleClient is designed to return a Pydantic model.\n        # It internally handles prompting the LLM to output JSON matching the Pydantic model's schema.\n        client_response = client_judge.structured_response(\n            input=judge_input_prompt,\n            output_cls=MatchingResult,\n            system_prompt=judge_system_prompt, # Pass the detailed system prompt\n            # temperature=0.2 # Lower temperature for more deterministic judging\n        )\n\n        # The structured_data will be a list of Pydantic model instances.\n        matching_data = client_response.structure_data[0] # Accessing the Pydantic model instance\n        item[\"is_matching\"] = matching_data.is_matching\n        # item[\"reasoning\"] = matching_data.reasoning # If you added reasoning to MatchingResult\n        print(f\"Query: {item['query']}\")\n        print(f\"  Prediction: {item['prediction']}\")\n        print(f\"  Is matching: {item['is_matching']}\")\n        # print(f\"  Reasoning: {item.get('reasoning', 'N/A')}\")\n\n    except Exception as e:\n        item[\"is_matching\"] = False # Default on error\n        # item[\"reasoning\"] = f\"Error during LLM call: {e!s}\"\n        print(f\"Error processing item for query '{item['query']}': {e}\")\n        print(f\"  Is matching: {item['is_matching']}\")\n</code></pre>"},{"location":"Guides/RAG/Evaluation/evaluation_overview/#building-a-golden-dataset","title":"Building a Golden Dataset","text":"<p>Creating a high-quality \"golden\" dataset is fundamental for robust RAG evaluation. This dataset consists of queries paired with ideal outcomes for both retrieval and generation stages.</p>"},{"location":"Guides/RAG/Evaluation/evaluation_overview/#retrieval-ground-truth","title":"Retrieval Ground Truth","text":"<p>For retrieval, the goal is to define, for each sample query, which document chunks are considered relevant.</p> <p>Ideal Format: The most effective ground truth for retrieval typically involves:</p> <ul> <li>A list of precise text spans that are relevant to the query.</li> <li>These spans can be represented as:<ul> <li>The exact string content of the relevant chunk.</li> <li>Alternatively, a pointer to the chunk's location, such as <code>filename</code>, <code>start_char_offset</code>, and <code>end_char_offset</code>. This is particularly useful for large documents or when context around the chunk is important or when the same string may appear on multiple documents.</li> </ul> </li> </ul> <p>Key Insights &amp; Considerations:</p> <ul> <li>Chunk Granularity:<ul> <li>A critical decision is the size of the ground truth chunks. Should they be individual sentences, paragraphs, or fixed-size blocks of text?</li> <li>Alignment with System: Ideally, the granularity of your ground truth chunks should align with how your RAG system chunks and retrieves documents. This makes comparisons more direct.</li> <li>Atomic vs. Contextual: Smaller, more atomic chunks can be good for evaluating precision (i.e., did the system find the exact piece of information?). Larger chunks might provide more context but could dilute the signal if only a small part of the chunk is truly relevant.</li> <li>Practical Approach: Often, sentence-to-paragraph level granularity strikes a good balance.</li> </ul> </li> <li>Number of Relevant Chunks: Some queries may have a single, definitive relevant chunk, while others might have multiple relevant pieces of information scattered across different chunks. Your golden dataset should reflect this variability.</li> <li>Metadata of Queries: Annotate queries features (e.g. the source of the query). This way you can subset dataset, in this way it will be representative of what your RAG system will encounter. This could include real user logs, expert-crafted questions, or even synthetically generated queries designed to test specific retrieval challenges.</li> </ul>"},{"location":"Guides/RAG/Evaluation/evaluation_overview/#generation-ground-truth","title":"Generation Ground Truth","text":"<p>For generation, the goal is to define, for each sample query (and ideally, its corresponding golden retrieved context), what constitutes a high-quality generated response.</p> <p>Ideal Format:</p> <ul> <li>Human-Generated Responses: The gold standard for generation ground truth is responses written or reviewed by humans. These capture nuance, correctness, and desired style better than any automated method.</li> <li>Multiple References: For a single query, having multiple valid human-generated reference answers can be highly beneficial. This acknowledges that there can be several good ways to answer a question.</li> <li>Negative References: Including examples of incorrect or inadequate responses is equally important. These negative references help train evaluation models to recognize what constitutes a poor response, whether due to factual inaccuracies, incompleteness, or inappropriate style. They provide contrast to positive examples and strengthen the evaluation framework's ability to discriminate between high and low-quality outputs.</li> </ul> <p>Key Insights &amp; Considerations:</p> <ul> <li>Quality over Quantity: A smaller dataset of high-quality, carefully curated human responses is often more valuable than a large dataset of noisy or inconsistent ones.</li> <li>Specificity and Completeness: Ground truth answers should be as specific and complete as required by the user's intent and the context.</li> <li>Factual Accuracy: All reference answers must be factually correct and up-to-date.</li> <li>Style and Tone: The ground truth should reflect the desired style, tone, and persona of your RAG system's output.</li> <li>Consistency with Retrieved Context (for RAG): When evaluating the generation component of a RAG system in isolation, the golden answer should ideally be based only on the information present in the \"golden\" retrieved context provided for that query. This helps differentiate between failures in retrieval versus failures in generation.</li> <li>Annotation Effort: Creating high-quality generation ground truth is labor-intensive. Plan for significant time and resources, especially if relying on human annotators. Clear instructions and calibration exercises for annotators are essential.</li> <li>Diversity: Ensure your dataset covers a wide range of query types, expected answer lengths, and complexities to thoroughly test the generation capabilities.</li> </ul> <p>Building a comprehensive and well-annotated golden dataset is an iterative process, but it's an invaluable investment for understanding and improving your RAG system's performance.</p>"},{"location":"Guides/RAG/Evaluation/evaluation_overview/#building-semi-synthetic-golden-datasets","title":"Building (Semi-)Synthetic Golden Datasets","text":"<p>While manually curated \"golden\" datasets are the ideal, they can be time-consuming and expensive to create. (Semi)synthetic dataset generation offers a way to augment or bootstrap this process using LLMs. This involves using an LLM to generate queries, answers, or both, potentially based on your existing document corpus.</p> <p>Pros:</p> <ul> <li>Scalability &amp; Speed: LLMs can generate large volumes of query-answer pairs much faster than human annotators.</li> <li>Cost-Effectiveness: Can be cheaper than extensive human annotation, especially for initial dataset creation.</li> <li>Coverage: Can help generate queries for a wider range of documents or topics in your corpus, potentially uncovering blind spots.</li> <li>Bootstrapping: Useful for creating an initial dataset when no or very little human-annotated data is available.</li> </ul> <p>Cons:</p> <ul> <li>Quality Variation: The quality of synthetic data can vary significantly. LLMs might generate:<ul> <li>Unrealistic Queries: Queries that don't reflect real user search intent or language.</li> <li>Hallucinated Answers: Factually incorrect or nonsensical answers.</li> <li>Lack of Domain Knowledge: LLMs might not have sufficient domain-specific knowledge or understanding of the nuances within your documents to create high-quality, relevant query-answer pairs.</li> <li>Limited Context Understanding: LLMs might struggle to generate queries/answers that require deep contextual understanding spanning multiple documents or complex reasoning.</li> </ul> </li> <li>Bias Amplification: If the LLM used for generation has biases, these can be reflected and amplified in the synthetic dataset.</li> <li>Evaluation of the Generator: The process itself requires careful evaluation \u2013 how do you ensure the synthetic data generator is good?</li> </ul> <p>Tips for Creating (Semi-)Synthetic Datasets:</p> <ul> <li>Human-in-the-Loop / Supervision:<ul> <li>Highly Recommended: Human review and refinement of synthetically generated data are crucial to ensure quality, relevance, and factual accuracy. Don't rely solely on fully synthetic generation without oversight.</li> <li>Few-Shot Prompting: Provide the LLM with a few high-quality, human-curated examples (few-shot learning) to guide its generation process. This can significantly improve the relevance and style of the generated queries and answers, making them more similar to final user queries.</li> </ul> </li> <li>Leverage Powerful Models for Ground Truth Generation:<ul> <li>Consider using a more powerful (and potentially more expensive) LLM or a more complex pipeline to generate ground truth data. This higher-quality synthetic data can then be used to evaluate your actual production RAG pipeline, which might use a more cost-effective model.</li> </ul> </li> <li>Contextual Grounding:<ul> <li>When generating query-answer pairs, ensure the LLM is grounded in your actual documents. This can involve providing specific document chunks as context and instructing the LLM to generate questions and answers based only on that context.</li> </ul> </li> <li>Iterative Refinement: Start with a small batch, review, refine your prompts or generation strategy, and then scale up.</li> <li>Vary Generation Techniques:<ul> <li>Question Generation from Text: Provide a document chunk and ask the LLM to generate relevant questions.</li> <li>Answer Generation from Question+Context: Provide a (human or synthetic) question and a relevant document chunk, and ask the LLM to generate an answer.</li> </ul> </li> <li>Negative Examples: Instruct the LLM to also generate plausible but incorrect answers or irrelevant questions to help build a more robust evaluation set.</li> <li>Control for Complexity and Style: Use prompts to guide the LLM on the desired complexity, type (e.g., factual, comparison, summary), and style of the generated queries and answers.</li> <li>Filter and Validate: Implement post-processing steps to filter out low-quality or irrelevant synthetic data. This could involve heuristics, similarity checks against existing data, or even using another LLM as a preliminary judge.</li> </ul> <p>Building a (semi)synthetic dataset is a powerful technique, but it requires careful planning, execution, and ongoing human oversight to ensure it genuinely contributes to improving your RAG system. </p>"}]}